# æœºå™¨å­¦ä¹ ï¼ŒNLP:ä½¿ç”¨ scikit-learnï¼Œpython å’Œ NLTK çš„æ–‡æœ¬åˆ†ç±»ã€‚

> åŸæ–‡ï¼š<https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a?source=collection_archive---------0----------------------->

![](img/283fe77769dcbf7f0efc95f3cbf577f5.png)

Text Classification

**æœ€æ–°æ›´æ–°:**
æˆ‘å·²ç»æŠŠå®Œæ•´çš„ä»£ç (Python å’Œ Jupyter ç¬”è®°æœ¬)ä¸Šä¼ åˆ° GitHub ä¸Š:[https://github.com/javedsha/text-classification](https://github.com/javedsha/text-classification)

**æ–‡æ¡£/æ–‡æœ¬åˆ†ç±»**æ˜¯*ç›‘ç£çš„*æœºå™¨å­¦ä¹ (ML)ä¸­é‡è¦è€Œå…¸å‹çš„ä»»åŠ¡ä¹‹ä¸€ã€‚ä¸ºæ–‡æ¡£åˆ†é…ç±»åˆ«ï¼Œæ–‡æ¡£å¯ä»¥æ˜¯ç½‘é¡µã€å›¾ä¹¦é¦†ä¹¦ç±ã€åª’ä½“æ–‡ç« ã€å›¾åº“ç­‰ã€‚æœ‰è®¸å¤šåº”ç”¨ï¼Œä¾‹å¦‚åƒåœ¾é‚®ä»¶è¿‡æ»¤ã€ç”µå­é‚®ä»¶è·¯ç”±ã€æƒ…æ„Ÿåˆ†æç­‰ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³æ¼”ç¤ºæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨ pythonã€scikit-learn å’Œä¸€ç‚¹ NLTK æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ã€‚

***å…è´£å£°æ˜* :** *æˆ‘æ˜¯æœºå™¨å­¦ä¹ æ–°æ‰‹ï¼Œä¹Ÿæ˜¯åšå®¢æ–°æ‰‹(ç¬¬ä¸€æ¬¡)ã€‚å› æ­¤ï¼Œå¦‚æœæœ‰ä»»ä½•é”™è¯¯ï¼Œè¯·è®©æˆ‘çŸ¥é“ã€‚æ„Ÿè°¢æ‰€æœ‰åé¦ˆã€‚*

è®©æˆ‘ä»¬å°†åˆ†ç±»é—®é¢˜åˆ†æˆä»¥ä¸‹æ­¥éª¤:

1.  å…ˆå†³æ¡ä»¶å’Œè®¾ç½®ç¯å¢ƒã€‚
2.  åœ¨ jupyter ä¸­åŠ è½½æ•°æ®é›†ã€‚
3.  ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–ç‰¹å¾ã€‚
4.  è¿è¡Œ ML ç®—æ³•ã€‚
5.  å‚æ•°è°ƒæ•´çš„ç½‘æ ¼æœç´¢ã€‚
6.  æœ‰ç”¨çš„æç¤ºå’Œä¸€ç‚¹ NLTKã€‚

## **æ­¥éª¤ 1:å…ˆå†³æ¡ä»¶å’Œè®¾ç½®ç¯å¢ƒ**

éµå¾ªè¿™ä¸ªä¾‹å­çš„å…ˆå†³æ¡ä»¶æ˜¯ python ç‰ˆæœ¬ **2.7.3** å’Œ jupyter notebookã€‚ä½ å¯ä»¥å®‰è£…[**anaconda**](https://www.continuum.io/downloads)**å®ƒä¼šä¸ºä½ å¾—åˆ°ä¸€åˆ‡ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦ä¸€ç‚¹ python å’Œ ML åŸºç¡€çŸ¥è¯†ï¼ŒåŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ scikit-learn (python)åº“ã€‚**

## **ç¬¬äºŒæ­¥:**åœ¨ jupyter ä¸­åŠ è½½æ•°æ®é›†ã€‚

æœ¬ä¾‹ä¸­ä½¿ç”¨çš„æ•°æ®é›†æ˜¯è‘—åçš„â€œ20 æ–°é—»ç»„â€æ•°æ®é›†ã€‚å…³äºæ¥è‡ªåŸ[ç½‘ç«™](http://qwone.com/~jason/20Newsgroups/)çš„æ•°æ®:

> 20 ä¸ªæ–°é—»ç»„æ•°æ®é›†æ˜¯å¤§çº¦ 20ï¼Œ000 ä¸ªæ–°é—»ç»„æ–‡æ¡£çš„é›†åˆï¼Œå¹³å‡åˆ†å¸ƒåœ¨ 20 ä¸ªä¸åŒçš„æ–°é—»ç»„ä¸­ã€‚æ®æˆ‘æ‰€çŸ¥ï¼Œå®ƒæœ€åˆæ˜¯ç”± Ken Lang æ”¶é›†çš„ï¼Œå¯èƒ½æ˜¯ä¸ºäº†ä»–çš„[news weaver:Learning to filter net news](http://qwone.com/~jason/20Newsgroups/lang95.bib)paperï¼Œå°½ç®¡ä»–æ²¡æœ‰æ˜ç¡®æåˆ°è¿™ä¸ªæ”¶é›†ã€‚20 ä¸ªæ–°é—»ç»„é›†åˆå·²ç»æˆä¸ºæœºå™¨å­¦ä¹ æŠ€æœ¯çš„æ–‡æœ¬åº”ç”¨å®éªŒçš„æµè¡Œæ•°æ®é›†ï¼Œä¾‹å¦‚æ–‡æœ¬åˆ†ç±»å’Œæ–‡æœ¬èšç±»ã€‚

è¿™ä¸ªæ•°æ®é›†å†…ç½®åœ¨ scikit ä¸­ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼ä¸‹è½½å®ƒã€‚

I .åœ¨ windows ä¸­æ‰“å¼€å‘½ä»¤æç¤ºç¬¦ï¼Œé”®å…¥â€œjupyter notebookâ€ã€‚è¿™å°†åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ç¬”è®°æœ¬ï¼Œå¹¶ä¸ºæ‚¨å¯åŠ¨ä¸€ä¸ªä¼šè¯ã€‚

äºŒã€‚é€‰æ‹©æ–°å»º> Python 2ã€‚ä½ å¯ä»¥ç»™ç¬”è®°æœ¬èµ·ä¸ªåå­—- *æ–‡æœ¬åˆ†ç±»æ¼”ç¤º 1*

![](img/a0da64046b12cb853ec73b4e8c443775.png)

ä¸‰ã€‚åŠ è½½æ•°æ®é›†:(è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…)

```
from sklearn.datasets import fetch_20newsgroups
twenty_train = fetch_20newsgroups(subset='train', shuffle=True)
```

*æ³¨:ä»¥ä¸Šï¼Œæˆ‘ä»¬åªæ˜¯åŠ è½½äº†* ***è®­ç»ƒ*** *æ•°æ®ã€‚æˆ‘ä»¬å°†åœ¨åé¢çš„ä¾‹å­ä¸­å•ç‹¬åŠ è½½æµ‹è¯•æ•°æ®ã€‚*

å››ã€‚æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ£€æŸ¥ç›®æ ‡åç§°(ç±»åˆ«)å’Œä¸€äº›æ•°æ®æ–‡ä»¶ã€‚

```
twenty_train.target_names #prints all the categories
print("\n".join(twenty_train.data[0].split("\n")[:3])) #prints first line of the first data file
```

## **ç¬¬ä¸‰æ­¥:ä»æ–‡æœ¬æ–‡ä»¶ä¸­æå–ç‰¹å¾ã€‚**

æ–‡æœ¬æ–‡ä»¶å®é™…ä¸Šæ˜¯ä¸€ç³»åˆ—å•è¯(æœ‰åºçš„)ã€‚ä¸ºäº†è¿è¡Œæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œæˆ‘ä»¬éœ€è¦å°†æ–‡æœ¬æ–‡ä»¶è½¬æ¢æˆæ•°å­—ç‰¹å¾å‘é‡ã€‚æˆ‘ä»¬å°†ä»¥ä½¿ç”¨ [**è¢‹å­—**](https://en.wikipedia.org/wiki/Bag-of-words_model) æ¨¡å‹ä¸ºä¾‹ã€‚ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ–‡æœ¬æ–‡ä»¶åˆ†å‰²æˆå•è¯(è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²)ï¼Œå¹¶è®¡ç®—æ¯ä¸ªå•è¯åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œæœ€åç»™æ¯ä¸ªå•è¯åˆ†é…ä¸€ä¸ªæ•´æ•° idã€‚**æˆ‘ä»¬å­—å…¸ä¸­çš„æ¯ä¸ªå”¯ä¸€çš„å•è¯éƒ½ä¼šå¯¹åº”ä¸€ä¸ªç‰¹å¾(æè¿°æ€§ç‰¹å¾)ã€‚**

Scikit-learn æœ‰ä¸€ä¸ªé«˜çº§ç»„ä»¶ï¼Œå®ƒå°†ä¸ºç”¨æˆ·â€œè®¡æ•°çŸ¢é‡å™¨â€åˆ›å»ºç‰¹å¾çŸ¢é‡ã€‚æ›´å¤šå…³äºå®ƒçš„[è¿™é‡Œ](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)ã€‚

```
from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(twenty_train.data)
X_train_counts.shape
```

è¿™é‡Œé€šè¿‡åš'*count _ vect . fit _ transform(twenty _ train . data)*'ï¼Œæˆ‘ä»¬æ­£åœ¨å­¦ä¹ è¯æ±‡è¯å…¸ï¼Œå®ƒè¿”å›ä¸€ä¸ªæ–‡æ¡£-æœ¯è¯­çŸ©é˜µã€‚[n ä¸ªæ ·æœ¬ï¼Œn ä¸ªç‰¹å¾]ã€‚

ä»…ä»…ç»Ÿè®¡æ¯ä¸ªæ–‡æ¡£çš„å­—æ•°æœ‰ä¸€ä¸ªé—®é¢˜:å®ƒä¼šç»™è¾ƒé•¿çš„æ–‡æ¡£æ¯”è¾ƒçŸ­çš„æ–‡æ¡£æ›´å¤šçš„æƒé‡ã€‚ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æ¯ä¸ªæ–‡æ¡£ä¸­ä½¿ç”¨é¢‘ç‡(**TF-Term frequency**)ï¼Œå³#count(word) / #Total wordsã€‚

**TF-IDF:** æœ€åï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥é™ä½æ›´å¸¸è§å•è¯çš„æƒé‡ï¼Œå¦‚(theï¼Œisï¼Œan ç­‰ã€‚)å‡ºç°åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­ã€‚è¿™è¢«ç§°ä¸º **TF-IDFï¼Œå³æœ¯è¯­é¢‘ç‡ä¹˜ä»¥é€†æ–‡æ¡£é¢‘ç‡ã€‚**

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢ä¸€è¡Œä»£ç å®ç°è¿™ä¸¤ä¸ªç›®æ ‡:

```
from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf.shape
```

æœ€åä¸€è¡Œå°†è¾“å‡ºæ–‡æ¡£-æœ¯è¯­çŸ©é˜µçš„ç»´åº¦-> (11314ï¼Œ130107)ã€‚

## **æ­¥éª¤å››ã€‚è¿è¡Œ ML ç®—æ³•ã€‚**

æœ‰å„ç§ç®—æ³•å¯ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚æˆ‘ä»¬å°±ä»æœ€ç®€å•çš„ä¸€ä¸ª'[æœ´ç´ è´å¶æ–¯](http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) (NB)' ( *ä¸è¦è§‰å¾—å¤ªå¹¼ç¨šï¼*ğŸ˜ƒ)

æ‚¨å¯ä»¥ä½¿ç”¨ä¸‹é¢ä¸¤è¡Œä»£ç åœ¨ scikit ä¸­è½»æ¾æ„å»ºä¸€ä¸ª NBclassifier:(æ³¨æ„â€”â€”NB æœ‰è®¸å¤šå˜ä½“ï¼Œä½†å…³äºå®ƒä»¬çš„è®¨è®ºè¶…å‡ºäº†èŒƒå›´)

```
from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)
```

è¿™å°†åœ¨æˆ‘ä»¬æä¾›çš„è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒ NB åˆ†ç±»å™¨ã€‚

**æ„å»ºç®¡é“:**æˆ‘ä»¬å¯ä»¥ç¼–å†™æ›´å°‘çš„ä»£ç ï¼Œé€šè¿‡å¦‚ä¸‹æ–¹å¼æ„å»ºç®¡é“æ¥å®Œæˆä¸Šè¿°æ‰€æœ‰å·¥ä½œ:

```
**>>> from** **sklearn.pipeline** **import** Pipeline
**>>>** text_clf = Pipeline([('vect', CountVectorizer()),
**... **                     ('tfidf', TfidfTransformer()),
**... **                     ('clf', MultinomialNB()),
**...** ])text_clf = text_clf.fit(twenty_train.data, twenty_train.target)
```

*â€˜vectâ€™ã€â€˜tfi dfâ€™å’Œâ€˜clfâ€™è¿™å‡ ä¸ªåå­—æ˜¯éšæ„å–çš„ï¼Œä½†ä»¥åä¼šç”¨åˆ°ã€‚*

**NB åˆ†ç±»å™¨çš„æ€§èƒ½:**ç°åœ¨æˆ‘ä»¬å°†åœ¨**æµ‹è¯•é›†**ä¸Šæµ‹è¯• NB åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚

```
import numpy as np
twenty_test = fetch_20newsgroups(subset='test', shuffle=True)
predicted = text_clf.predict(twenty_test.data)
np.mean(predicted == twenty_test.target)
```

æˆ‘ä»¬å¾—åˆ°çš„å‡†ç¡®ç‡æ˜¯ **~77.38%** ï¼Œå¯¹äº start å’Œä¸€ä¸ªå¹¼ç¨šçš„åˆ†ç±»å™¨æ¥è¯´å·²ç»ä¸é”™äº†ã€‚è¿˜æœ‰ï¼Œæ­å–œä½ ï¼ï¼ï¼æ‚¨ç°åœ¨å·²ç»æˆåŠŸåœ°ç¼–å†™äº†ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ç®—æ³•ğŸ‘

**æ”¯æŒå‘é‡æœº(SVM):** è®©æˆ‘ä»¬å°è¯•ä½¿ç”¨ä¸åŒçš„ç®—æ³• SVMï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚æ›´å¤šå…³äºå®ƒçš„[åœ¨è¿™é‡Œ](http://scikit-learn.org/stable/modules/svm.html)ã€‚

```
>>> from sklearn.linear_model import SGDClassifier>>> text_clf_svm = Pipeline([('vect', CountVectorizer()),
...                      ('tfidf', TfidfTransformer()),
...                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',
...                                            alpha=1e-3, n_iter=5, random_state=42)),
... ])>>> _ = text_clf_svm.fit(twenty_train.data, twenty_train.target)>>> predicted_svm = text_clf_svm.predict(twenty_test.data)
>>> np.mean(predicted_svm == twenty_test.target)
```

æˆ‘ä»¬å¾—åˆ°çš„å‡†ç¡®ç‡æ˜¯ **~82.38%ã€‚**å’¦ï¼Œå¥½ä¸€ç‚¹äº†ğŸ‘Œ

## **ç¬¬äº”æ­¥ã€‚ç½‘æ ¼æœç´¢**

å‡ ä¹æ‰€æœ‰çš„åˆ†ç±»å™¨éƒ½æœ‰å„ç§å‚æ•°ï¼Œå¯ä»¥è°ƒæ•´è¿™äº›å‚æ•°ä»¥è·å¾—æœ€ä½³æ€§èƒ½ã€‚Scikit æä¾›äº†ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å·¥å…·â€˜GridSearchCVâ€™ã€‚

```
>>> from sklearn.model_selection import GridSearchCV
>>> parameters = {'vect__ngram_range': [(1, 1), (1, 2)],
...               'tfidf__use_idf': (True, False),
...               'clf__alpha': (1e-2, 1e-3),
... }
```

è¿™é‡Œï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå‚æ•°åˆ—è¡¨ï¼Œæˆ‘ä»¬å¸Œæœ›å¯¹è¿™äº›å‚æ•°è¿›è¡Œæ€§èƒ½è°ƒä¼˜ã€‚æ‰€æœ‰å‚æ•°åç§°éƒ½ä»¥åˆ†ç±»å™¨åç§°å¼€å§‹(è®°ä½æˆ‘ä»¬ç»™å‡ºçš„ä»»æ„åç§°)ã€‚å¦‚ vect _ _ ngram _ range åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å‘Šè¯‰ä½¿ç”¨ä¸€å…ƒå’ŒäºŒå…ƒï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªæ˜¯æœ€ä½³çš„ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡ä¼ é€’åˆ†ç±»å™¨ã€å‚æ•°å’Œ n_jobs=-1 æ¥åˆ›å»ºç½‘æ ¼æœç´¢çš„å®ä¾‹ï¼Œn _ jobs =-1 å‘Šè¯‰ä½¿ç”¨æ¥è‡ªç”¨æˆ·æœºå™¨çš„å¤šä¸ªå†…æ ¸ã€‚

```
gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)
gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)
```

è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿçš„æ—¶é—´ï¼Œå…·ä½“å–å†³äºæœºå™¨é…ç½®ã€‚

æœ€åï¼Œè¦æŸ¥çœ‹æœ€ä½³å¹³å‡åˆ†æ•°å’Œå‚æ•°ï¼Œè¯·è¿è¡Œä»¥ä¸‹ä»£ç :

```
gs_clf.best_score_
gs_clf.best_params_
```

NB åˆ†ç±»å™¨çš„å‡†ç¡®ç‡ç°åœ¨å·²ç»æé«˜åˆ° **~90.6%** **(ä¸å†é‚£ä¹ˆå¹¼ç¨šäº†ï¼ğŸ˜„)å’Œå¯¹åº”çš„å‚æ•°æ˜¯{'clf__alpha': 0.01ï¼Œ' tfidf__use_idf': Trueï¼Œ' vect__ngram_range': (1ï¼Œ2)}ã€‚**

ç±»ä¼¼åœ°ï¼Œå¯¹äº SVM åˆ†ç±»å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹é¢çš„ä»£ç è·å¾—äº†æé«˜çš„å‡†ç¡®ç‡ **~89.79%** ã€‚*æ³¨æ„:æ‚¨å¯ä»¥é€šè¿‡è°ƒæ•´å…¶ä»–å‚æ•°æ¥è¿›ä¸€æ­¥ä¼˜åŒ– SVM åˆ†ç±»å™¨ã€‚è¿™æ˜¯ç•™ç»™ä½ å»æ¢ç´¢æ›´å¤šã€‚*

```
>>> from sklearn.model_selection import GridSearchCV
>>> parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)],
...               'tfidf__use_idf': (True, False),
...               'clf-svm__alpha': (1e-2, 1e-3),
... }
gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)
gs_clf_svm = gs_clf_svm.fit(twenty_train.data, twenty_train.target)
gs_clf_svm.best_score_
gs_clf_svm.best_params_
```

## **ç¬¬å…­æ­¥:**æœ‰ç”¨çš„å°æŠ€å·§å’Œä¸€ç‚¹ NLTKã€‚

1.  **ä»æ•°æ®ä¸­åˆ é™¤** [**åœæ­¢å­—**](https://en.wikipedia.org/wiki/Stop_words) **:** (theï¼Œthen etc)ã€‚åªæœ‰å½“åœç”¨è¯å¯¹æ½œåœ¨é—®é¢˜æ²¡æœ‰ç”¨æ—¶ï¼Œæ‰åº”è¯¥è¿™æ ·åšã€‚åœ¨å¤§å¤šæ•°çš„æ–‡æœ¬åˆ†ç±»é—®é¢˜ä¸­ï¼Œè¿™ç¡®å®æ˜¯æ²¡æœ‰ç”¨çš„ã€‚è®©æˆ‘ä»¬çœ‹çœ‹åˆ é™¤åœç”¨è¯æ˜¯å¦ä¼šæé«˜å‡†ç¡®æ€§ã€‚æŒ‰å¦‚ä¸‹æ–¹å¼æ›´æ–°ç”¨äºåˆ›å»º CountVectorizer å¯¹è±¡çš„ä»£ç :

```
>>> from sklearn.pipeline import Pipeline
>>> text_clf = Pipeline([('vect', CountVectorizer(**stop_words='english**')),
...                      ('tfidf', TfidfTransformer()),
...                      ('clf', MultinomialNB()),
... ])
```

è¿™æ˜¯æˆ‘ä»¬ä¸º NB åˆ†ç±»å™¨æ„å»ºçš„ç®¡é“ã€‚åƒä»¥å‰ä¸€æ ·è¿è¡Œå‰©ä½™çš„æ­¥éª¤ã€‚è¿™å°±æŠŠå‡†ç¡®ç‡ä» **77.38%æé«˜åˆ°äº† 81.69%** (é‚£å¤ªå¥½äº†)ã€‚*ä½ å¯ä»¥å¯¹ SVM åšåŒæ ·çš„å°è¯•ï¼ŒåŒæ—¶è¿›è¡Œç½‘æ ¼æœç´¢ã€‚*

2. **FitPrior=False:** å½“[å¤šé¡¹å¼ B](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) è®¾ç½®ä¸º False æ—¶ï¼Œå°†ä½¿ç”¨ç»Ÿä¸€çš„å…ˆéªŒã€‚è¿™å¹¶æ²¡æœ‰å¤šå¤§å¸®åŠ©ï¼Œä½†å°†å‡†ç¡®ç‡ä» 81.69%æé«˜åˆ°äº† 82.14%(æ²¡æœ‰å¤ªå¤§çš„æé«˜)ã€‚è¯•ç€çœ‹çœ‹è¿™æ˜¯å¦é€‚ç”¨äºä½ çš„æ•°æ®é›†ã€‚

**3ã€‚è¯å¹²åŒ–:è¯å¹²åŒ–æ˜¯å°†è¯å½¢å˜åŒ–(æœ‰æ—¶æ˜¯æ´¾ç”Ÿçš„)çš„å•è¯ç®€åŒ–ä¸ºè¯å¹²ã€è¯æ ¹æˆ–è¯æ ¹å½¢å¼çš„è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œè¯å¹²ç®—æ³•å°†å•è¯â€œfishingâ€ã€â€œfishedâ€å’Œâ€œfisherâ€ç®€åŒ–ä¸ºè¯æ ¹å•è¯â€œfishâ€ã€‚**

æˆ‘ä»¬éœ€è¦ NLTKï¼Œå®ƒå¯ä»¥ä»[å®‰è£…åˆ°è¿™é‡Œ](http://www.nltk.org/)ã€‚NLTK é™„å¸¦äº†å„ç§è¯å¹²åˆ†æå™¨(*å…³äºè¯å¹²åˆ†æå™¨å¦‚ä½•å·¥ä½œçš„ç»†èŠ‚è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´*)ï¼Œå®ƒä»¬å¯ä»¥å¸®åŠ©å°†å•è¯ç®€åŒ–ä¸ºå®ƒä»¬çš„æ ¹å½¢å¼ã€‚å†æ¬¡ä½¿ç”¨è¿™ä¸ªï¼Œå¦‚æœå®ƒå¯¹ä½ é—®é¢˜æœ‰æ„ä¹‰çš„è¯ã€‚

ä¸‹é¢æˆ‘ç”¨äº†é›ªçƒè¯å¹²åˆ†æå™¨ï¼Œå®ƒå¯¹è‹±è¯­éå¸¸æœ‰æ•ˆã€‚

```
import nltk
nltk.download()from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english", ignore_stopwords=True)class StemmedCountVectorizer(CountVectorizer):
    def build_analyzer(self):
        analyzer = super(StemmedCountVectorizer, self).build_analyzer()
        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])stemmed_count_vect = StemmedCountVectorizer(stop_words='english')text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect),
...                      ('tfidf', TfidfTransformer()),
...                      ('mnb', MultinomialNB(fit_prior=False)),
... ])text_mnb_stemmed = text_mnb_stemmed.fit(twenty_train.data, twenty_train.target)predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)np.mean(predicted_mnb_stemmed == twenty_test.target)
```

æˆ‘ä»¬å¾—åˆ°çš„è¯å¹²å‡†ç¡®ç‡çº¦ä¸º 81.67%ã€‚åœ¨æˆ‘ä»¬ä½¿ç”¨ NB åˆ†ç±»å™¨çš„æƒ…å†µä¸‹ï¼Œè¾¹é™…æ”¹è¿›ã€‚ä½ ä¹Ÿå¯ä»¥å°è¯• SVM å’Œå…¶ä»–ç®—æ³•ã€‚

**ç»“è®º:**æˆ‘ä»¬å·²ç»å­¦ä¹ äº† NLP ä¸­çš„ç»å…¸é—®é¢˜ï¼Œæ–‡æœ¬åˆ†ç±»ã€‚æˆ‘ä»¬å­¦ä¹ äº†ä¸€äº›é‡è¦çš„æ¦‚å¿µï¼Œå¦‚å•è¯è¢‹ã€TF-IDF å’Œä¸¤ä¸ªé‡è¦çš„ç®—æ³• NB å’Œ SVMã€‚æˆ‘ä»¬çœ‹åˆ°ï¼Œå¯¹äºæˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä¸¤ç§ç®—æ³•åœ¨ä¼˜åŒ–æ—¶å‡ ä¹ç›¸ç­‰ã€‚æœ‰æ—¶ï¼Œå¦‚æœæˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®é›†ï¼Œç®—æ³•çš„é€‰æ‹©å‡ ä¹ä¸ä¼šæœ‰ä»€ä¹ˆä¸åŒã€‚æˆ‘ä»¬è¿˜äº†è§£äº†å¦‚ä½•æ‰§è¡Œç½‘æ ¼æœç´¢ä»¥è¿›è¡Œæ€§èƒ½è°ƒä¼˜ï¼Œå¹¶ä½¿ç”¨äº† NLTK è¯å¹²æ³•ã€‚æ‚¨å¯ä»¥åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨è¿™äº›ä»£ç ï¼Œçœ‹çœ‹å“ªäº›ç®—æ³•æœ€é€‚åˆæ‚¨ã€‚

**æ›´æ–°:**å¦‚æœæœ‰äººå°è¯•äº†ä¸åŒçš„ç®—æ³•ï¼Œè¯·åœ¨è¯„è®ºåŒºåˆ†äº«ç»“æœï¼Œå¯¹å¤§å®¶éƒ½ä¼šæœ‰ç”¨ã€‚

è¯·è®©æˆ‘çŸ¥é“æ˜¯å¦æœ‰ä»»ä½•é”™è¯¯å’Œåé¦ˆæ˜¯å—æ¬¢è¿çš„âœŒï¸

æ¨èï¼Œè¯„è®ºï¼Œåˆ†äº«å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ã€‚

## **å‚è€ƒæ–‡çŒ®:**

http://scikit-learn.org/(ä»£ç 

ã€http://qwone.com/~jason/20Newsgroups/ (æ•°æ®é›†)