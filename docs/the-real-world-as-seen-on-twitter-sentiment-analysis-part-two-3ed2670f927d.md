# è¿™å°±æ˜¯æ¨ç‰¹å¦‚ä½•çœ‹å¾…ä¸–ç•Œ:æƒ…æ„Ÿåˆ†æç¬¬äºŒéƒ¨åˆ†

> åŸæ–‡ï¼š<https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-two-3ed2670f927d?source=collection_archive---------6----------------------->

![](img/6b2fa8567cccd0236400c0b0e7ce8944.png)

image by [Deepthiyathiender](https://commons.wikimedia.org/wiki/File:State_Choropleth.png)

è¿™ç¯‡åšå®¢æ–‡ç« æ˜¯ä¸Šä¸€ç¯‡æ–‡ç« çš„å»¶ç»­ï¼Œä¸Šä¸€ç¯‡æ–‡ç« è®²è¿°äº†æœºå™¨å­¦ä¹ (ML)æƒ…æ„Ÿåˆ†æä»»åŠ¡çš„æ–‡æœ¬é¢„å¤„ç†çš„å†…éƒ¨å·¥ä½œã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æŠŠæ¥è‡ª[é›† 140](http://www.sentiment140.com/) çš„æ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚åœ¨è®­ç»ƒæ¨¡å‹ä¹‹åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒæ¥å¯¹æœªè§è¿‡çš„ twitter æ•°æ®è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»ï¼Œè¿™äº›æ•°æ®å·²ç»ä»¥ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„æ–¹å¼è¿›è¡Œäº†é¢„å¤„ç†ã€‚

# ç¬¬ä¸€éƒ¨åˆ†:æ¨¡å‹è®­ç»ƒ

## æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨

åœ¨è¿™ä¸ªåˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºäº[è´å¶æ–¯å®šç†](https://en.wikipedia.org/wiki/Bayes%27_theorem)çš„**æœ´ç´ è´å¶æ–¯(NB)** åˆ†ç±»å™¨ã€‚ç®€è€Œè¨€ä¹‹ï¼ŒNB åˆ†ç±»å™¨å‡è®¾ä¸€ä¸ªç±»ä¸­ç‰¹å®šç‰¹å¾çš„å­˜åœ¨ä¸ä»»ä½•å…¶ä»–ç‰¹å¾çš„å­˜åœ¨æ— å…³ã€‚å› æ­¤ï¼Œä¸¾ä¾‹æ¥è¯´ï¼Œå¦‚æœä¸€ä¸ªæ°´æœæ˜¯æ©™è‰²çš„ï¼Œåœ†å½¢çš„ï¼Œç›´å¾„çº¦ä¸º 3 è‹±å¯¸ï¼Œé‚£ä¹ˆå®ƒå°±å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ©™å­ã€‚æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨è®¤ä¸ºè¿™äº›â€œç‰¹å¾â€(æ©™è‰²ã€åœ†å½¢ã€ç›´å¾„ 3 è‹±å¯¸)ä¸­çš„æ¯ä¸€ä¸ªéƒ½ç‹¬ç«‹åœ°å½±å“æ°´æœæ˜¯è‹¹æœçš„æ¦‚ç‡ï¼Œè€Œä¸è€ƒè™‘ç‰¹å¾ä¹‹é—´çš„ä»»ä½•ç›¸å…³æ€§ã€‚ç„¶è€Œï¼Œç‰¹å¾å¹¶ä¸æ€»æ˜¯ç‹¬ç«‹çš„ï¼Œè¿™é€šå¸¸è¢«è§†ä¸ºæœ´ç´ è´å¶æ–¯ç®—æ³•çš„ç¼ºç‚¹ï¼Œè¿™ä¹Ÿæ˜¯å®ƒè¢«æ ‡è®°ä¸ºâ€œæœ´ç´ â€çš„åŸå› ã€‚ç„¶è€Œï¼Œå®ƒç†è§£ã€æ„å»ºå’Œè®­ç»ƒç›¸å¯¹ç®€å•ï¼Œè€Œä¸”é€Ÿåº¦éå¸¸å¿«ï¼Œè¿™ä½¿å¾—å®ƒæˆä¸ºæƒ…æ„Ÿåˆ†ç±»çš„è‰¯å¥½å€™é€‰ã€‚

è´å¶æ–¯å®šç†æä¾›äº†ä¸€ç§ä» P(c)ï¼ŒP(x)å’Œ P(x|c)è®¡ç®—åéªŒæ¦‚ç‡ P(c|x)çš„æ–¹æ³•ã€‚çœ‹ä¸‹é¢çš„ç­‰å¼:

![](img/7e056be39492af465faf6bf6683b9621.png)

```
- P(c|x) is the posterior probability of class (c, target) given predictor (x, attributes).
- P(c) is the prior probability of class.
- P(x|c) is the likelihood which is the probability of predictor given class.
- P(x) is the prior probability of predictor.
```

## åˆ›å»ºç®¡é“æ¥ä¸€æ­¥ç®¡ç†é¢„å¤„ç†æ­¥éª¤

*   [Scikit Learn](http://scikit-learn.org/stable/) æä¾›äº†ä¸€ä¸ª[ç®¡é“](http://scikit-learn.org/stable/modules/pipeline.html)åŠŸèƒ½ï¼Œå…è®¸æ‚¨å®šä¹‰ä¸€ä¸ªç®¡é“å·¥ä½œæµï¼Œè¯¥å·¥ä½œæµå°†é‡‡å–ä¸Šè¿°æ‰€æœ‰æ­¥éª¤ï¼Œç”šè‡³æ˜¯ä¸€ä¸ªåˆ†ç±»å™¨å’Œç½‘æ ¼æœç´¢å‚æ•°ã€‚
*   ç®¡é“ä½¿ä»£ç æ›´å…·å¯è¯»æ€§ï¼Œå¹¶ä½¿äº¤æ¢ç®¡é“ç‰‡æ®µå˜å¾—å®¹æ˜“(ç®¡é“ç‰‡æ®µå¯ä»¥åŒ…å«å…¶ä»– ML ç®—æ³•å¹¶å°è¯•ä¸åŒçš„é…ç½®)ã€‚
*   ç®¡é“è¿˜å…è®¸å¯¹æ¨¡å‹å·¥ä½œæµè¿›è¡Œäº¤å‰éªŒè¯ã€‚
*   é€šè¿‡ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„æ ·æœ¬æ¥è®­ç»ƒè½¬æ¢å™¨å’Œé¢„æµ‹å™¨ï¼Œç®¡é“è¿˜æœ‰åŠ©äºé¿å…åœ¨äº¤å‰éªŒè¯ä¸­å°†æµ‹è¯•æ•°æ®ä¸­çš„ç»Ÿè®¡æ•°æ®æ³„æ¼åˆ°è®­ç»ƒæ¨¡å‹ä¸­ã€‚

## äº¤å‰éªŒè¯:

*   è®­ç»ƒå¥½æ¨¡å‹çš„æ¨èæ–¹æ³•æ˜¯é¦–å…ˆä½¿ç”¨è®­ç»ƒé›†æœ¬èº«çš„ä¸€éƒ¨åˆ†è¿›è¡Œäº¤å‰éªŒè¯ï¼Œä»¥æ£€æŸ¥æ‚¨æ˜¯å¦ä½¿ç”¨äº†å®¹é‡è¿‡å¤§çš„æ¨¡å‹(å³æ¨¡å‹æ˜¯å¦è¿‡åº¦æ‹Ÿåˆæ•°æ®)ã€‚
*   ä¸ºäº†åŒæ—¶äº¤å‰éªŒè¯å’Œé€‰æ‹©æœ€ä½³å‚æ•°é…ç½®ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† [GridSearchCVã€‚](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)
*   è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾åœ°æµ‹è¯•ä¸åŒçš„è¶…å‚æ•°é…ç½®ï¼Œä¾‹å¦‚ä½¿ç”¨ KFold ç­–ç•¥å°†æ¨¡å‹åˆ†æˆéšæœºéƒ¨åˆ†ï¼Œä»¥ç¡®å®šå®ƒæ˜¯æ³›åŒ–å¾—å¥½è¿˜æ˜¯è¿‡åº¦æ‹Ÿåˆã€‚
*   GridSearchCV å…è®¸æ‚¨ä½¿ç”¨è¦è¿­ä»£çš„è¶…å‚æ•°é…ç½®å€¼å®šä¹‰ ParameterGridã€‚å¯¹æ‰€æœ‰ç»„åˆè¿›è¡Œæµ‹è¯•å’Œè¯„åˆ†ï¼Œå¹¶è¿”å›æœ€ä½³æ¨¡å‹ã€‚
*   å¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œæœ‰ 4 + 2 + 2 ä¸ªå‚æ•°ç»„åˆè¦æµ‹è¯•ï¼Œæœ‰ 10 kfold éªŒè¯ï¼Œå› æ­¤æ¨¡å‹å°†åœ¨éªŒè¯é›†ä¸Šè®­ç»ƒå’Œæµ‹è¯• 8 x 10 = 80 æ¬¡ã€‚

```
*# Run Train Data Through Pipeline analyzer=text_process*
*# uncomment below to train on a larger dataset but it's very slow for a slower machine.**# X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2)*
X_train, X_test, y_train, y_test = train_test_split(data['text'][:5000], data['label'][:5000], test_size=0.2)*# create pipeline*
pipeline = Pipeline([
    ('bow', CountVectorizer(strip_accents='ascii',
                            stop_words='english',
                            lowercase=**True**)),  *# strings to token integer counts*
    ('tfidf', TfidfTransformer()),  *# integer counts to weighted TF-IDF scores*
    ('classifier', MultinomialNB()),  *# train on TF-IDF vectors w/ Naive Bayes classifier*
])*# this is where we define the values for GridSearchCV to iterate over*
parameters = {'bow__ngram_range': [(1, 1), (1, 2)],
              'tfidf__use_idf': (**True**, **False**),
              'classifier__alpha': (1e-2, 1e-3),
             }*# do 10-fold cross validation for each of the 6 possible combinations of the above params*
grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, verbose=1)
grid.fit(X_train,y_train)*# summarize results*
print("**\n**Best Model: **%f** using **%s**" % (grid.best_score_, grid.best_params_))
print('**\n**')
means = grid.cv_results_['mean_test_score']
stds = grid.cv_results_['std_test_score']
params = grid.cv_results_['params']
**for** mean, stdev, param **in** zip(means, stds, params):
    print("Mean: **%f** Stdev:(**%f**) with: **%r**" % (mean, stdev, param))
```

![](img/7b491caf3ffd764a6bf1ece0b359e602.png)

*   æˆ‘ä»¬å‘ GridsearchCV å¯¹è±¡ä¼ é€’äº† 8 ä¸ªå‚æ•°å’Œ 10 ä¸ªç”¨äºäº¤å‰éªŒè¯çš„æŠ˜å å‚æ•°ï¼Œè¿™æ„å‘³ç€å¯¹äºæ¯ä¸ªå‚æ•°ç»„åˆï¼Œç½‘æ ¼å°†è¿è¡Œ 10 æ¬¡ä¸åŒçš„è¿­ä»£ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„æµ‹è¯•é›†ã€‚
*   åœ¨å°è¯•äº†ä¸åŒçš„æ¨¡å‹å‚æ•°ç»„åˆä¹‹åï¼ŒGridsearchCV è¿”å›äº†æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å¯¹æ–°çš„(twitter)æ•°æ®è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å°†ä¸‹é¢çš„æ¨¡å‹ä¿å­˜åˆ°å·¥ä½œç›®å½•ä¸­ï¼Œä»¥ä¾¿å°†æ¥åœ¨ä¸é‡æ–°è®­ç»ƒå®ƒçš„æƒ…å†µä¸‹æ£€ç´¢è®­ç»ƒè¿‡çš„æ¨¡å‹ã€‚
*   å¦‚æœæ‚¨è®¡åˆ’å°†æ¨¡å‹éƒ¨ç½²åˆ°å…¶ä»–åœ°æ–¹ï¼Œæ¯”å¦‚ç§»åŠ¨åº”ç”¨ç¨‹åºæˆ– web åº”ç”¨ç¨‹åºï¼Œé‚£ä¹ˆä¿å­˜æ¨¡å‹ä¹Ÿæ˜¯ä¸€ä¸ªå¿…è¦çš„æ­¥éª¤ã€‚

# ç¬¬ 2 éƒ¨åˆ†:æ¨¡å‹è¯„ä¼°

åœ¨ä¸‹é¢çš„ä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨æµ‹è¯•ç»´æŒæ•°æ®é›†ä¸Šæµ‹è¯•æˆ‘ä»¬çš„è®­ç»ƒæ¨¡å‹ã€‚

```
*# save best model to current working directory*
joblib.dump(grid, "twitter_sentiment.pkl")*# load from file and predict using the best configs found in the CV step*
model_NB = joblib.load("twitter_sentiment.pkl" )*# get predictions from best model above*
y_preds = model_NB.predict(X_test)print('accuracy score: ',accuracy_score(y_test, y_preds))
print('**\n**')
print('confusion matrix: **\n**',confusion_matrix(y_test,y_preds))
print('**\n**')
print(classification_report(y_test, y_preds))
```

![](img/30804d83f85af4df98f005ea70d6c107.png)

*   åœ¨ä¸Šé¢çš„å•å…ƒæ ¼ä¸­ï¼Œæˆ‘ä½¿ç”¨äº†æœ€ä½³æ¨¡å‹æ¥å¯¹çœ‹ä¸è§çš„æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ï¼Œè¿™è®©æˆ‘ä»¬å¯ä»¥å¯¹æ€§èƒ½æŒ‡æ ‡è¿›è¡Œåˆ†çº§å’Œæ£€ç´¢ã€‚
*   æˆ‘ä»¬å¯ä»¥ä»ä¸Šé¢å¾—åˆ°çš„ä¸€äº›æ€§èƒ½æŒ‡æ ‡åŒ…æ‹¬åˆ†ç±»æŠ¥å‘Šå’Œæ··æ·†çŸ©é˜µã€‚
*   ä¸€ä¸ª[æ··æ·†çŸ©é˜µ(CM)](https://en.wikipedia.org/wiki/Confusion_matrix) éå¸¸ç®€å•æ˜äº†ï¼Œä½†ä¸‹é¢æ˜¯è§£é‡Šå®ƒä»¥åŠå¾—å‡ºåˆ†ç±»æŠ¥å‘Šçš„å…³é”®:

![](img/4f260a0da2054fb8b302c5722b2d574e.png)

**æ ¹æ®ä¸Šé¢çš„ç²¾åº¦æŒ‡æ ‡:**

*   95%æ˜¯æ¨¡å‹åœ¨æ•°æ®é›†ä¸­æ‰€æœ‰æ ‡ç­¾ä¸­é¢„æµ‹æ­£ç¡®æ ‡ç­¾çš„æ¬¡æ•°ã€‚
*   å½“æ•°æ®ä¸­ç±»çš„åˆ†å¸ƒéå¸¸å‡è¡¡æ—¶ï¼Œå‡†ç¡®æ€§å¯ä»¥è®©æ‚¨å¾ˆå¥½åœ°äº†è§£æ¨¡å‹çš„æ‰§è¡Œæƒ…å†µã€‚

**æ¥è‡ªæ··æ·†çŸ©é˜µ:**

*   è¯¥æ¨¡å‹å°† 553 ä¸ªæ ‡ç­¾æ­£ç¡®é¢„æµ‹ä¸ºé˜´æ€§ï¼Œå°† 799 ä¸ªæ ‡ç­¾æ­£ç¡®é¢„æµ‹ä¸ºé˜³æ€§ã€‚
*   æˆ‘ä»¬è¿˜å¾—åˆ° 4 ä¸ªé¢„æµ‹ä¸ºé˜³æ€§çš„æ ‡ç­¾ï¼Œå°½ç®¡å®ƒä»¬æ˜¯é˜´æ€§çš„(å‡é˜´æ€§)ã€‚
*   ä» CM ä¸­æˆ‘ä»¬å¯ä»¥çŸ¥é“çš„å¦ä¸€ä»¶äº‹æ˜¯ï¼Œæ¨¡å‹é¢„æµ‹ 62 ä¸ªæ ‡ç­¾ä¸ºé˜´æ€§ï¼Œä½†å®ƒä»¬ç»“æœæ˜¯é˜³æ€§(å‡é˜³æ€§)ã€‚

**æ¥è‡ªåˆ†ç±»æŠ¥å‘Š:**

å¯ä»¥ä»æ··æ·†çŸ©é˜µä¸­çš„åº¦é‡è·å¾—çš„åˆ†ç±»æŠ¥å‘Šç»™å‡ºäº†å…³äºæ¨¡å‹æ€§èƒ½çš„æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚

*   **ç²¾åº¦**:æ ‡ç­¾ 0 ä¸º 99%ï¼Œæ ‡ç­¾ 1 ä¸º 93%ã€‚è¿™ä¸ªæ•°å­—å‘Šè¯‰æˆ‘ä»¬åœ¨è¯¥ç±»åˆ«çš„æ‰€æœ‰é¢„æµ‹ä¸­ï¼Œæ­£ç¡®é¢„æµ‹çš„æ ‡ç­¾æ‰€å çš„æ¯”ä¾‹ã€‚
*   **å›å¿†**:æ ‡ç­¾ 0 ä¸º 90%ï¼Œæ ‡ç­¾ 1 ä¸º 100%ã€‚è¿™æ˜¯è¯¥ç±»åˆ«çš„çœŸå®æ ‡ç­¾ä¸­æ­£ç¡®é¢„æµ‹çš„æ•°é‡ã€‚
*   **f1 -score** :è¿™æ˜¯è¯¥ç±»çš„ç²¾åº¦å’Œå¬å›ç‡çš„åŠ æƒå¹³å‡å€¼ã€‚å®ƒé€šå¸¸ç»™å‡ºè¯¥æ ‡ç­¾çš„æ¨¡å‹è¡¨ç°çš„æ›´å¤§ç”»é¢ï¼Œå¹¶ä¸”æ˜¾ç„¶è¯¥æ•°å­—è¶Šé«˜è¶Šå¥½ã€‚æ ‡ç­¾ 0 ä¸º 94 %ï¼Œæ ‡ç­¾ 1 ä¸º 96%ã€‚0 è¡¨ç¤ºæ¶ˆææƒ…ç»ªï¼Œ1 è¡¨ç¤ºç§¯ææƒ…ç»ªã€‚
*   ä»ä¸Šè¿°æŒ‡æ ‡æ¥çœ‹ï¼Œè¯¥æ¨¡å‹ä¼¼ä¹è¡¨ç°å¾—ç›¸å¯¹è¾ƒå¥½ï¼Œå°½ç®¡å®ƒåœ¨é¢„æµ‹ç±» 0 çš„æ­£ç¡®æ ‡ç­¾æ–¹é¢å¯ä»¥åšå¾—æ›´å¥½ã€‚
*   æ ¹æ®ç ”ç©¶ï¼Œç”±äº[è¯„åˆ†è€…ä¹‹é—´çš„å¯é æ€§é—®é¢˜](https://en.wikipedia.org/wiki/Inter-rater_reliability)ï¼Œäººç±»è¯„åˆ†è€…é€šå¸¸åŒæ„ 80%çš„æ—¶é—´ã€Roebuckï¼Œk .(2012â€“10â€“24ã€‘ã€‘ã€‚
*   å› æ­¤ï¼Œä¸€ä¸ª 70%å‡†ç¡®çš„ç¨‹åºåšå¾—å‡ ä¹å’Œäººç±»ä¸€æ ·å¥½ï¼Œå³ä½¿è¿™æ ·çš„å‡†ç¡®æ€§å¯èƒ½çœ‹èµ·æ¥å¹¶ä¸é‚£ä¹ˆä»¤äººå°è±¡æ·±åˆ»ã€‚å¦‚æœä¸€ä¸ªç¨‹åºåœ¨ 100%çš„æ—¶é—´é‡Œéƒ½æ˜¯â€œæ­£ç¡®çš„â€ï¼Œäººç±»ä»ç„¶ä¼šåœ¨ 20%çš„æ—¶é—´é‡Œä¸åŒæ„å®ƒï¼Œå› ä¸ºä»–ä»¬å¯¹ä»»ä½•ç­”æ¡ˆéƒ½ä¸åŒæ„ã€‚
*   æˆ‘ä»¬è¿˜å¯ä»¥å°†æˆ‘ä»¬çš„æ¨¡å‹ä¸ç±»ä¼¼çš„å·¥ä½œè¿›è¡Œæ¯”è¾ƒï¼Œè¿™äº›å·¥ä½œå¯¹æƒ…æ„Ÿåˆ†æå’Œæƒ…æ„Ÿæ ‡ç­¾çš„äºŒå…ƒåˆ†ç±»è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ã€‚æœ€ç»ˆçš„æ¨¡å‹æ˜¯åŸºäºæ”¯æŒå‘é‡æœº( [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) )å¹¶è¿›è€Œè¾¾åˆ° 79.08%çš„å‡†ç¡®ç‡ã€‚
*   åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»é‡‡å–æªæ–½æ¥é¿å…è¿‡åº¦æ‹Ÿåˆæˆ‘ä»¬å»ºç«‹æ¨¡å‹ç®¡é“çš„æ–¹å¼ï¼Œä½†æœ€ç»ˆçœŸæ­£çš„æµ‹è¯•æ˜¯åœ¨çœ‹ä¸è§çš„æ•°æ®ä¸Šæµ‹è¯•æ¨¡å‹ã€‚

# ç¬¬ 3 éƒ¨åˆ†:çœŸå®ä¸–ç•Œçš„æ¨¡å‹æ€§èƒ½

ä¸‹ä¸€æ­¥ï¼Œæˆ‘æƒ³ç”¨æˆ‘ä»¬çš„æ¨¡å‹å¯¹é€šè¿‡ twitter API è·å¾—çš„æ•°æ®è¿›è¡Œæƒ…æ„Ÿé¢„æµ‹ï¼Œå¹¶è¯„ä¼°å®ƒçš„æ€§èƒ½ã€‚é¢„æµ‹å’Œé¢„è§ˆç»“æœæ˜¯åœ¨ä¸‹é¢çš„ä»£ç ä¸­å®Œæˆçš„ã€‚

```
*# run predictions on twitter data*
tweet_preds = model_NB.predict(df_twtr['message'])*# append predictions to dataframe*
df_tweet_preds = df_twtr.copy()
df_tweet_preds['predictions'] = tweet_preds
df_tweet_preds.shape# Output
(4164, 8)
```

æŸ¥çœ‹ä¸‹é¢çš„ä¸€äº›ç¤ºä¾‹é¢„æµ‹ã€‚

0 =æ­£ï¼Œ1 =è´Ÿã€‚

```
*# print text and sentiment*index = random.sample(range(tweet_preds.shape[0]), 20)
**for** text, sentiment **in** zip(df_tweet_preds.message[index],
                           df_tweet_preds.predictions[index]):
    print (sentiment, '--', text, '**\n**')#------------------------------
# SAMPLE PREDICTIONS BELOW    #
#------------------------------0 -- Spot on correct. The Paul Ryan Story: From Flimflam to Fascism [https://t.co/BPwrobl0aS](https://t.co/BPwrobl0aS)0 -- .@Varneyco @charleshurt   Paul Ryan didn't get Tax Reform.  Donald Trump got Tax Reform.  Boy George could have beeâ€¦ [https://t.co/vGmRJWQnlG](https://t.co/vGmRJWQnlG)1 -- @HumanBeings1st @LotraineH Paul Ryan is cold-heartedğŸ‘ğŸ¼ğŸ‘¹He hates Social Security and Medicare and Medicaid and has tâ€¦ [https://t.co/oQAXvfEuLI](https://t.co/oQAXvfEuLI)0 -- Paul Ryan is an unconvincing charlatan whose primary goal in life has been to make life harder for poor people. Theâ€¦ [https://t.co/yyK80Tsgmz](https://t.co/yyK80Tsgmz)0 -- absolutely delish...#winningMeghan McCain Erupts After Audience Cheers Paul Ryan Retirement: â€˜You Deserve Trumpâ€™ [https://t.co/7s2AUVGrgp](https://t.co/7s2AUVGrgp)0 -- Is anyone surprised Paul Ryan isnâ€™t going to run? #morningjoe0 -- @CNN If the Repubs stand by and continue to let the lunatic run the asylum, then they are accountable, if the Repubâ€¦ [https://t.co/uNNkd9yljp](https://t.co/uNNkd9yljp)0 -- .@IronStache wins eight months early [https://t.co/7txWs0YvER](https://t.co/7txWs0YvER)
```

ä¸‹é¢ï¼Œæˆ‘åœ¨äº’è”ç½‘ä¸Šçš„ä¸€äº›æ–‡æœ¬ä¸Šæµ‹è¯•äº†è¿™ä¸ªæ¨¡å‹ï¼Œä»¥æ›´å¥½åœ°äº†è§£å®ƒåœ¨ä¸€äº›éšæœºæ•°æ®ä¸Šçš„è¡¨ç°ã€‚

```
*# Testing random text from the internet**# load model*
model_NB = joblib.load("twitter_sentiment.pkl" )*# test string*
sample_str = """While ride-sharing first mover Uber has fallen on tough times with
scandal and abyssal track records of leadership, and cash burning
growth-orientated practices, the world has caught up with self-driving
tech with many players now in the race."""p = model_NB.predict([sample_str])*# formatting helper*
**def** sentiment_str(x):
    **if** x==0:
        **return** 'Negative'
    **else**:
        **return** 'Positive'
*#_____________________________________________**# test result ___ 0=Negative, 1=Positive*
print("the sentence: **\n\n**'**{}**' **\n\n**has a **{}** sentiment".format(sample_str,sentiment_str(p[0])))
```

![](img/f6d1eddf91d5009901516e1f03796329.png)

# è§‚å¯Ÿ

*   æˆ‘åœ¨ä¸Šé¢çš„å•å…ƒæ ¼ä¸­å¯è§†åŒ–äº†ç”±æˆ‘ä»¬çš„æ¨¡å‹åˆ†ç±»çš„ 10 ä¸ª tweets ç¤ºä¾‹ï¼Œè¿™äº›ç¤ºä¾‹çœ‹èµ·æ¥æ‰§è¡Œå¾—åŸºæœ¬æ­£ç¡®ï¼Œä½†æœ‰æ—¶ä¼šä»¤äººå°´å°¬åœ°å¤±è´¥ã€‚
*   å½“åœ¨é twitter æ–‡æœ¬ä¸Šæµ‹è¯•æ—¶ï¼Œè¯¥æ¨¡å‹ä¼¼ä¹æ‹¾å–äº†è®¸å¤šåµŒå…¥çš„æƒ…æ„Ÿï¼Œä½†éœ€è¦æ›´å¤šå·¥ä½œæ¥ä»æ‰€è¿°æ–‡æœ¬ä¸­æå–ä¸€èˆ¬ç»“è®ºã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦å¯¹æ•´ä¸ªæ®µè½è¿›è¡Œè®­ç»ƒï¼Œè€Œä¸ä»…ä»…æ˜¯ä¸€å¥è¯ã€‚
*   å…³äºæƒ…æ„Ÿåˆ†æçš„ä¸€ä¸ªæ£˜æ‰‹çš„é—®é¢˜æ˜¯ï¼Œå³ä½¿è¯¥æ¨¡å‹åœ¨ä¸€äº›åŸºå‡†ä¸Šè¡¨ç°è‰¯å¥½ï¼Œæœ€ç»ˆçš„è¡¨ç°ä¹Ÿå°†ä¸å¾—ä¸ç•™ç»™éš¾ä»¥é‡åŒ–çš„å®é™…äººç±»æ³•å®˜ã€‚
*   è¯è™½å¦‚æ­¤ï¼Œæˆ‘æŸ¥çœ‹äº†è®¸å¤šéšæœºçš„è´Ÿé¢åˆ†ç±»å’Œæ­£é¢åˆ†ç±»çš„æ¨æ–‡ï¼Œæœ‰ä¸€äº›éå¸¸å¥½çš„åˆ†ç±»ï¼Œè¿˜æœ‰ä¸€äº›åˆ†ç±»ä¼¼ä¹ä¸å±äºä»»ä½•åœ°æ–¹ã€‚è¿™ä¹Ÿè®©æˆ‘è§‰å¾—æˆ‘ä»¬åº”è¯¥æ¢ç´¢ç¬¬ä¸‰ç§åˆ†ç±»ã€‚æ‰€ä»¥æˆ‘ä»¬ä¼šæœ‰**ç§¯æã€æ¶ˆæå’Œä¸­æ€§çš„æƒ…ç»ª**ã€‚
*   è™½ç„¶æˆ‘è®¤ä¸ºæ¨¡å‹å·²ç»å®Œæˆäº†ä¸€äº›å­¦ä¹ ï¼Œä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒæ•´æ¨¡å‹çš„å‚æ•°ã€æ¢ç´¢å…¶ä»–ç®—æ³•æˆ–åˆ›å»ºç®—æ³•é›†åˆæ¥åšå¾—æ›´å¥½ã€‚

æ¯”è¾ƒ twitter æ•°æ®ä¸Šçš„æ¨¡å‹é¢„æµ‹ç»Ÿè®¡

```
pos = df_tweet_preds.predictions.value_counts()[0]
neg = df_tweet_preds.predictions.value_counts()[1]print('Model predictions: Positives - **{}**, Negatives - **{}**'.format(neg,pos))*# save dataframe with appended preditions* 
df_tweet_preds.to_pickle('paulry_predicts_df.p')
```

![](img/ee89a938b43cb396a69c0a3bdeb04fb6.png)

# ç¬¬ 7 éƒ¨åˆ†:åœ°è´¨å›¾ã€æœ€ç»ˆæƒ³æ³•å’Œç»“è®º

*   åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸‹è½½æ¨æ–‡çš„åœ°ç†å¯è§†åŒ–ï¼Œè¿™æ˜¯æ•°æ®æ•…äº‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚
*   åœ¨è®­ç»ƒä»»ä½• ML æ¨¡å‹æ—¶ï¼Œä¸€ä¸ªé‡è¦çš„éƒ¨åˆ†æ˜¯èƒ½å¤Ÿä»¥ä¿ƒè¿›åˆ©ç›Šç›¸å…³è€…å†³ç­–çš„æ–¹å¼å‘ˆç°æˆ‘ä»¬çš„å‘ç°ã€‚
*   æˆ‘ä»¬å°†åŠ è½½åŒ…å«é¢„æµ‹çš„æ•°æ®æ¡†æ¶ï¼Œå¹¶æ¢ç´¢å›½å®¶å’Œç¾å›½å„å·çš„åˆ†å¸ƒï¼Œç‰¹åˆ«æ˜¯å› ä¸ºæˆ‘ä»¬çš„ twitter æ•°æ®å›´ç»•ç€ç¾å›½æ–°é—»ã€‚
*   å€¼å¾—ä¸€æçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†åœ¨å‘å±•è¿‡ç¨‹ä¸­å­˜åœ¨ä¸€äº›é™åˆ¶ã€‚å¤§å¤šæ•° twitter ç”¨æˆ·ä¸å¹¿æ’­ä»–ä»¬çš„åœ°ç†ä½ç½®ï¼Œä½†æˆ‘ä»¬çš„æœç´¢æ ‡å‡†åªæå–äº†å¯ç”¨äº†ä½ç½®ä¿¡æ¯çš„æ¨æ–‡ã€‚
*   å› æ­¤ï¼Œæˆ‘ä»¬å¯èƒ½é”™è¿‡äº†å¾ˆå¤šæ¨æ–‡ï¼Œè¿™äº›æ¨æ–‡æç»˜çš„ç”»é¢å¯èƒ½ä¸æˆ‘ä»¬åœ¨åœ°å›¾ä¸Šç»˜åˆ¶çš„ç”»é¢ä¸åŒã€‚äº†è§£è¿™ä¸€ç‚¹ä¸ä»…å¯¹è§£é‡Šç»“æœè‡³å…³é‡è¦ï¼Œè€Œä¸”å¯¹ç†è§£æˆ‘ä»¬å¦‚ä½•ä½¿æ¨¡å‹æ›´åŠ ç¨³å¥ä¹Ÿè‡³å…³é‡è¦ã€‚

```
*# load dataframe with predictions*
df = pd.read_pickle('paulry_predicts_df.p')*# get all the countries in dataset*
df.country.unique()
```

![](img/1a082f1ae4e23a99859e9b7e203489df.png)

```
*# get top countries in the dataset by percentage of tweets*
df.country.value_counts(1).head(10)
```

![](img/cb5a029ad27220fec27e52d146c5eb01.png)

åœ¨[132]:

```
*# plot histogram of tweets counts by country of origin*
sns.set_style("darkgrid")
x = df.country.value_counts(1).head(20)
x.plot(kind='bar',figsize=(10,6),fontsize=13,color='steelblue')
plt.ylabel('**% o**f Total Tweets', fontsize=13)
```

![](img/96d82ec249a2d80c7e18327c179a8d30.png)

*   æ­£å¦‚æˆ‘ä»¬æ‰€æ€€ç–‘çš„ï¼Œç»å¤§å¤šæ•°æ¨æ–‡æ¥è‡ªç¾å›½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€šè¿‡çŠ¶æ€è®¡æ•°æ¥è·å–çŠ¶æ€ã€‚

```
*# get latitudes and longitudes**# some helper funtions to get longs and lats*
**def** lats(x):
    **return** x[1]**def** longs(x):
    **return** x[0]*# --------------------------------------------------------#*
*# append longs and lats to dframe*
df['latitude'] = df['geo_code'].apply(lats)
df['longitude'] = df['geo_code'].apply(longs)
df.columns
```

![](img/ee5d6982b39dcf920380e2e73b56eee9.png)

```
*# for US tweets extract state abbreviations for a new STATE column*
*# helper function to extract state origin of every tweet*
**def** get_state(x):

    states = ["AL", "AK", "AZ", "AR", "CA", "CO", "CT", "DC", "DE", "FL", "GA", 
              "HI", "ID", "IL", "IN", "IA", "KS", "KY", "LA", "ME", "MD", 
              "MA", "MI", "MN", "MS", "MO", "MT", "NE", "NV", "NH", "NJ", 
              "NM", "NY", "NC", "ND", "OH", "OK", "OR", "PA", "RI", "SC", 
              "SD", "TN", "TX", "UT", "VT", "VA", "WA", "WV", "WI", "WY"]states_dict = {
            'AK': 'Alaska','AL': 'Alabama','AR': 'Arkansas','AS': 'American Samoa',
            'AZ': 'Arizona','CA': 'California','CO': 'Colorado','CT': 'Connecticut',
            'DC': 'District of Columbia','DE': 'Delaware','FL': 'Florida','GA': 'Georgia',
            'GU': 'Guam','HI': 'Hawaii','IA': 'Iowa','ID': 'Idaho','IL': 'Illinois',
            'IN': 'Indiana','KS': 'Kansas','KY': 'Kentucky','LA': 'Louisiana',
            'MA': 'Massachusetts','MD': 'Maryland','ME': 'Maine','MI': 'Michigan',
            'MN': 'Minnesota','MO': 'Missouri','MP': 'Northern Mariana Islands',
            'MS': 'Mississippi','MT': 'Montana','NA': 'National','NC': 'North Carolina',
            'ND': 'North Dakota','NE': 'Nebraska','NH': 'New Hampshire','NJ': 'New Jersey',
            'NM': 'New Mexico','NV': 'Nevada','NY': 'New York','OH': 'Ohio','OK': 'Oklahoma',
            'OR': 'Oregon','PA': 'Pennsylvania','PR': 'Puerto Rico','RI': 'Rhode Island',
            'SC': 'South Carolina','SD': 'South Dakota','TN': 'Tennessee','TX': 'Texas',
            'UT': 'Utah','VA': 'Virginia','VI': 'Virgin Islands','VT': 'Vermont',
            'WA': 'Washington','WI': 'Wisconsin','WV': 'West Virginia','WY': 'Wyoming'
    }abv = x.split(',')[-1].lstrip().upper()
    state_name = x.split(',')[0].lstrip()
    **if** abv **in** states:
        state = abv
    **else**:
        **if** state_name **in** states_dict.values():
            state = list(states_dict.keys())[list(states_dict.values()).index(state_name)]
        **else**:
            state = 'Non_USA'    
    **return** state*# ____________________________________________________________________________**# create abreviated states column*
df = df.copy()
df['states'] = df['full_name'].apply(get_state)
list(df['states'].head())
```

è¾“å‡º:

```
['Non_USA', 'PA', 'FL', 'NY', 'FL']
```

åœ¨ä¸‹é¢åˆ›å»ºä¸€äº›å¯è§†åŒ–æ•ˆæœã€‚

```
*# save updated dataframe*
df.to_pickle('df_paulry_longs_lats.p')*# retrive updated dataframe*
df = pd.read_pickle('df_paulry_longs_lats.p')*# plot tweets distribution by state*plt.style.use('seaborn-darkgrid')
df_states = df[df.country=='United States']
df_states = df_states[df_states.states!='Non_USA']x = df_states.states.value_counts()
x.plot(kind='bar',figsize=(14,6),fontsize=12,color='steelblue')
plt.ylabel('Origin of Tweets', fontsize=13)
```

![](img/94c7d26f8b21a46a60d4070b18fdd8f7.png)

*   ä»ä¸Šé¢æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å¤§å¤šæ•°æ¨æ–‡æ¥è‡ªå“ªé‡Œï¼Œçœ‹èµ·æ¥å¯èƒ½ä¸äººå£æ•°é‡å’Œå·çš„å¤§å°æœ‰å…³ã€‚
*   æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡å°†ç§¯æå’Œæ¶ˆæç›¸åŠ æ¥æå–æ¯ä¸ªçŠ¶æ€çš„æ€»ä½“æƒ…ç»ªï¼Œæœ€åçš„æ•°å­—å°±æ˜¯æŒ‡æ ‡ã€‚
*   æˆ‘ä»¬å°†ä½¿ç”¨æ¯ä¸ªå·çš„æ€»ä½“æƒ…ç»ªæ¥åˆ›å»ºä¸€ä¸ªçƒ­å›¾ï¼Œæ˜¾ç¤ºä»æœ€æ¶ˆæåˆ°æœ€ç§¯æçš„çŠ¶æ€ã€‚

![](img/ee0f02b38cd83dd8c12d40d84cd96f32.png)

```
*# Create the sentiment by state Heat Map*colorscale=[
            [0, 'rgb(31,120,180)'], 
            [0.35, 'rgb(166, 206, 227)'], 
            [0.75, 'rgb(251,154,153)'], 
            [1, 'rgb(227,26,28)']
           ]data = dict(type='choropleth',
            colorscale = colorscale,
            reversescale=**True**,
            locations = df_state_sentiment['states'],
            z = df_state_sentiment['total_sentiment'],
            locationmode = 'USA-states',
            text = df_state_sentiment['states'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Twitter Sentiment"}
            )layout = dict(title = 'Twitter Sentiment: GOP House Speaker: Paul Ryan',
              geo = dict(scope='usa'
                        )
             )choromap_us = go.Figure(data = [data],layout = layout)*# plotly.offline.plot(choromap_us, filename='img_map.html')  # save html map*
IFrame('img_map.html', width=950, height=700)  *# view saved map html file*
```

![](img/6a4b635ac61a75bd41d93fdf15200e5f.png)

*   æ€»æƒ…ç»ªæ˜¯é€šè¿‡å¯¹æ¯ä¸ªå·çš„ç§¯æå’Œæ¶ˆæå› ç´ æ±‚å’Œå¾—å‡ºæœ€ç»ˆæ•°å­—ã€‚
*   ä»ä¸Šé¢çš„çƒ­å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åšå‡ºä¸€äº›æœ‰è¶£çš„è§‚å¯Ÿã€‚
*   ä»åœ°å›¾å³ä¾§çš„è‰²æ ‡æ¥çœ‹ï¼Œè“è‰²è¿‡æ¸¡åˆ°çº¢è‰²ï¼Œè¡¨ç¤ºç§¯æåˆ°æ¶ˆæçš„æƒ…ç»ªã€‚è¯¥é‡è¡¨å®é™…ä¸Šæ˜¯ä» 0 å¼€å§‹çš„ï¼Œå› ä¸ºæ²¡æœ‰ä¸€ä¸ªå·çš„æ€»ä½“æƒ…ç»ªæ˜¯ç§¯æçš„ã€‚æ‰€ä»¥æˆ‘ä»¬èµ¤å­—è¶Šå¤šï¼Œæƒ…ç»ªå°±è¶Šæ¶ˆæã€‚
*   çº½çº¦å’ŒåŠ å·å¯¹è¿™ä¸ªå…³é”®è¯æœç´¢çš„è´Ÿé¢æƒ…ç»ªæœ€å¤§ã€‚è¯·è®°ä½ï¼Œçº½çº¦å’ŒåŠ å·çš„æ¨ç‰¹ç”¨æˆ·æœ€å¤šï¼Œè¿™å¾ˆå¯èƒ½ä¼šæ‰­æ›²ä¸Šé¢çš„å›¾ç‰‡ã€‚
*   å› æ­¤ï¼Œæˆ‘è§‰å¾—è¿™å¼ çƒ­å›¾ä¸å¤ªå¯é ï¼Œä½†å®ƒæ˜¯æ¢ç´¢ ***æƒ…ç»ª vs***çŠ¶æ€å¯è§†åŒ–çš„è‰¯å¥½èµ·ç‚¹ã€‚
*   æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦ä¸€ä¸ªçƒ­å›¾ï¼Œæˆ‘ä»¬å°†æ ¹æ®å„è‡ªçš„åœ°ç†ä½ç½®æ•°æ®ç»˜åˆ¶æ•°æ®é›†ä¸­çš„æ¯æ¡æ¨æ–‡ã€‚

```
*# use the folium library to create all tweet origins in the dataset on map of US*geoplots = []
**for** index, row **in** df_states[['latitude','longitude','predictions']].iterrows():
    geoplots.append([row['latitude'],row['longitude'],row['predictions']])mus = folium.Map(location=[39, -99], zoom_start=4)
plugins.Fullscreen(
    position='topright',
    title='Expand me',
    title_cancel='Exit me',
    force_separate_button=**True**).add_to(mus)mus.choropleth(
    geo_data='us_states.geojson',
    fill_color='red', 
    fill_opacity=0.1, 
    line_opacity=0.2,
    name='US States')

mus.add_child(plugins.HeatMap(geoplots,
                            name='Twitter HeatMap',
                            radius=10,
                            max_zoom=1,
                            blur=10, 
                            max_val=3.0))
folium.TileLayer('cartodbpositron').add_to(mus)
folium.TileLayer('cartodbdark_matter').add_to(mus)
folium.TileLayer('Mapbox Control Room').add_to(mus)
folium.LayerControl().add_to(mus)
mus.save("twitter_us_map.html") 
IFrame('twitter_us_map.html', width=960, height=520)
```

![](img/d1242654e1ba0b977e295f440ac7f1cc.png)

*   ä¸Šé¢çš„ç¾å›½åœ°å›¾æ˜¾ç¤ºï¼Œè®¸å¤šæ¨æ–‡æ¥è‡ªè¿™äº›äººå£å¯†åº¦é«˜çš„åœ°åŒºã€‚
*   å……å…¶é‡ï¼Œåœ°å›¾ç»™æˆ‘ä»¬æä¾›äº†ä¸€ç§æ–¹å¼æ¥æŸ¥çœ‹æ¨æ–‡æ¥è‡ªå“ªé‡Œï¼Œä¸‹ä¸€æ­¥å°†æ ¹æ®å®ƒä»¬çš„æƒ…æ„Ÿå€¼æ¥æ˜ å°„å®ƒä»¬ï¼Œä»¥ä¾¿æ›´å¥½åœ°è¡¨è¾¾æƒ…æ„Ÿã€‚
*   æˆ‘ä»¬è¿˜å¯ä»¥åœ¨åœ°å›¾ä¸Šæ·»åŠ ä¸€ä¸ªæ—¶é—´ç»´åº¦ï¼Œä»¥å¯è§†åŒ–æ¨æ–‡ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ—¶é—´åºåˆ—ä¸­å®æ—¶å‘ç”Ÿï¼Œè¿™å°†æ˜¯ä¸€ä¸ªè·Ÿè¸ªç›¸å…³æ–¹æ„Ÿå…´è¶£çš„ä¸»é¢˜çš„ä¼Ÿå¤§å·¥å…·ã€‚è¿™ç§å·¥å…·ä»¥å‰åœ¨æ€»ç»Ÿç«é€‰å’Œè‡ªç„¶ç¾å®³ä¸­ä½¿ç”¨è¿‡ã€‚

# ç»“è®º

*   åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå¯¹äºæƒ…ç»ªåˆ†ææ¨¡å‹æ¥è¯´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°å¾—ç›¸å¯¹è¾ƒå¥½ï¼Œå…¶*å‡†ç¡®ç‡ä¸º 76%* ï¼Œä½†æˆ‘ä»¬è¿˜å¯ä»¥åšå¾ˆå¤šäº‹æƒ…æ¥æé«˜å¯¹è¿™ç§è¡¨ç°çš„ä¿¡å¿ƒã€‚
*   æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„åŸºæœ¬æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡è¯•éªŒä»¥ä¸‹å»ºè®®æ¥æé«˜å…¶æ€§èƒ½:
*   å¯ä»¥å¯¹è®­ç»ƒè¯­æ–™åº“æ‰§è¡Œé¢å¤–çš„æ•°æ®å½’ä¸€åŒ–ã€‚æˆ‘ä»¬ä»è®­ç»ƒæ•°æ®ä¸­ç§»é™¤äº†åœç”¨è¯ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥å°è¯•ä½¿ç”¨[è¯å¹²](https://en.wikipedia.org/wiki/Stemming)å’Œ[è¯æ±‡åŒ–](http://nbviewer.jupyter.org/github/RonKG/machine-learning-portfolio-projects/blob/master/3.%20NLP_twitter_sentiment_analysis/FINAL____twitter_sentiment_twitter.ipynb)ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ˜¯åŸºäºè¯æ ¹æˆ–åŸºæœ¬è¯æ¥åˆ†æè¯­æ–™åº“ä¸­çš„å•è¯ã€‚
*   æˆ‘ä»¬è¿˜å¯ä»¥å°è¯•ä¸åŒçš„å•è¯æ ‡è®°æ–¹æ³•ï¼Œç‰¹åˆ«æ³¨æ„å¤§å†™å•è¯å’Œç‰¹æ®Šå­—ç¬¦ã€‚
*   è¡¨æƒ…ç¬¦å·ä¹Ÿå¯ä»¥ç”¨æ¥æå–æ›´å¤šçš„æ–‡æœ¬å†…å®¹ã€‚
*   å°è¯•ä½¿ç”¨å¤šç±»æ ‡ç­¾æ¥å®¹çº³æ‰€æœ‰å¯ä»¥é€šè¿‡æ–‡æœ¬ä¼ è¾¾çš„äººç±»æƒ…æ„Ÿã€‚*ä¾‹å¦‚:æ‚²ä¼¤ã€å¿«ä¹ã€å…´å¥‹ã€åŒçƒ¦ç­‰..*
*   è€ƒè™‘å…¶ä»– ML ç®—æ³•ï¼Œå¦‚æ”¯æŒå‘é‡æœºã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œç­‰ï¼Œç”šè‡³è¿™äº›ç®—æ³•çš„é›†åˆã€‚
*   æœ€åï¼Œæˆ‘æƒ³çŸ¥é“åœ¨ä¸åŒçš„ä¸»é¢˜ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹å¹¶ä¸ºä¸åŒçš„æ•°æ®é›†é€‰æ‹©æœ€ç›¸å…³çš„ä¸€ä¸ªæ˜¯å¦æœ‰ç›Šã€‚ä¾‹å¦‚ï¼Œåœ¨å¹¿æ³›çš„æ³•å¾‹æ–‡æ¡£åº“ä¸Šè®­ç»ƒçš„ç”¨äº*æ³•å¾‹*çš„æ¨¡å‹ï¼Œæˆ–è€…åœ¨ä½“è‚²ç›¸å…³æ–‡æœ¬ä¸Šè®­ç»ƒçš„ç”¨äº*ä½“è‚²*çš„æ¨¡å‹ï¼Œæ¯”å¦‚ä¸ä½“è‚²å’Œä½“è‚²åäººç›¸å…³è”çš„æ¨æ–‡ã€‚å½“ç„¶ï¼Œå¯¹äºå®éªŒå’Œéšåçš„è¯„ä¼°æ¥è¯´ï¼Œè¿™æ–¹é¢çš„åŸºç¡€æ— ç–‘æ˜¯ä¸°å¯Œçš„ã€‚

è¿™ç¯‡æ–‡ç« çš„ä»£ç å’Œæ›´å¤šå†…å®¹å¯ä»¥åœ¨æˆ‘çš„ [GitHub](https://github.com/RonKG/machine-learning-projects-2/tree/master/3.%20NLP_twitter_sentiment_analysis) è·å¾—ã€‚