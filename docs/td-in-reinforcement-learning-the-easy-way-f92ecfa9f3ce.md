# TD åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæœ€ç®€å•çš„æ–¹æ³•

> åŸæ–‡ï¼š<https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce?source=collection_archive---------4----------------------->

æ›´æ–°:å­¦ä¹ å’Œç»ƒä¹  TD æ–¹æ³•çš„æœ€å¥½æ–¹å¼æ˜¯å»[http://rl-lab.com/gridworld-td](http://rl-lab.com/gridworld-td)

å‡è®¾ä½ æ­£åœ¨é©¾é©¶è£…æœ‰ GPS çš„æ±½è½¦ã€‚åœ¨æ—…ç¨‹å¼€å§‹æ—¶ï¼ŒGPS ä¼šç»™ä½ ä¸€ä¸ªåˆ°è¾¾æ—¶é—´çš„ä¼°è®¡å€¼(åŸºäºç»Ÿè®¡æ•°æ®)ï¼Œå½“ä½ å¼€è½¦é‡åˆ°äº¤é€šå µå¡æ—¶(æˆ–è€…æ²¡æœ‰)ï¼Œå®ƒä¼šæ”¹è¿›ä¼°è®¡å€¼å¹¶ç»™ä½ å…¶ä»–åˆ°è¾¾æ—¶é—´ã€‚

æ‚¨ä¼šæ³¨æ„åˆ°ï¼Œåœ¨æ—…ç¨‹çš„æ¯ä¸€æ®µï¼Œæ‚¨éƒ½ä¼šå¾—åˆ°ä¸€äº›å…³äºåˆ°è¾¾æ—¶é—´çš„ä¼°è®¡ã€‚

ç°åœ¨å‡è®¾ä½ çš„å…¨çƒå®šä½ç³»ç»Ÿæ²¡æœ‰ç»™ä½ ä»»ä½•ä¼°è®¡ï¼Œä½†å­˜å‚¨çš„æ•°æ®ï¼Œç›´åˆ°ä½ åˆ°è¾¾ï¼Œç„¶åç»™ä½ ä¸€ä¸ªè¯¦ç»†çš„æŠ¥å‘Šï¼Œæ¯æ®µè·¯èŠ±äº†å¤šå°‘æ—¶é—´ã€‚è¿™å¯¹ä½ æœ‰ç”¨å—ï¼Ÿ

ç­”æ¡ˆä¼šæ˜¯:è¿™å–å†³äºä½ æƒ³åšä»€ä¹ˆã€‚
ä½†è‚¯å®šçš„æ˜¯ï¼Œä½ ä¼šæ„Ÿè°¢æ—©æœŸçš„åé¦ˆï¼Œå³ä½¿ä¸æ˜¯å¾ˆå‡†ç¡®ã€‚

è¿™å°±æ˜¯è’™ç‰¹å¡ç½—å’Œæ—¶æ€å·®çš„åŒºåˆ«ã€‚
ç¤ºä¾‹ä¸­çš„åä¸€ç§æ–¹æ³•æ˜¯åŸºäºè’™ç‰¹å¡ç½—çš„ï¼Œå› ä¸ºå®ƒä¼šç­‰åˆ°åˆ°è¾¾ç›®çš„åœ°åå†è®¡ç®—è¡Œç¨‹å„éƒ¨åˆ†çš„ä¼°è®¡å€¼ã€‚è€Œå‰è€…æ˜¯æ—¶é—´å·®å¼‚ã€‚

äº‹å®ä¸Šï¼Œå¦‚æœå°†è’™ç‰¹å¡ç½—(MC)æ–¹æ³•å’ŒåŠ¨æ€è§„åˆ’(DP)æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œå°±å¾—åˆ°æ—¶é—´å·®åˆ†(TD)æ–¹æ³•ã€‚

*æ³¨æ„:TD å°†è¢«è®°ä¸º TD(0 ),è¿™æ„å‘³ç€å®ƒå°†å‘å‰çœ‹ä¸€æ­¥ã€‚TD(0)æ˜¯ TD(n)çš„ç‰¹ä¾‹ã€‚*

å›æƒ³ä¸€ä¸‹ï¼Œåœ¨ MC ä¸­ï¼Œæˆ‘ä»¬æ’­æ”¾ä¸€æ•´é›†ï¼Œç›´åˆ°ç»“æŸï¼Œç„¶åæˆ‘ä»¬è®¡ç®—è¯¥é›†ä¸­å‡ºç°çš„æ¯ä¸ªå·çš„æŠ˜æ‰£å¥–åŠ±ã€‚æˆ‘ä»¬åšäº†å¤§é‡çš„äº‹ä»¶ï¼Œç„¶åæˆ‘ä»¬å¹³å‡æ¯ä¸ªçŠ¶æ€çš„ä¸åŒå€¼ã€‚

åœ¨ DP ä¸­ï¼Œæˆ‘ä»¬éšæœºåˆå§‹åŒ–æ‰€æœ‰çŠ¶æ€ï¼Œç„¶åæ ¹æ®å‘¨å›´çŠ¶æ€çš„(å…ˆå‰è®¡ç®—çš„)å€¼è¿­ä»£è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„å€¼ã€‚æˆ‘ä»¬ä¸€ç›´è¿™æ ·åšï¼Œç›´åˆ°æˆ‘ä»¬æ³¨æ„åˆ°ä»»ä½•çŠ¶æ€å€¼éƒ½æ²¡æœ‰æ˜¾è‘—çš„æ”¹å–„ã€‚

# æ”¿ç­–é¢„æµ‹

æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œåœ¨ MC ä¸­ï¼Œæˆ‘ä»¬æ’­æ”¾ä¸€é›†ç›´åˆ°ç»“æŸï¼Œç„¶åæˆ‘ä»¬å‘åç§»åŠ¨ï¼Œç»™æ¯ä¸ªçŠ¶æ€åˆ†é…è¯¥é›†çš„è´´ç°æ”¶ç›Š Gã€‚ä½†è¿™æ„å‘³ç€æˆ‘ä»¬å¿…é¡»ç­‰åˆ°æœ€åæ‰èƒ½çŸ¥é“ G.
çš„å€¼ï¼Œç„¶è€Œåœ¨ TD(0)ä¸­ï¼Œæˆ‘ä»¬æ ¹æ®ä¸‹ä¸€ä¸ªçŠ¶æ€çš„ ***ä¼°è®¡å€¼*** æ¥æ›´æ–°å½“å‰çŠ¶æ€ã€‚è¿˜è®°å¾— GPS çš„ä¾‹å­å—ï¼Œåœ¨æŸä¸€ç‚¹ä¸Šï¼ŒGPS å¯èƒ½ä¼šæ³¨æ„åˆ°æ‚¨çš„é€Ÿåº¦ä¸‹é™åˆ° 10Km/hï¼Œå› æ­¤å®ƒä¼šå°†å…¶åˆ°è¾¾æ—¶é—´çš„ä¼°è®¡å€¼æ›´æ–°+30 åˆ†é’Ÿï¼Œä½†è¿™å¯èƒ½æ˜¯ä¸€ä¸ªéå¸¸çŸ­æš‚çš„å‡é€Ÿï¼Œå‡ åˆ†é’Ÿåæ‚¨ä¼šå†æ¬¡åŠ é€Ÿï¼ŒGPS ä¼šå°†å…¶ä¼°è®¡å€¼æ›´æ–°-20 åˆ†é’Ÿã€‚
ä¸ TD(0)ç›¸åŒï¼ŒV(s)æ ¹æ®ä»¥ä¸‹å…¬å¼æ›´æ–°:

![](img/8f75ff90beabf722a32ba1cccd0c01b5.png)

where Î± is the step size âˆˆ ]0,1], ğ›„ is the discount factor

è¿™æ˜¯ä¸€ä¸ªå¢é‡å¹³å‡è®¡ç®—ã€‚æŸ¥çœ‹æ–‡ç« æœ«å°¾çš„â€œå¢é‡å¹³å‡è®¡ç®—â€äº†è§£è¯¦æƒ…ã€‚

å¾ˆæ˜æ˜¾ï¼Œä»…åŸºäºä¸€ä¸ªäº‹ä»¶ï¼Œå¯¹ V(s)çš„ä¼°è®¡æ˜¯ä¸å‡†ç¡®çš„ã€‚å’Œä½ é‚£å¤©çš„æ±½è½¦æ—…è¡Œä¸€æ ·ï¼åªåšä¸€æ¬¡ï¼Œä½ ä¸ä¼šå¾ˆå¥½åœ°ä¼°è®¡æ€»æ—¶é—´å’Œæ¯ä¸€éƒ¨åˆ†çš„æ—¶é—´ã€‚ä¹Ÿè®¸é‚£å¤©ä½ æ˜¯å¹¸è¿çš„ï¼Œæ²¡æœ‰äº¤é€šå µå¡ï¼Œæˆ–è€…ç›¸åï¼Œä½ æ˜¯ä¸å¹¸çš„ï¼Œç”±äºè½¦ç¥¸ä½ è¢«å›°åœ¨ä¸€ä¸ªä¸å¯»å¸¸çš„å µå¡ä¸­ã€‚
**ä½†æ˜¯å¦‚æœä½ æ¯å¤©éƒ½è¿™æ ·åš(å¤šæ”¾å‡ é›†)ï¼Œä½ å°±èƒ½æ¯å¤©éƒ½ç²¾ç¡®ä½ çš„ä¼°è®¡ã€‚**

TD(0)ä¸­ç­–ç•¥è¯„ä¼°çš„ç®—æ³•(ä¼ªä»£ç )å¦‚ä¸‹:

```
Evaluate_Policy(policy):
  randomly_initialize_non_terminal_states_values()Loop number_of_episodes:
  let s = start_state() # Play episode until the end
  Loop until game_over(): let a = get_action(policy, s, 0.1) 
                      # get action to perform on state s according 
                      # to the given policy 90% of the time, and a
                      # random action 10% of the time. let (s', r) = make_move(s, a) #make move from s using a and get 
                                  #the new state s' and the reward r # incrementally compute the average at V(s). Notice that V(s)
     # depends on an estimate of V(s') and not on the return 
     # G as in MC 
     let V(s) = V(s) + alpha * [r + gamma * V(s') - V(s)] let s = s' End Loop
End Loop
```

# æ”¿ç­–æ§åˆ¶

TD(0)ä¸­çš„ç­–ç•¥æ§åˆ¶æœ‰ä¸¤ç§å®ç°: ***SARSA*** å’Œ ***Q-Learning*** ã€‚

***SARSA*** æ˜¯ä¸€ä¸ª ***On-Policy*** æ–¹æ³•ï¼Œè¿™æ„å‘³ç€å®ƒæ ¹æ®æŸä¸ªç­–ç•¥æ¥è®¡ç®— ***Q å€¼*** ï¼Œç„¶åä»£ç†éµå¾ªè¯¥ç­–ç•¥ã€‚

***Q-Learning*** æ˜¯ä¸€ç§ ***Off-Policy*** æ–¹æ³•ã€‚å®ƒåŒ…æ‹¬æ ¹æ®è´ªå©ªç­–ç•¥è®¡ç®— ***Q å€¼*** ï¼Œä½†æ˜¯ä»£ç†ä¸ä¸€å®šéµå¾ªè´ªå©ªç­–ç•¥ã€‚

## è¨å°”è¨

åƒå¾€å¸¸ä¸€æ ·ï¼Œå½“æ‰§è¡ŒåŠ¨ä½œæ—¶ï¼Œä½ éœ€è¦è®¡ç®—åŠ¨ä½œ-çŠ¶æ€å‡½æ•°( ***Q å€¼*** )ï¼Œå› ä¸ºå®ƒå°†çŠ¶æ€å’ŒåŠ¨ä½œæ˜ å°„åˆ°ä¼°è®¡ã€‚
åœ¨[è’™ç‰¹å¡æ´›](https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511)çš„æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è§£é‡Šäº†ä¸ºä»€ä¹ˆå•ç‹¬çš„***ã€V(s)ã€‘***æ— åŠ©äºç¡®å®šæœ€ä¼˜æ”¿ç­–(è®¡åˆ’ï¼Œæˆ–åœ¨æ¯ä¸ªçŠ¶æ€ä¸‹é‡‡å–çš„è¡ŒåŠ¨)ã€‚
å› æ­¤ï¼Œå‡è®¾æˆ‘ä»¬å¤„äºçŠ¶æ€ ***s*** å¹¶ä¸”æˆ‘ä»¬æƒ³è¦åŸºäºçŠ¶æ€ ***s*** å’ŒåŠ¨ä½œ*æ¥è®¡ç®— ***Q å€¼*** ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼ŒTD(0)ä½¿ç”¨å¢é‡å¹³å‡å€¼æ¥è®¡ç®—ä»»ä½•çŠ¶æ€çš„å€¼ã€‚è¿™ä¸ªå¹³å‡è®¡ç®—ï¼Œç”¨ä¸‹ä¸€ä¸ªçŠ¶æ€çš„å€¼æ¥è¡¨ç¤ºã€‚
æ—¢ç„¶æˆ‘ä»¬åœ¨è®¡ç®— ***Q å€¼*** ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸‹ä¸€ä¸ªçŠ¶æ€***ã€sâ€™***çš„ ***Q å€¼*** ã€‚ç„¶è€Œ ***Q*** éœ€è¦çŠ¶æ€å’ŒåŠ¨ä½œä¸¤ä¸ªå‚æ•°ã€‚*

*![](img/b52d8f65adbb3424f5a896b9eec0b708.png)*

****SARSA*** è§£å†³è¿™ä¸ªé—®é¢˜çš„æ–¹æ³•æ˜¯ï¼Œä¸ºäº†å¾—åˆ° ***Q å€¼ï¼Œ*** åœ¨çŠ¶æ€***ã€sâ€™***é€‰æ‹©ä¸€ä¸ªåŠ¨ä½œ***ã€aâ€™***(åŸºäºÎµè´ªå©ªæ–¹æ³•)ï¼Œç„¶åå½“ä»£ç†åˆ°è¾¾***ã€sâ€™***æ—¶ï¼Œæˆ‘ä»¬å°†æ‰§è¡ŒåŠ¨ä½œ***ã€aâ€™***ã€‚*

*ä¸‹å›¾ç»™å‡ºäº†ä¸€ä¸ª SARSA ç¤ºä¾‹ã€‚*

*![](img/bc5e97a39bc042eef3fb6aa0c4d9b22c.png)**![](img/8f1d8a5a21da1130ec973a0847ec3ff8.png)*

*Four actions per state: North, South, West, East*

*åœ¨å·¦ä¾§ç½‘æ ¼ä¸­ï¼Œä»£ç†å¤„äºçŠ¶æ€ ***s*** ï¼Œå®ƒè®¡ç®—å‘åŒ—ç§»åŠ¨çš„å€¼(è“è‰²ç®­å¤´)ï¼Œä¸ºäº†èƒ½å¤Ÿè¿›è¡Œè®¡ç®—ï¼Œå®ƒéœ€è¦å‘ä¸œç§»åŠ¨çš„ ***Q å€¼******sâ€™***(ç°è‰²ç®­å¤´)ã€‚
å³è¾¹çš„ç½‘æ ¼æ˜¾ç¤ºå½“ä»£ç†ç§»åŠ¨åˆ°çŠ¶æ€***â€˜sâ€™***æ—¶ï¼Œå®ƒéµå¾ªç­–ç•¥å…ˆå‰å†³å®šçš„åŠ¨ä½œï¼Œå¹¶è®¡ç®—å‘ä¸œ(è“è‰²ç®­å¤´)åŠ¨ä½œçš„ ***Q å€¼***â€¦*

*ä»¥ä¸‹æ˜¯ ***SARSA*** çš„ä¼ªä»£ç :*

```
*SARRA():
  #initialization
  for each state s in AllNonTerminalStates:
     for each action a in Actions(s):
         Q(s,a) = random()
  for each s in TerminalStates:
      Q(s,_) = 0 #Q(s) = 0 for all actions in s Loop number_of_episodes:
    let s = start_state() # get action to perform on state s according 
    # to the given policy 90% of the time, and a
    # random action 10% of the time.    
    let a = get_epsilon_greedy_action(s, 0.1) # Play episode until the end
    Loop until game_over(): # make move from s using a and get the new state s'
       # and the reward r
       let (s', r) = make_move(s, a) # choose action to perform on state s'
      # a' will be used executed in the next iteration
      # but for the moment it will be used to get Q(s', a')
      let a' = get_epsilon_greedy_action(s', 0.1) # incrementally compute the average at Q(s,a)
     let Q(s, a) = Q(s, a) + alpha*[r + gamma * Q(s', a') - Q(s, a)] let s = s'  # move to the next state
     let a = a'  # use the same action a' as determined above End Loop
  End Loop*
```

## *q å­¦ä¹ *

****Q-learning*** ä¸*ç±»ä¼¼ï¼Œåªæ˜¯åœ¨è®¡ç®—***ã€sï¼Œaã€‘***Q(sï¼Œa)***ã€sâ€™ï¼Œaâ€™***æ—¶ï¼Œå®ƒä½¿ç”¨è´ªå©ªç­–ç•¥ä»ä¸‹ä¸€ä¸ªçŠ¶æ€***sâ€™***ä¸­ç¡®å®š*Q(sâ€™ï¼Œaâ€™*ã€‚
è®°ä½è´ªå©ªç­–ç•¥é€‰æ‹©ç»™å‡ºæœ€é«˜
***Q å€¼*** çš„åŠ¨ä½œã€‚ç„¶è€Œï¼Œè¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå®ƒä¸ä¸€å®šéµå¾ªé‚£ç§è´ªå©ªçš„æ”¿ç­–ã€‚**

**![](img/c649ef3d85de2baf51f067d5bba643c6.png)**

**è¿™ä¸ªå½¢è±¡çš„æ‰“å‡»è¯´æ˜äº† ***Q-Learning*** çš„æœºç†:**

**![](img/4fd32733eb5f853989fef9784045d8b5.png)**

**å·¦ä¾§ç½‘æ ¼æ˜¾ç¤ºäº†å¤„äºçŠ¶æ€ ***s*** çš„ä»£ç†åœ¨å‘åŒ—è¡Œé©¶æ—¶è®¡ç®— Q å€¼(è“è‰²ç®­å¤´)ã€‚ä¸ºæ­¤ï¼Œå®ƒåœ¨è®¡ç®—ä¸­ä½¿ç”¨ç”±çŠ¶æ€ s(æ©™è‰²ç®­å¤´)çš„è´ªå©ªç­–ç•¥ç¡®å®šçš„ ***Q å€¼*** ã€‚
å³è¾¹çš„ç½‘æ ¼æ˜¾ç¤ºä»£ç†ç§»åŠ¨åˆ°çŠ¶æ€***â€˜sâ€™***ï¼Œä½†ä¸ä¸€å®šéµå¾ªè´ªå©ªç­–ç•¥ç¡®å®šçš„åŠ¨ä½œ(æ©™è‰²ç®­å¤´)ï¼Œè€Œæ˜¯é€‰æ‹©éšæœºåŠ¨ä½œ(è“è‰²ç®­å¤´)ã€‚**

*****Q-learning*** çš„ç®—æ³•å¦‚ä¸‹:**

```
 **QLearning():
  #initialization
  for each state s in AllNonTerminalStates:
     for each action a in Actions(s):
         Q(s,a) = random()
  for each s in TerminalStates:
      Q(s,_) = 0 #Q(s) = 0 for all actions in s Loop number_of_episodes:
    let s = start_state() # Play episode until the end
    Loop until game_over(): # get action to perform on state s according 
      # to the given policy 90% of the time, and a
      # random action 10% of the time.    
      let a = get_epsilon_greedy_action(s, 0.1) # make move from s using a and get the new state s'
      # and the reward r
      let (s', r) = make_move(s, a) # choose the max Q-value (qmax) on state s'
      let qmax = get_max_qvalue_on_state(s') # incrementally compute the average at Q(s,a)
      let Q(s, a) = Q(s, a) + alpha*[r + gamma * qmax - Q(s, a)] let s = s'  # move to the next state End Loop
End Loop**
```

# **å¢é‡å¹³å‡è®¡ç®—**

**è¿™ä¸€æ®µè¯´æ˜äº†å¢é‡å¹³å‡å€¼çš„è®¡ç®—æ–¹æ³•ã€‚
å¹³å‡å€¼çš„é¡¹ä»¥æ—¢æœ‰ A(n+1)åˆæœ‰ A(n)çš„æ–¹å¼æ’åˆ—ã€‚**

**![](img/e32b8fb231783b18b6b8ad37ed90b057.png)**

**Incremental Average Computation**

**è¯·æ³¨æ„ï¼Œ1/(n+1)è¡¨ç¤ºçŠ¶æ€å€¼å’ŒåŠ¨ä½œå€¼å‡½æ•°ä¸­çš„Î±é¡¹ã€‚**

# **ç»“è®º**

**æ—¶é—´å·®å¼‚æ¯”åŠ¨æ€è§„åˆ’æ–¹æ³•æ›´å¥½ï¼Œå› ä¸ºå®ƒä¸éœ€è¦ç¯å¢ƒçš„æ¨¡å‹ï¼Œä¹Ÿä¸éœ€è¦æŠ¥é…¬å’Œæ¦‚ç‡åˆ†å¸ƒã€‚TD ä¹Ÿæ¯”è’™ç‰¹å¡ç½—æ–¹æ³•æœ‰ä¼˜åŠ¿ï¼Œå› ä¸ºä¸éœ€è¦ç­‰åˆ°ä¸€é›†ç»“æŸæ‰çŸ¥é“å›æŠ¥ï¼Œåªéœ€è¦ä¸€ä¸ªæ—¶é—´æ­¥ã€‚**

# **ç›¸å…³æ–‡ç« **

*   **[å¼€å‘äººå‘˜å¼ºåŒ–å­¦ä¹ æ”¿ç­–](https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182)**
*   **[å¼ºåŒ–å­¦ä¹ ä¸­çš„ Q vs Vï¼Œæœ€ç®€å•çš„æ–¹æ³•](https://medium.com/p/9350e1523031)**
*   **[æ•°å­¦èƒŒåçš„å¼ºåŒ–å­¦ä¹ ï¼Œæœ€ç®€å•çš„æ–¹æ³•](https://medium.com/p/1b7ed0c030f4)**
*   **[è’™ç‰¹å¡æ´›å¼ºåŒ–å­¦ä¹ ï¼Œç®€å•æ˜“è¡Œ](https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511)**
*   **[åŠ¨æ€ç¼–ç¨‹åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„ç®€ä¾¿æ–¹æ³•](https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac)**