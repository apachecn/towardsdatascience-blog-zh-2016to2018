# å¦‚ä½•åœ¨ TensorFlow ä¸­ä½¿ç”¨æ•°æ®é›†

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428?source=collection_archive---------0----------------------->

![](img/c6828cd52a765a6e50e819756da7b7d0.png)

***æˆ‘åœ¨***[***LinkedIn***](https://www.linkedin.com/in/francesco-saverio-zuppichini-94659a150/?originalSubdomain=ch)***ï¼Œå¿«æ¥æ‰“ä¸ªæ‹›å‘¼*** ğŸ‘‹

å†…ç½®çš„è¾“å…¥ç®¡é“ã€‚å†ä¹Ÿä¸ç”¨â€œfeed-dictâ€äº†

***16/02/2020:æˆ‘å·²ç»æ¢æˆ PyTorch*** ğŸ˜

*29/05/2019:æˆ‘ä¼šæŠŠæ•™ç¨‹æ›´æ–°åˆ° tf 2.0ğŸ˜(æˆ‘æ­£åœ¨å®Œæˆæˆ‘çš„ç¡•å£«è®ºæ–‡)*

*2018 å¹´ 2 æœˆ 6 æ—¥æ›´æ–°:æ·»åŠ äº†ç¬¬äºŒä¸ªå®Œæ•´ç¤ºä¾‹ï¼Œå°† csv ç›´æ¥è¯»å…¥æ•°æ®é›†*

*2018 å¹´ 5 æœˆ 25 æ—¥æ›´æ–°:æ·»åŠ äº†ç¬¬äºŒä¸ªå®Œæ•´ç¤ºä¾‹ï¼Œå¸¦æœ‰ä¸€ä¸ª* ***å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨***

*æ›´æ–°è‡³ TensorFlow 1.8*

æ‚¨åº”è¯¥çŸ¥é“ï¼Œ`feed-dict`æ˜¯å‘ TensorFlow ä¼ é€’ä¿¡æ¯çš„æœ€æ…¢æ–¹å¼ï¼Œå¿…é¡»é¿å…ä½¿ç”¨ã€‚å°†æ•°æ®è¾“å…¥æ¨¡å‹çš„æ­£ç¡®æ–¹æ³•æ˜¯ä½¿ç”¨è¾“å…¥ç®¡é“æ¥ç¡®ä¿ GPU æ°¸è¿œä¸ä¼šç­‰å¾…æ–°çš„ä¸œè¥¿è¿›æ¥ã€‚

å¹¸è¿çš„æ˜¯ï¼ŒTensorFlow æœ‰ä¸€ä¸ªå†…ç½®çš„ APIï¼Œåä¸º [Dataset](https://www.tensorflow.org/programmers_guide/datasets) ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•åˆ›å»ºè¾“å…¥ç®¡é“ï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆåœ°å°†æ•°æ®è¾“å…¥æ¨¡å‹ã€‚

æœ¬æ–‡å°†è§£é‡Šæ•°æ®é›†çš„åŸºæœ¬æœºåˆ¶ï¼Œæ¶µç›–æœ€å¸¸è§çš„ç”¨ä¾‹ã€‚

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ° jupyter ç¬”è®°æœ¬çš„æ‰€æœ‰ä»£ç :

[https://github . com/FrancescoSaverioZuppichini/tensor flow-Dataset-Tutorial/blob/master/Dataset _ Tutorial . ipynb](https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb)

# ä¸€èˆ¬æ¦‚è¿°

ä¸ºäº†ä½¿ç”¨æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‰ä¸ªæ­¥éª¤:

*   **å¯¼å…¥æ•°æ®**ã€‚ä»ä¸€äº›æ•°æ®åˆ›å»ºæ•°æ®é›†å®ä¾‹
*   åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨ã€‚é€šè¿‡ä½¿ç”¨åˆ›å»ºçš„æ•°æ®é›†æ¥åˆ¶ä½œè¿­ä»£å™¨å®ä¾‹æ¥éå†æ•°æ®é›†
*   **æ¶ˆè´¹æ•°æ®**ã€‚é€šè¿‡ä½¿ç”¨åˆ›å»ºçš„è¿­ä»£å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ•°æ®é›†ä¸­è·å–å…ƒç´ ï¼Œä»¥æä¾›ç»™æ¨¡å‹

# å¯¼å…¥æ•°æ®

æˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€äº›æ•°æ®æ”¾å…¥æˆ‘ä»¬çš„æ•°æ®é›†

## æ¥è‡ª numpy

è¿™æ˜¯å¸¸è§çš„æƒ…å†µï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª numpy æ•°ç»„ï¼Œæˆ‘ä»¬æƒ³æŠŠå®ƒä¼ é€’ç»™ tensorflowã€‚

```
# create a random vector of shape (100,2)
x = np.random.sample((100,2))
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ é€’ä¸æ­¢ä¸€ä¸ª numpy æ•°ç»„ï¼Œä¸€ä¸ªç»å…¸çš„ä¾‹å­æ˜¯å½“æˆ‘ä»¬å°†ä¸€äº›æ•°æ®åˆ†æˆç‰¹å¾å’Œæ ‡ç­¾æ—¶

```
features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))
dataset = tf.data.Dataset.from_tensor_slices((features,labels))
```

## æ¥è‡ªå¼ é‡

å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€äº›å¼ é‡åˆå§‹åŒ–æˆ‘ä»¬çš„æ•°æ®é›†

```
# using a tensor
dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))
```

## ä»å ä½ç¬¦

å½“æˆ‘ä»¬æƒ³è¦åŠ¨æ€åœ°æ”¹å˜æ•°æ®é›†ä¸­çš„æ•°æ®æ—¶ï¼Œè¿™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œæˆ‘ä»¬å°†åœ¨åé¢çœ‹åˆ°å¦‚ä½•æ”¹å˜ã€‚

```
x = tf.placeholder(tf.float32, shape=[None,2])
dataset = tf.data.Dataset.from_tensor_slices(x)
```

## æ¥è‡ªå‘ç”µæœº

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»ç”Ÿæˆå™¨åˆå§‹åŒ–ä¸€ä¸ªæ•°æ®é›†ï¼Œå½“æˆ‘ä»¬æœ‰ä¸€ä¸ªä¸åŒå…ƒç´ é•¿åº¦çš„æ•°ç»„(ä¾‹å¦‚ä¸€ä¸ªåºåˆ—)æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨:

```
# from generator
sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])def generator():
    for el in sequence:
        yield eldataset = tf.data.Dataset().batch(1).from_generator(generator,
                                           output_types= tf.int64, 
                                           output_shapes=(tf.TensorShape([None, 1])))iter = dataset.make_initializable_iterator()
el = iter.get_next()with tf.Session() as sess:
    sess.run(iter.initializer)
    print(sess.run(el))
    print(sess.run(el))
    print(sess.run(el))
```

è¾“å‡º:

```
[[1]]
[[2]
 [3]]
[[3]
 [4]
 [5]]
```

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨è¿˜éœ€è¦æŒ‡å®šå°†ç”¨äºåˆ›å»ºæ­£ç¡®å¼ é‡çš„æ•°æ®çš„ç±»å‹å’Œå½¢çŠ¶ã€‚

## ä» csv æ–‡ä»¶

æ‚¨å¯ä»¥ç›´æ¥å°† csv æ–‡ä»¶è¯»å…¥æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œæˆ‘æœ‰ä¸€ä¸ª csv æ–‡ä»¶ï¼Œé‡Œé¢æœ‰æ¨æ–‡å’Œä»–ä»¬çš„æƒ…ç»ªã€‚

![](img/c6b42c4856e7250e61a751073c6db564.png)

tweets.csv

æˆ‘ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨`tf.contrib.data.make_csv_dataset`è½»æ¾åœ°ä»å®ƒåˆ›å»ºä¸€ä¸ª`Dataset`ã€‚è¯·æ³¨æ„ï¼Œè¿­ä»£å™¨å°†åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œä»¥ key ä½œä¸ºåˆ—åï¼Œä»¥å¼ é‡çš„å½¢å¼ä½¿ç”¨æ­£ç¡®çš„è¡Œå€¼ã€‚

```
# load a csv
CSV_PATH = './tweets.csv'
dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)
iter = dataset.make_one_shot_iterator()
next = iter.get_next()
print(next) # next is a dict with key=columns names and value=column data
inputs, labels = next['text'], next['sentiment']with  tf.Session() as sess:
    sess.run([inputs, labels])
```

`next`åœ¨å“ªé‡Œ

```
{'sentiment': <tf.Tensor 'IteratorGetNext_15:0' shape=(?,) dtype=int32>, 'text': <tf.Tensor 'IteratorGetNext_15:1' shape=(?,) dtype=string>}
```

# åˆ›å»ºè¿­ä»£å™¨

æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•åˆ›å»ºæ•°æ®é›†ï¼Œä½†æ˜¯å¦‚ä½•å–å›æˆ‘ä»¬çš„æ•°æ®å‘¢ï¼Ÿæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ä¸€ä¸ª`Iterator`ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿéå†æ•°æ®é›†å¹¶æ£€ç´¢æ•°æ®çš„çœŸå®å€¼ã€‚æœ‰å››ç§ç±»å‹çš„è¿­ä»£å™¨ã€‚

*   **ä¸€é’ˆã€‚**å®ƒå¯ä»¥éå†ä¸€æ¬¡æ•°æ®é›†ï¼Œä½ **ä¸èƒ½ç»™å®ƒ**ä»»ä½•å€¼ã€‚
*   **å¯åˆå§‹åŒ–**:å¯ä»¥åŠ¨æ€æ”¹å˜è°ƒç”¨å®ƒçš„`initializer`æ“ä½œï¼Œç”¨`feed_dict`ä¼ é€’æ–°æ•°æ®ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå¯ä»¥è£…æ»¡ä¸œè¥¿çš„æ¡¶ã€‚
*   **å¯é‡æ–°åˆå§‹åŒ–**:å®ƒå¯ä»¥ä»ä¸åŒçš„`Dataset.`åˆå§‹åŒ–ï¼Œå½“ä½ æœ‰ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†éœ€è¦ä¸€äº›é¢å¤–çš„å˜æ¢ï¼Œä¾‹å¦‚ shuffleï¼Œå’Œä¸€ä¸ªæµ‹è¯•æ•°æ®é›†æ—¶éå¸¸æœ‰ç”¨ã€‚è¿™å°±åƒä½¿ç”¨å¡”å¼èµ·é‡æœºæ¥é€‰æ‹©ä¸åŒçš„å®¹å™¨ã€‚
*   **Feedable** : å¯ä»¥ç”¨è¿­ä»£å™¨é€‰æ‹©ä½¿ç”¨ã€‚æŒ‰ç…§å‰é¢çš„ä¾‹å­ï¼Œå°±åƒä¸€ä¸ªå¡”åŠé€‰æ‹©ç”¨å“ªä¸ªå¡”åŠæ¥é€‰æ‹©æ‹¿å“ªä¸ªé›†è£…ç®±ã€‚åœ¨æˆ‘çœ‹æ¥æ˜¯æ²¡ç”¨çš„ã€‚

## ä¸€æ¬¡æ€§è¿­ä»£å™¨

è¿™æ˜¯æœ€ç®€å•çš„è¿­ä»£å™¨ã€‚ä½¿ç”¨ç¬¬ä¸€ä¸ªä¾‹å­

```
x = np.random.sample((100,2))
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)# create the iterator
iter = dataset.make_one_shot_iterator()
```

ç„¶åä½ éœ€è¦è°ƒç”¨`get_next()`æ¥è·å–åŒ…å«ä½ çš„æ•°æ®çš„å¼ é‡

```
...
# create the iterator
iter = dataset.make_one_shot_iterator()
el = iter.get_next()
```

æˆ‘ä»¬å¯ä»¥è¿è¡Œ`el`æ¥æŸ¥çœ‹å®ƒçš„å€¼

```
with tf.Session() as sess:
    print(sess.run(el)) # output: [ 0.42116176  0.40666069]
```

## å¯åˆå§‹åŒ–è¿­ä»£å™¨

å¦‚æœæˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªåŠ¨æ€æ•°æ®é›†ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶æ”¹å˜æ•°æ®æºï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå¸¦æœ‰å ä½ç¬¦çš„æ•°æ®é›†ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šç”¨çš„`feed-dict`æœºåˆ¶åˆå§‹åŒ–å ä½ç¬¦ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ª*å¯åˆå§‹åŒ–çš„è¿­ä»£å™¨*å®Œæˆçš„ã€‚ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„ç¤ºä¾‹ä¸‰

```
# using a placeholder
x = tf.placeholder(tf.float32, shape=[None,2])
dataset = tf.data.Dataset.from_tensor_slices(x)data = np.random.sample((100,2))iter = dataset.make_initializable_iterator() # create the iterator
el = iter.get_next()with tf.Session() as sess:
    # feed the placeholder with data
    sess.run(iter.initializer, feed_dict={ x: data }) 
    print(sess.run(el)) # output [ 0.52374458  0.71968478]
```

è¿™æ¬¡æˆ‘ä»¬å«`make_initializable_iterator`ã€‚ç„¶åï¼Œåœ¨`sess`èŒƒå›´å†…ï¼Œæˆ‘ä»¬è¿è¡Œ`initializer`æ“ä½œæ¥ä¼ é€’æˆ‘ä»¬çš„æ•°æ®ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯ä¸€ä¸ªéšæœºçš„ numpy æ•°ç»„ã€‚ã€‚

æƒ³è±¡ä¸€ä¸‹ï¼Œç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªæµ‹è¯•é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªçœŸå®çš„å¸¸è§åœºæ™¯:

```
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.array([[1,2]]), np.array([[0]]))
```

ç„¶åï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¯„ä¼°å®ƒï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨è®­ç»ƒåå†æ¬¡åˆå§‹åŒ–è¿­ä»£å™¨æ¥å®Œæˆ

```
# initializable iterator to switch between dataset
EPOCHS = 10x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
dataset = tf.data.Dataset.from_tensor_slices((x, y))train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.array([[1,2]]), np.array([[0]]))iter = dataset.make_initializable_iterator()
features, labels = iter.get_next()with tf.Session() as sess:
#     initialise iterator with train data
    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
    for _ in range(EPOCHS):
        sess.run([features, labels])
#     switch to test data
    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
    print(sess.run([features, labels]))
```

## **å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨**

è¿™ä¸ªæ¦‚å¿µç±»ä¼¼äºä»¥å‰ï¼Œæˆ‘ä»¬è¦åœ¨æ•°æ®ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æ˜¯å‘åŒä¸€ä¸ªæ•°æ®é›†æä¾›æ–°æ•°æ®ï¼Œè€Œæ˜¯åˆ‡æ¢æ•°æ®é›†ã€‚å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªæµ‹è¯•æ•°æ®é›†

```
# making fake data using numpy
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
```

æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸¤ä¸ªæ•°æ®é›†

```
# create two datasets, one for training and one for test
train_dataset = tf.data.Dataset.from_tensor_slices(train_data)
test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
```

è¿™å°±æ˜¯è¯€çªï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ³›å‹è¿­ä»£å™¨

```
# create a iterator of the correct shape and type
iter = tf.data.Iterator.from_structure(train_dataset.output_types,
                                           train_dataset.output_shapes)
```

ç„¶åæ˜¯ä¸¤ä¸ªåˆå§‹åŒ–æ“ä½œ:

```
# create the initialisation operations
train_init_op = iter.make_initializer(train_dataset)
test_init_op = iter.make_initializer(test_dataset)
```

æˆ‘ä»¬åƒä»¥å‰ä¸€æ ·å¾—åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ 

```
features, labels = iter.get_next()
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ä¼šè¯ç›´æ¥è¿è¡Œä¸¤ä¸ªåˆå§‹åŒ–æ“ä½œã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å¾—åˆ°:

```
# Reinitializable iterator to switch between Datasets
EPOCHS = 10
# making fake data using numpy
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
# create two datasets, one for training and one for test
train_dataset = tf.data.Dataset.from_tensor_slices(train_data)
test_dataset = tf.data.Dataset.from_tensor_slices(test_data)
# create a iterator of the correct shape and type
iter = tf.data.Iterator.from_structure(train_dataset.output_types,
                                           train_dataset.output_shapes)
features, labels = iter.get_next()
# create the initialisation operations
train_init_op = iter.make_initializer(train_dataset)
test_init_op = iter.make_initializer(test_dataset)with tf.Session() as sess:
    sess.run(train_init_op) # switch to train dataset
    for _ in range(EPOCHS):
        sess.run([features, labels])
    sess.run(test_init_op) # switch to val dataset
    print(sess.run([features, labels]))
```

## å¯é¦ˆé€è¿­ä»£å™¨

è¿™éå¸¸ç±»ä¼¼äº`reinitializable`è¿­ä»£å™¨ï¼Œä½†æ˜¯å®ƒä¸æ˜¯åœ¨æ•°æ®é›†ä¹‹é—´åˆ‡æ¢ï¼Œè€Œæ˜¯åœ¨è¿­ä»£å™¨ä¹‹é—´åˆ‡æ¢ã€‚åœ¨æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ä¹‹å

```
train_dataset = tf.data.Dataset.from_tensor_slices((x,y))
test_dataset = tf.data.Dataset.from_tensor_slices((x,y))
```

ä¸€ä¸ªç”¨äºåŸ¹è®­ï¼Œä¸€ä¸ªç”¨äºæµ‹è¯•ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„è¿­ä»£å™¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä½¿ç”¨`initializable`è¿­ä»£å™¨ï¼Œä½†æ˜¯ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨`one shot`è¿­ä»£å™¨

```
train_iterator = train_dataset.make_initializable_iterator()
test_iterator = test_dataset.make_initializable_iterator()
```

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å’Œ`handle`ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¯ä»¥åŠ¨æ€æ”¹å˜çš„å ä½ç¬¦ã€‚

```
handle = tf.placeholder(tf.string, shape=[])
```

ç„¶åï¼Œä¸ä¹‹å‰ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†çš„å½¢çŠ¶å®šä¹‰ä¸€ä¸ªæ³›å‹è¿­ä»£å™¨

```
iter = tf.data.Iterator.from_string_handle(
    handle, train_dataset.output_types, train_dataset.output_shapes)
```

ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ 

```
next_elements = iter.get_next()
```

ä¸ºäº†åœ¨è¿­ä»£å™¨ä¹‹é—´åˆ‡æ¢ï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨`next_elemenents`æ“ä½œï¼Œå¹¶åœ¨ feed_dict ä¸­ä¼ é€’æ­£ç¡®çš„`handle`ã€‚ä¾‹å¦‚ï¼Œè¦ä»è®­ç»ƒé›†ä¸­è·å–ä¸€ä¸ªå…ƒç´ :

```
sess.run(next_elements, feed_dict = {handle: train_handle})
```

å¦‚æœä½ æ­£åœ¨ä½¿ç”¨`initializable`è¿­ä»£å™¨ï¼Œå°±åƒæˆ‘ä»¬æ­£åœ¨åšçš„é‚£æ ·ï¼Œè®°å¾—åœ¨å¼€å§‹ä¹‹å‰åˆå§‹åŒ–å®ƒä»¬

```
sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})
```

ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å¾—åˆ°:

```
# feedable iterator to switch between iterators
EPOCHS = 10
# making fake data using numpy
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((10,2)), np.random.sample((10,1)))
# create placeholder
x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
# create two datasets, one for training and one for test
train_dataset = tf.data.Dataset.from_tensor_slices((x,y))
test_dataset = tf.data.Dataset.from_tensor_slices((x,y))
# create the iterators from the dataset
train_iterator = train_dataset.make_initializable_iterator()
test_iterator = test_dataset.make_initializable_iterator()
# same as in the doc [https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator](https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator)
handle = tf.placeholder(tf.string, shape=[])
iter = tf.data.Iterator.from_string_handle(
    handle, train_dataset.output_types, train_dataset.output_shapes)
next_elements = iter.get_next()with tf.Session() as sess:
    train_handle = sess.run(train_iterator.string_handle())
    test_handle = sess.run(test_iterator.string_handle())

    # initialise iterators. 
    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})
    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})

    for _ in range(EPOCHS):
        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})
        print(x, y)

    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})
    print(x,y)
```

# æ¶ˆè´¹æ•°æ®

åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¼šè¯æ¥æ‰“å°æ•°æ®é›†ä¸­`next`å…ƒç´ çš„å€¼ã€‚

```
...
next_el = iter.get_next()
...
print(sess.run(next_el)) # will output the current element
```

ä¸ºäº†å°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»ä¼ é€’ä»`get_next()`ç”Ÿæˆçš„å¼ é‡

åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ª numpy æ•°ç»„çš„æ•°æ®é›†ï¼Œä½¿ç”¨äº†ç¬¬ä¸€éƒ¨åˆ†ä¸­çš„ç›¸åŒç¤ºä¾‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦å°†`.random.sample`åŒ…è£…åœ¨å¦ä¸€ä¸ª numpy æ•°ç»„ä¸­ï¼Œä»¥æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œæˆ‘ä»¬éœ€è¦è¿™ä¸ªç»´åº¦æ¥æ‰¹é‡å¤„ç†æ•°æ®

```
# using two numpy arrays
features, labels = (np.array([np.random.sample((100,2))]), 
                    np.array([np.random.sample((100,1))]))dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)
```

ç„¶ååƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨

```
iter = dataset.make_one_shot_iterator()
x, y = iter.get_next()
```

æˆ‘ä»¬åˆ¶ä½œä¸€ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ

```
# make a simple model
net = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8)
prediction = tf.layers.dense(net, 1)loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)
```

æˆ‘ä»¬**ç›´æ¥**ä½¿ç”¨æ¥è‡ª`iter.get_next()`çš„å¼ é‡ä½œä¸ºç¬¬ä¸€å±‚çš„è¾“å…¥ï¼Œå¹¶ä½œä¸ºæŸå¤±å‡½æ•°çš„æ ‡ç­¾ã€‚åŒ…è£…åœ¨ä¸€èµ·:

```
EPOCHS = 10
BATCH_SIZE = 16
# using two numpy arrays
features, labels = (np.array([np.random.sample((100,2))]), 
                    np.array([np.random.sample((100,1))]))dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()
x, y = iter.get_next()# make a simple model
net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8, activation=tf.tanh)
prediction = tf.layers.dense(net, 1, activation=tf.tanh)loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(EPOCHS):
        _, loss_value = sess.run([train_op, loss])
        print("Iter: {}, Loss: {:.4f}".format(i, loss_value))
```

è¾“å‡º:

```
Iter: 0, Loss: 0.1328 
Iter: 1, Loss: 0.1312 
Iter: 2, Loss: 0.1296 
Iter: 3, Loss: 0.1281 
Iter: 4, Loss: 0.1267 
Iter: 5, Loss: 0.1254 
Iter: 6, Loss: 0.1242 
Iter: 7, Loss: 0.1231 
Iter: 8, Loss: 0.1220 
Iter: 9, Loss: 0.1210
```

# æœ‰ç”¨çš„ä¸œè¥¿

## ä¸€æ‰¹

é€šå¸¸æ‰¹å¤„ç†æ•°æ®æ˜¯ä¸€ä»¶ç—›è‹¦çš„äº‹æƒ…ï¼Œæœ‰äº†`Dataset` APIï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–¹æ³•`batch(BATCH_SIZE)`ä»¥æä¾›çš„å¤§å°è‡ªåŠ¨æ‰¹å¤„ç†æ•°æ®é›†ã€‚é»˜è®¤å€¼ä¸º 1ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ‰¹é‡å¤§å°ä¸º 4

```
# BATCHING
BATCH_SIZE = 4
x = np.random.sample((100,2))
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()
el = iter.get_next()with tf.Session() as sess:
    print(sess.run(el)) 
```

è¾“å‡º:

```
[[ 0.65686128  0.99373963]
 [ 0.69690451  0.32446826]
 [ 0.57148422  0.68688242]
 [ 0.20335116  0.82473219]]
```

## é‡å¤

ä½¿ç”¨`.repeat()`,æˆ‘ä»¬å¯ä»¥æŒ‡å®šæ•°æ®é›†è¿­ä»£çš„æ¬¡æ•°ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’å‚æ•°ï¼Œå®ƒå°†æ°¸è¿œå¾ªç¯ä¸‹å»ï¼Œé€šå¸¸æœ€å¥½æ˜¯æ°¸è¿œå¾ªç¯ä¸‹å»ï¼Œç”¨æ ‡å‡†å¾ªç¯ç›´æ¥æ§åˆ¶å†å…ƒæ•°ã€‚

## æ´—ç‰Œ

æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨æ–¹æ³•`shuffle()`æ¥æ‰“ä¹±æ•°æ®é›†ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæ—¶æœŸéƒ½ä¼šæ‰“ä¹±æ•°æ®é›†ã€‚

*è®°ä½:æ‰“ä¹±æ•°æ®é›†å¯¹äºé¿å…è¿‡åº¦é€‚åº”éå¸¸é‡è¦ã€‚*

æˆ‘ä»¬è¿˜å¯ä»¥è®¾ç½®å‚æ•°`buffer_size`ï¼Œä¸€ä¸ªå›ºå®šå¤§å°çš„ç¼“å†²åŒºï¼Œä¸‹ä¸€ä¸ªå…ƒç´ å°†ä»å…¶ä¸­ç»Ÿä¸€é€‰æ‹©ã€‚ç¤ºä¾‹:

```
# BATCHING
BATCH_SIZE = 4
x = np.array([[1],[2],[3],[4]])
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.shuffle(buffer_size=100)
dataset = dataset.batch(BATCH_SIZE)iter = dataset.make_one_shot_iterator()
el = iter.get_next()with tf.Session() as sess:
    print(sess.run(el))
```

é¦–æ¬¡è¿è¡Œè¾“å‡º:

```
[[4]
 [2]
 [3]
 [1]]
```

ç¬¬äºŒè½®è¾“å‡º:

```
[[3]
 [1]
 [2]
 [4]]
```

æ²¡é”™ã€‚å®ƒè¢«æ´—ç‰Œäº†ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥è®¾ç½®`seed`å‚æ•°ã€‚

# åœ°å›¾

æ‚¨å¯ä»¥ä½¿ç”¨`map`æ–¹æ³•å°†è‡ªå®šä¹‰å‡½æ•°åº”ç”¨äºæ•°æ®é›†çš„æ¯ä¸ªæˆå‘˜ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå…ƒç´ ä¹˜ä»¥ 2:

```
# MAP
x = np.array([[1],[2],[3],[4]])
# make a dataset from a numpy array
dataset = tf.data.Dataset.from_tensor_slices(x)
dataset = dataset.map(lambda x: x*2)iter = dataset.make_one_shot_iterator()
el = iter.get_next()with tf.Session() as sess:
#     this will run forever
        for _ in range(len(x)):
            print(sess.run(el))
```

è¾“å‡º:

```
[2]
[4]
[6]
[8]
```

# å®Œæ•´ç¤ºä¾‹

## **å¯åˆå§‹åŒ–çš„**è¿­ä»£å™¨

åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰¹å¤„ç†æ¥è®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª**å¯åˆå§‹åŒ–çš„è¿­ä»£å™¨**åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¹‹é—´åˆ‡æ¢

```
# Wrapping all together -> Switch between train and test set using Initializable iterator
EPOCHS = 10
# create a placeholder to dynamically switch between batch sizes
batch_size = tf.placeholder(tf.int64)x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()# using two numpy arrays
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((20,2)), np.random.sample((20,1)))iter = dataset.make_initializable_iterator()
features, labels = iter.get_next()
# make a simple model
net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8, activation=tf.tanh)
prediction = tf.layers.dense(net, 1, activation=tf.tanh)loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # initialise iterator with train data
    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})
    print('Training...')
    for i in range(EPOCHS):
        tot_loss = 0
        for _ in range(n_batches):
            _, loss_value = sess.run([train_op, loss])
            tot_loss += loss_value
        print("Iter: {}, Loss: {:.4f}".format(i, tot_loss / n_batches))
    # initialise iterator with test data
    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})
    print('Test Loss: {:4f}'.format(sess.run(loss)))
```

**æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªæ‰¹é‡å¤§å°çš„å ä½ç¬¦ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒååŠ¨æ€åˆ‡æ¢å®ƒ**

è¾“å‡º

```
Training...
Iter: 0, Loss: 0.2977
Iter: 1, Loss: 0.2152
Iter: 2, Loss: 0.1787
Iter: 3, Loss: 0.1597
Iter: 4, Loss: 0.1277
Iter: 5, Loss: 0.1334
Iter: 6, Loss: 0.1000
Iter: 7, Loss: 0.1154
Iter: 8, Loss: 0.0989
Iter: 9, Loss: 0.0948
Test Loss: 0.082150
```

## å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨

åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰¹å¤„ç†è®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨**å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨**åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¹‹é—´åˆ‡æ¢

```
# Wrapping all together -> Switch between train and test set using Reinitializable iterator
EPOCHS = 10
# create a placeholder to dynamically switch between batch sizes
batch_size = tf.placeholder(tf.int64)x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])
train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()
test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it
# using two numpy arrays
train_data = (np.random.sample((100,2)), np.random.sample((100,1)))
test_data = (np.random.sample((20,2)), np.random.sample((20,1)))# create a iterator of the correct shape and type
iter = tf.data.Iterator.from_structure(train_dataset.output_types,
                                           train_dataset.output_shapes)
features, labels = iter.get_next()
# create the initialisation operations
train_init_op = iter.make_initializer(train_dataset)
test_init_op = iter.make_initializer(test_dataset)# make a simple model
net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input
net = tf.layers.dense(net, 8, activation=tf.tanh)
prediction = tf.layers.dense(net, 1, activation=tf.tanh)loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label
train_op = tf.train.AdamOptimizer().minimize(loss)with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    # initialise iterator with train data
    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})
    print('Training...')
    for i in range(EPOCHS):
        tot_loss = 0
        for _ in range(n_batches):
            _, loss_value = sess.run([train_op, loss])
            tot_loss += loss_value
        print("Iter: {}, Loss: {:.4f}".format(i, tot_loss / n_batches))
    # initialise iterator with test data
    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})
    print('Test Loss: {:4f}'.format(sess.run(loss)))
```

# å…¶ä»–èµ„æº

å¼ é‡æµæ•°æ®é›†æ•™ç¨‹:[https://www.tensorflow.org/programmers_guide/datasets](https://www.tensorflow.org/programmers_guide/datasets)

æ•°æ®é›†æ–‡æ¡£:

[https://www.tensorflow.org/api_docs/python/tf/data/Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)

# ç»“è®º

`Dataset` API ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§å¿«é€Ÿè€Œå¥å£®çš„æ–¹å¼æ¥åˆ›å»ºä¼˜åŒ–çš„è¾“å…¥ç®¡é“ï¼Œä»¥è®­ç»ƒã€è¯„ä¼°å’Œæµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¯ä»¥ç”¨å®ƒä»¬è¿›è¡Œçš„å¤§å¤šæ•°å¸¸è§æ“ä½œã€‚

ä½ å¯ä»¥ç”¨æˆ‘ä¸ºè¿™ç¯‡æ–‡ç« åšçš„ç¬”è®°æœ¬ä½œä¸ºå‚è€ƒã€‚

æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼Œ

å¼—æœ—è¥¿æ–¯ç§‘Â·è¨ç»´é‡Œå¥¥