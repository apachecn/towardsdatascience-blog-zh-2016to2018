<html>
<head>
<title>How to use Dataset in TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">å¦‚ä½•åœ¨TensorFlowä¸­ä½¿ç”¨æ•°æ®é›†</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428?source=collection_archive---------0-----------------------#2018-02-06">https://towardsdatascience.com/how-to-use-dataset-in-tensorflow-c758ef9e4428?source=collection_archive---------0-----------------------#2018-02-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c6828cd52a765a6e50e819756da7b7d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1vYxIkzAjm82cROBYEwPHw.png"/></div></div></figure><div class=""/><p id="9a1d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf"> <em class="kz">æˆ‘åœ¨</em></strong><a class="ae la" href="https://www.linkedin.com/in/francesco-saverio-zuppichini-94659a150/?originalSubdomain=ch" rel="noopener ugc nofollow" target="_blank"><strong class="kd jf"><em class="kz">LinkedIn</em></strong></a><strong class="kd jf"><em class="kz">ï¼Œå¿«æ¥æ‰“ä¸ªæ‹›å‘¼</em> </strong>ğŸ‘‹</p><p id="3b13" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">å†…ç½®çš„è¾“å…¥ç®¡é“ã€‚å†ä¹Ÿä¸ç”¨â€œfeed-dictâ€äº†</p><p id="aa73" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf"> <em class="kz"> 16/02/2020:æˆ‘å·²ç»æ¢æˆPyTorch </em> </strong>ğŸ˜</p><p id="9378" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz"> 29/05/2019:æˆ‘ä¼šæŠŠæ•™ç¨‹æ›´æ–°åˆ°tf 2.0ğŸ˜(æˆ‘æ­£åœ¨å®Œæˆæˆ‘çš„ç¡•å£«è®ºæ–‡)</em></p><p id="c805" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">2018å¹´2æœˆ6æ—¥æ›´æ–°:æ·»åŠ äº†ç¬¬äºŒä¸ªå®Œæ•´ç¤ºä¾‹ï¼Œå°†csvç›´æ¥è¯»å…¥æ•°æ®é›†</em></p><p id="bac6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">2018å¹´5æœˆ25æ—¥æ›´æ–°:æ·»åŠ äº†ç¬¬äºŒä¸ªå®Œæ•´ç¤ºä¾‹ï¼Œå¸¦æœ‰ä¸€ä¸ª</em> <strong class="kd jf"> <em class="kz">å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨</em> </strong></p><p id="b2c5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">æ›´æ–°è‡³TensorFlow 1.8 </em></p><p id="c637" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æ‚¨åº”è¯¥çŸ¥é“ï¼Œ<code class="fe lb lc ld le b">feed-dict</code>æ˜¯å‘TensorFlowä¼ é€’ä¿¡æ¯çš„æœ€æ…¢æ–¹å¼ï¼Œå¿…é¡»é¿å…ä½¿ç”¨ã€‚å°†æ•°æ®è¾“å…¥æ¨¡å‹çš„æ­£ç¡®æ–¹æ³•æ˜¯ä½¿ç”¨è¾“å…¥ç®¡é“æ¥ç¡®ä¿GPUæ°¸è¿œä¸ä¼šç­‰å¾…æ–°çš„ä¸œè¥¿è¿›æ¥ã€‚</p><p id="eb59" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">å¹¸è¿çš„æ˜¯ï¼ŒTensorFlowæœ‰ä¸€ä¸ªå†…ç½®çš„APIï¼Œåä¸º<a class="ae la" href="https://www.tensorflow.org/programmers_guide/datasets" rel="noopener ugc nofollow" target="_blank"> Dataset </a>ï¼Œå¯ä»¥æ›´å®¹æ˜“åœ°å®Œæˆè¿™é¡¹ä»»åŠ¡ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†äº†è§£å¦‚ä½•åˆ›å»ºè¾“å…¥ç®¡é“ï¼Œä»¥åŠå¦‚ä½•é«˜æ•ˆåœ°å°†æ•°æ®è¾“å…¥æ¨¡å‹ã€‚</p><p id="ab9c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æœ¬æ–‡å°†è§£é‡Šæ•°æ®é›†çš„åŸºæœ¬æœºåˆ¶ï¼Œæ¶µç›–æœ€å¸¸è§çš„ç”¨ä¾‹ã€‚</p><p id="a258" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°jupyterç¬”è®°æœ¬çš„æ‰€æœ‰ä»£ç :</p><p id="1e68" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae la" href="https://github.com/FrancescoSaverioZuppichini/Tensorflow-Dataset-Tutorial/blob/master/dataset_tutorial.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/FrancescoSaverioZuppichini/tensor flow-Dataset-Tutorial/blob/master/Dataset _ Tutorial . ipynb</a></p><h1 id="2ba7" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">ä¸€èˆ¬æ¦‚è¿°</h1><p id="e98a" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">ä¸ºäº†ä½¿ç”¨æ•°æ®é›†ï¼Œæˆ‘ä»¬éœ€è¦ä¸‰ä¸ªæ­¥éª¤:</p><ul class=""><li id="6411" class="mi mj je kd b ke kf ki kj km mk kq ml ku mm ky mn mo mp mq bi translated"><strong class="kd jf">å¯¼å…¥æ•°æ®</strong>ã€‚ä»ä¸€äº›æ•°æ®åˆ›å»ºæ•°æ®é›†å®ä¾‹</li><li id="7614" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated">åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨ã€‚é€šè¿‡ä½¿ç”¨åˆ›å»ºçš„æ•°æ®é›†æ¥åˆ¶ä½œè¿­ä»£å™¨å®ä¾‹æ¥éå†æ•°æ®é›†</li><li id="a8e1" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf">æ¶ˆè´¹æ•°æ®</strong>ã€‚é€šè¿‡ä½¿ç”¨åˆ›å»ºçš„è¿­ä»£å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ä»æ•°æ®é›†ä¸­è·å–å…ƒç´ ï¼Œä»¥æä¾›ç»™æ¨¡å‹</li></ul><h1 id="72a5" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">å¯¼å…¥æ•°æ®</h1><p id="3d41" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">æˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸€äº›æ•°æ®æ”¾å…¥æˆ‘ä»¬çš„æ•°æ®é›†</p><h2 id="c87f" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">æ¥è‡ªnumpy</h2><p id="2001" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">è¿™æ˜¯å¸¸è§çš„æƒ…å†µï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªnumpyæ•°ç»„ï¼Œæˆ‘ä»¬æƒ³æŠŠå®ƒä¼ é€’ç»™tensorflowã€‚</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="94c1" class="mw lg je le b gy nq nr l ns nt"># create a random vector of shape (100,2)<br/>x = np.random.sample((100,2))<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span></pre><p id="ea94" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬ä¹Ÿå¯ä»¥ä¼ é€’ä¸æ­¢ä¸€ä¸ªnumpyæ•°ç»„ï¼Œä¸€ä¸ªç»å…¸çš„ä¾‹å­æ˜¯å½“æˆ‘ä»¬å°†ä¸€äº›æ•°æ®åˆ†æˆç‰¹å¾å’Œæ ‡ç­¾æ—¶</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="92da" class="mw lg je le b gy nq nr l ns nt">features, labels = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>dataset = tf.data.Dataset.from_tensor_slices((features,labels))</span></pre><h2 id="7209" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">æ¥è‡ªå¼ é‡</h2><p id="64a2" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€äº›å¼ é‡åˆå§‹åŒ–æˆ‘ä»¬çš„æ•°æ®é›†</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="778f" class="mw lg je le b gy nq nr l ns nt"># using a tensor<br/>dataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([100, 2]))</span></pre><h2 id="c832" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">ä»å ä½ç¬¦</h2><p id="f0e4" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">å½“æˆ‘ä»¬æƒ³è¦åŠ¨æ€åœ°æ”¹å˜æ•°æ®é›†ä¸­çš„æ•°æ®æ—¶ï¼Œè¿™æ˜¯å¾ˆæœ‰ç”¨çš„ï¼Œæˆ‘ä»¬å°†åœ¨åé¢çœ‹åˆ°å¦‚ä½•æ”¹å˜ã€‚</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="862e" class="mw lg je le b gy nq nr l ns nt">x = tf.placeholder(tf.float32, shape=[None,2])<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span></pre><h2 id="1d27" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">æ¥è‡ªå‘ç”µæœº</h2><p id="fefe" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»ç”Ÿæˆå™¨åˆå§‹åŒ–ä¸€ä¸ªæ•°æ®é›†ï¼Œå½“æˆ‘ä»¬æœ‰ä¸€ä¸ªä¸åŒå…ƒç´ é•¿åº¦çš„æ•°ç»„(ä¾‹å¦‚ä¸€ä¸ªåºåˆ—)æ—¶ï¼Œè¿™å¾ˆæœ‰ç”¨:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="dadf" class="mw lg je le b gy nq nr l ns nt"># from generator<br/>sequence = np.array([[[1]],[[2],[3]],[[3],[4],[5]]])</span><span id="c310" class="mw lg je le b gy nu nr l ns nt">def generator():<br/>    for el in sequence:<br/>        yield el</span><span id="6fe7" class="mw lg je le b gy nu nr l ns nt">dataset = tf.data.Dataset().batch(1).from_generator(generator,<br/>                                           output_types= tf.int64, <br/>                                           output_shapes=(tf.TensorShape([None, 1])))</span><span id="70f9" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator()<br/>el = iter.get_next()</span><span id="ad45" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(iter.initializer)<br/>    print(sess.run(el))<br/>    print(sess.run(el))<br/>    print(sess.run(el))</span></pre><p id="758d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¾“å‡º:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="fa24" class="mw lg je le b gy nq nr l ns nt">[[1]]<br/>[[2]<br/> [3]]<br/>[[3]<br/> [4]<br/> [5]]</span></pre><p id="4f2d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨è¿˜éœ€è¦æŒ‡å®šå°†ç”¨äºåˆ›å»ºæ­£ç¡®å¼ é‡çš„æ•°æ®çš„ç±»å‹å’Œå½¢çŠ¶ã€‚</p><h2 id="5e3f" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">ä»csvæ–‡ä»¶</h2><p id="c1d9" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">æ‚¨å¯ä»¥ç›´æ¥å°†csvæ–‡ä»¶è¯»å…¥æ•°æ®é›†ã€‚ä¾‹å¦‚ï¼Œæˆ‘æœ‰ä¸€ä¸ªcsvæ–‡ä»¶ï¼Œé‡Œé¢æœ‰æ¨æ–‡å’Œä»–ä»¬çš„æƒ…ç»ªã€‚</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/c6b42c4856e7250e61a751073c6db564.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*LE5N8IQTcgahmZt_odeo8Q.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">tweets.csv</figcaption></figure><p id="b807" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ç°åœ¨å¯ä»¥é€šè¿‡è°ƒç”¨<code class="fe lb lc ld le b">tf.contrib.data.make_csv_dataset</code>è½»æ¾åœ°ä»å®ƒåˆ›å»ºä¸€ä¸ª<code class="fe lb lc ld le b">Dataset</code>ã€‚è¯·æ³¨æ„ï¼Œè¿­ä»£å™¨å°†åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œä»¥keyä½œä¸ºåˆ—åï¼Œä»¥å¼ é‡çš„å½¢å¼ä½¿ç”¨æ­£ç¡®çš„è¡Œå€¼ã€‚</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ab21" class="mw lg je le b gy nq nr l ns nt"># load a csv<br/>CSV_PATH = './tweets.csv'<br/>dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)<br/>iter = dataset.make_one_shot_iterator()<br/>next = iter.get_next()<br/>print(next) # next is a dict with key=columns names and value=column data<br/>inputs, labels = next['text'], next['sentiment']</span><span id="34dd" class="mw lg je le b gy nu nr l ns nt">with  tf.Session() as sess:<br/>    sess.run([inputs, labels])</span></pre><p id="ff66" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe lb lc ld le b">next</code>åœ¨å“ªé‡Œ</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="8295" class="mw lg je le b gy nq nr l ns nt">{'sentiment': &lt;tf.Tensor 'IteratorGetNext_15:0' shape=(?,) dtype=int32&gt;, 'text': &lt;tf.Tensor 'IteratorGetNext_15:1' shape=(?,) dtype=string&gt;}</span></pre><h1 id="e56f" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">åˆ›å»ºè¿­ä»£å™¨</h1><p id="8aa5" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•åˆ›å»ºæ•°æ®é›†ï¼Œä½†æ˜¯å¦‚ä½•å–å›æˆ‘ä»¬çš„æ•°æ®å‘¢ï¼Ÿæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ä¸€ä¸ª<code class="fe lb lc ld le b">Iterator</code>ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿéå†æ•°æ®é›†å¹¶æ£€ç´¢æ•°æ®çš„çœŸå®å€¼ã€‚æœ‰å››ç§ç±»å‹çš„è¿­ä»£å™¨ã€‚</p><ul class=""><li id="5f45" class="mi mj je kd b ke kf ki kj km mk kq ml ku mm ky mn mo mp mq bi translated"><strong class="kd jf">ä¸€é’ˆã€‚</strong>å®ƒå¯ä»¥éå†ä¸€æ¬¡æ•°æ®é›†ï¼Œä½ <strong class="kd jf">ä¸èƒ½ç»™å®ƒ</strong>ä»»ä½•å€¼ã€‚</li><li id="cdcd" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf">å¯åˆå§‹åŒ–</strong>:å¯ä»¥åŠ¨æ€æ”¹å˜è°ƒç”¨å®ƒçš„<code class="fe lb lc ld le b">initializer</code>æ“ä½œï¼Œç”¨<code class="fe lb lc ld le b">feed_dict</code>ä¼ é€’æ–°æ•°æ®ã€‚å®ƒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªå¯ä»¥è£…æ»¡ä¸œè¥¿çš„æ¡¶ã€‚</li><li id="1fb7" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf">å¯é‡æ–°åˆå§‹åŒ–</strong>:å®ƒå¯ä»¥ä»ä¸åŒçš„<code class="fe lb lc ld le b">Dataset.</code>åˆå§‹åŒ–ï¼Œå½“ä½ æœ‰ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†éœ€è¦ä¸€äº›é¢å¤–çš„å˜æ¢ï¼Œä¾‹å¦‚shuffleï¼Œå’Œä¸€ä¸ªæµ‹è¯•æ•°æ®é›†æ—¶éå¸¸æœ‰ç”¨ã€‚è¿™å°±åƒä½¿ç”¨å¡”å¼èµ·é‡æœºæ¥é€‰æ‹©ä¸åŒçš„å®¹å™¨ã€‚</li><li id="d202" class="mi mj je kd b ke mr ki ms km mt kq mu ku mv ky mn mo mp mq bi translated"><strong class="kd jf"> Feedable </strong> : <strong class="kd jf"> </strong>å¯ä»¥ç”¨è¿­ä»£å™¨é€‰æ‹©ä½¿ç”¨ã€‚æŒ‰ç…§å‰é¢çš„ä¾‹å­ï¼Œå°±åƒä¸€ä¸ªå¡”åŠé€‰æ‹©ç”¨å“ªä¸ªå¡”åŠæ¥é€‰æ‹©æ‹¿å“ªä¸ªé›†è£…ç®±ã€‚åœ¨æˆ‘çœ‹æ¥æ˜¯æ²¡ç”¨çš„ã€‚</li></ul><h2 id="419b" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">ä¸€æ¬¡æ€§è¿­ä»£å™¨</h2><p id="1bf5" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">è¿™æ˜¯æœ€ç®€å•çš„è¿­ä»£å™¨ã€‚ä½¿ç”¨ç¬¬ä¸€ä¸ªä¾‹å­</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="1d3a" class="mw lg je le b gy nq nr l ns nt">x = np.random.sample((100,2))<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span><span id="da8c" class="mw lg je le b gy nu nr l ns nt"># create the iterator<br/>iter = dataset.make_one_shot_iterator()</span></pre><p id="8e08" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç„¶åä½ éœ€è¦è°ƒç”¨<code class="fe lb lc ld le b">get_next()</code>æ¥è·å–åŒ…å«ä½ çš„æ•°æ®çš„å¼ é‡</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5f65" class="mw lg je le b gy nq nr l ns nt">...<br/># create the iterator<br/>iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span></pre><p id="f142" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬å¯ä»¥è¿è¡Œ<code class="fe lb lc ld le b">el</code>æ¥æŸ¥çœ‹å®ƒçš„å€¼</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="496f" class="mw lg je le b gy nq nr l ns nt">with tf.Session() as sess:<br/>    print(sess.run(el)) # output: [ 0.42116176  0.40666069]</span></pre><h2 id="4fcf" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">å¯åˆå§‹åŒ–è¿­ä»£å™¨</h2><p id="ebe2" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">å¦‚æœæˆ‘ä»¬æƒ³è¦æ„å»ºä¸€ä¸ªåŠ¨æ€æ•°æ®é›†ï¼Œå¯ä»¥åœ¨è¿è¡Œæ—¶æ”¹å˜æ•°æ®æºï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€ä¸ªå¸¦æœ‰å ä½ç¬¦çš„æ•°æ®é›†ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šç”¨çš„<code class="fe lb lc ld le b">feed-dict</code>æœºåˆ¶åˆå§‹åŒ–å ä½ç¬¦ã€‚è¿™æ˜¯é€šè¿‡ä¸€ä¸ª<em class="kz">å¯åˆå§‹åŒ–çš„è¿­ä»£å™¨</em>å®Œæˆçš„ã€‚ä½¿ç”¨ä¸Šä¸€èŠ‚ä¸­çš„ç¤ºä¾‹ä¸‰</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="d1b6" class="mw lg je le b gy nq nr l ns nt"># using a placeholder<br/>x = tf.placeholder(tf.float32, shape=[None,2])<br/>dataset = tf.data.Dataset.from_tensor_slices(x)</span><span id="13bd" class="mw lg je le b gy nu nr l ns nt">data = np.random.sample((100,2))</span><span id="0187" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator() # create the iterator<br/>el = iter.get_next()</span><span id="aa1d" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    # feed the placeholder with data<br/>    sess.run(iter.initializer, feed_dict={ x: data }) <br/>    print(sess.run(el)) # output [ 0.52374458  0.71968478]</span></pre><p id="3fa6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¿™æ¬¡æˆ‘ä»¬å«<code class="fe lb lc ld le b">make_initializable_iterator</code>ã€‚ç„¶åï¼Œåœ¨<code class="fe lb lc ld le b">sess</code>èŒƒå›´å†…ï¼Œæˆ‘ä»¬è¿è¡Œ<code class="fe lb lc ld le b">initializer</code>æ“ä½œæ¥ä¼ é€’æˆ‘ä»¬çš„æ•°æ®ï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯ä¸€ä¸ªéšæœºçš„numpyæ•°ç»„ã€‚ã€‚</p><p id="3640" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æƒ³è±¡ä¸€ä¸‹ï¼Œç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒé›†å’Œä¸€ä¸ªæµ‹è¯•é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªçœŸå®çš„å¸¸è§åœºæ™¯:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="581f" class="mw lg je le b gy nq nr l ns nt">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.array([[1,2]]), np.array([[0]]))</span></pre><p id="c347" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç„¶åï¼Œæˆ‘ä»¬å°†è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååœ¨æµ‹è¯•æ•°æ®é›†ä¸Šè¯„ä¼°å®ƒï¼Œè¿™å¯ä»¥é€šè¿‡åœ¨è®­ç»ƒåå†æ¬¡åˆå§‹åŒ–è¿­ä»£å™¨æ¥å®Œæˆ</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ddc5" class="mw lg je le b gy nq nr l ns nt"># initializable iterator to switch between dataset<br/>EPOCHS = 10</span><span id="e6c1" class="mw lg je le b gy nu nr l ns nt">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/>dataset = tf.data.Dataset.from_tensor_slices((x, y))</span><span id="a8f4" class="mw lg je le b gy nu nr l ns nt">train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.array([[1,2]]), np.array([[0]]))</span><span id="b529" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator()<br/>features, labels = iter.get_next()</span><span id="1963" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>#     initialise iterator with train data<br/>    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1]})<br/>    for _ in range(EPOCHS):<br/>        sess.run([features, labels])<br/>#     switch to test data<br/>    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1]})<br/>    print(sess.run([features, labels]))</span></pre><h2 id="f9f6" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated"><strong class="ak">å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨</strong></h2><p id="dcc3" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">è¿™ä¸ªæ¦‚å¿µç±»ä¼¼äºä»¥å‰ï¼Œæˆ‘ä»¬è¦åœ¨æ•°æ®ä¹‹é—´åŠ¨æ€åˆ‡æ¢ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æ˜¯å‘åŒä¸€ä¸ªæ•°æ®é›†æä¾›æ–°æ•°æ®ï¼Œè€Œæ˜¯åˆ‡æ¢æ•°æ®é›†ã€‚å’Œä»¥å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ä¸ªè®­ç»ƒæ•°æ®é›†å’Œä¸€ä¸ªæµ‹è¯•æ•°æ®é›†</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ad4e" class="mw lg je le b gy nq nr l ns nt"># making fake data using numpy<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))</span></pre><p id="117a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸¤ä¸ªæ•°æ®é›†</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5807" class="mw lg je le b gy nq nr l ns nt"># create two datasets, one for training and one for test<br/>train_dataset = tf.data.Dataset.from_tensor_slices(train_data)<br/>test_dataset = tf.data.Dataset.from_tensor_slices(test_data)</span></pre><p id="9c7a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¿™å°±æ˜¯è¯€çªï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ³›å‹è¿­ä»£å™¨</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5034" class="mw lg je le b gy nq nr l ns nt"># create a iterator of the correct shape and type<br/>iter = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)</span></pre><p id="0001" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç„¶åæ˜¯ä¸¤ä¸ªåˆå§‹åŒ–æ“ä½œ:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="01dc" class="mw lg je le b gy nq nr l ns nt"># create the initialisation operations<br/>train_init_op = iter.make_initializer(train_dataset)<br/>test_init_op = iter.make_initializer(test_dataset)</span></pre><p id="dbd0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬åƒä»¥å‰ä¸€æ ·å¾—åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ </p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="73b2" class="mw lg je le b gy nq nr l ns nt">features, labels = iter.get_next()</span></pre><p id="c288" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„ä¼šè¯ç›´æ¥è¿è¡Œä¸¤ä¸ªåˆå§‹åŒ–æ“ä½œã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å¾—åˆ°:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="a1e9" class="mw lg je le b gy nq nr l ns nt"># Reinitializable iterator to switch between Datasets<br/>EPOCHS = 10<br/># making fake data using numpy<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))<br/># create two datasets, one for training and one for test<br/>train_dataset = tf.data.Dataset.from_tensor_slices(train_data)<br/>test_dataset = tf.data.Dataset.from_tensor_slices(test_data)<br/># create a iterator of the correct shape and type<br/>iter = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)<br/>features, labels = iter.get_next()<br/># create the initialisation operations<br/>train_init_op = iter.make_initializer(train_dataset)<br/>test_init_op = iter.make_initializer(test_dataset)</span><span id="62df" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(train_init_op) # switch to train dataset<br/>    for _ in range(EPOCHS):<br/>        sess.run([features, labels])<br/>    sess.run(test_init_op) # switch to val dataset<br/>    print(sess.run([features, labels]))</span></pre><h2 id="6090" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">å¯é¦ˆé€è¿­ä»£å™¨</h2><p id="32ad" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">è¿™éå¸¸ç±»ä¼¼äº<code class="fe lb lc ld le b">reinitializable</code>è¿­ä»£å™¨ï¼Œä½†æ˜¯å®ƒä¸æ˜¯åœ¨æ•°æ®é›†ä¹‹é—´åˆ‡æ¢ï¼Œè€Œæ˜¯åœ¨è¿­ä»£å™¨ä¹‹é—´åˆ‡æ¢ã€‚åœ¨æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ•°æ®é›†ä¹‹å</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="fea0" class="mw lg je le b gy nq nr l ns nt">train_dataset = tf.data.Dataset.from_tensor_slices((x,y))<br/>test_dataset = tf.data.Dataset.from_tensor_slices((x,y))</span></pre><p id="0a6d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ä¸€ä¸ªç”¨äºåŸ¹è®­ï¼Œä¸€ä¸ªç”¨äºæµ‹è¯•ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„è¿­ä»£å™¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬ä½¿ç”¨<code class="fe lb lc ld le b">initializable</code>è¿­ä»£å™¨ï¼Œä½†æ˜¯ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨<code class="fe lb lc ld le b">one shot</code>è¿­ä»£å™¨</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="82a0" class="mw lg je le b gy nq nr l ns nt">train_iterator = train_dataset.make_initializable_iterator()<br/>test_iterator = test_dataset.make_initializable_iterator()</span></pre><p id="add3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰å’Œ<code class="fe lb lc ld le b">handle</code>ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¯ä»¥åŠ¨æ€æ”¹å˜çš„å ä½ç¬¦ã€‚</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="3e79" class="mw lg je le b gy nq nr l ns nt">handle = tf.placeholder(tf.string, shape=[])</span></pre><p id="59dc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç„¶åï¼Œä¸ä¹‹å‰ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨æ•°æ®é›†çš„å½¢çŠ¶å®šä¹‰ä¸€ä¸ªæ³›å‹è¿­ä»£å™¨</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="10a3" class="mw lg je le b gy nq nr l ns nt">iter = tf.data.Iterator.from_string_handle(<br/>    handle, train_dataset.output_types, train_dataset.output_shapes)</span></pre><p id="b239" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç„¶åï¼Œæˆ‘ä»¬å¾—åˆ°ä¸‹ä¸€ä¸ªå…ƒç´ </p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="b224" class="mw lg je le b gy nq nr l ns nt">next_elements = iter.get_next()</span></pre><p id="cc53" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ä¸ºäº†åœ¨è¿­ä»£å™¨ä¹‹é—´åˆ‡æ¢ï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨<code class="fe lb lc ld le b">next_elemenents</code>æ“ä½œï¼Œå¹¶åœ¨feed_dictä¸­ä¼ é€’æ­£ç¡®çš„<code class="fe lb lc ld le b">handle</code>ã€‚ä¾‹å¦‚ï¼Œè¦ä»è®­ç»ƒé›†ä¸­è·å–ä¸€ä¸ªå…ƒç´ :</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="7032" class="mw lg je le b gy nq nr l ns nt">sess.run(next_elements, feed_dict = {handle: train_handle})</span></pre><p id="df57" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">å¦‚æœä½ æ­£åœ¨ä½¿ç”¨<code class="fe lb lc ld le b">initializable</code>è¿­ä»£å™¨ï¼Œå°±åƒæˆ‘ä»¬æ­£åœ¨åšçš„é‚£æ ·ï¼Œè®°å¾—åœ¨å¼€å§‹ä¹‹å‰åˆå§‹åŒ–å®ƒä»¬</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="05c0" class="mw lg je le b gy nq nr l ns nt">sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})<br/>    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})</span></pre><p id="125f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç»¼ä¸Šæ‰€è¿°ï¼Œæˆ‘ä»¬å¾—åˆ°:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="978d" class="mw lg je le b gy nq nr l ns nt"># feedable iterator to switch between iterators<br/>EPOCHS = 10<br/># making fake data using numpy<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((10,2)), np.random.sample((10,1)))<br/># create placeholder<br/>x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/># create two datasets, one for training and one for test<br/>train_dataset = tf.data.Dataset.from_tensor_slices((x,y))<br/>test_dataset = tf.data.Dataset.from_tensor_slices((x,y))<br/># create the iterators from the dataset<br/>train_iterator = train_dataset.make_initializable_iterator()<br/>test_iterator = test_dataset.make_initializable_iterator()<br/># same as in the doc <a class="ae la" href="https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator</a><br/>handle = tf.placeholder(tf.string, shape=[])<br/>iter = tf.data.Iterator.from_string_handle(<br/>    handle, train_dataset.output_types, train_dataset.output_shapes)<br/>next_elements = iter.get_next()</span><span id="5d0f" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    train_handle = sess.run(train_iterator.string_handle())<br/>    test_handle = sess.run(test_iterator.string_handle())<br/>    <br/>    # initialise iterators. <br/>    sess.run(train_iterator.initializer, feed_dict={ x: train_data[0], y: train_data[1]})<br/>    sess.run(test_iterator.initializer, feed_dict={ x: test_data[0], y: test_data[1]})<br/>    <br/>    for _ in range(EPOCHS):<br/>        x,y = sess.run(next_elements, feed_dict = {handle: train_handle})<br/>        print(x, y)<br/>        <br/>    x,y = sess.run(next_elements, feed_dict = {handle: test_handle})<br/>    print(x,y)</span></pre><h1 id="85f9" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">æ¶ˆè´¹æ•°æ®</h1><p id="8426" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">åœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¼šè¯æ¥æ‰“å°æ•°æ®é›†ä¸­<code class="fe lb lc ld le b">next</code>å…ƒç´ çš„å€¼ã€‚</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="28ae" class="mw lg je le b gy nq nr l ns nt">...<br/>next_el = iter.get_next()<br/>...<br/>print(sess.run(next_el)) # will output the current element</span></pre><p id="55e4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ä¸ºäº†å°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹ï¼Œæˆ‘ä»¬å¿…é¡»ä¼ é€’ä»<code class="fe lb lc ld le b">get_next()</code>ç”Ÿæˆçš„å¼ é‡</p><p id="340f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªnumpyæ•°ç»„çš„æ•°æ®é›†ï¼Œä½¿ç”¨äº†ç¬¬ä¸€éƒ¨åˆ†ä¸­çš„ç›¸åŒç¤ºä¾‹ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬éœ€è¦å°†<code class="fe lb lc ld le b">.random.sample</code>åŒ…è£…åœ¨å¦ä¸€ä¸ªnumpyæ•°ç»„ä¸­ï¼Œä»¥æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œæˆ‘ä»¬éœ€è¦è¿™ä¸ªç»´åº¦æ¥æ‰¹é‡å¤„ç†æ•°æ®</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="1fdf" class="mw lg je le b gy nq nr l ns nt"># using two numpy arrays<br/>features, labels = (np.array([np.random.sample((100,2))]), <br/>                    np.array([np.random.sample((100,1))]))</span><span id="f3cb" class="mw lg je le b gy nu nr l ns nt">dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)</span></pre><p id="f5f5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç„¶ååƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè¿­ä»£å™¨</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="0643" class="mw lg je le b gy nq nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>x, y = iter.get_next()</span></pre><p id="2b4f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬åˆ¶ä½œä¸€ä¸ªæ¨¡å‹ï¼Œä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œ</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="d506" class="mw lg je le b gy nq nr l ns nt"># make a simple model<br/>net = tf.layers.dense(x, 8) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8)<br/>prediction = tf.layers.dense(net, 1)</span><span id="eb7b" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span></pre><p id="14aa" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬<strong class="kd jf">ç›´æ¥</strong>ä½¿ç”¨æ¥è‡ª<code class="fe lb lc ld le b">iter.get_next()</code>çš„å¼ é‡ä½œä¸ºç¬¬ä¸€å±‚çš„è¾“å…¥ï¼Œå¹¶ä½œä¸ºæŸå¤±å‡½æ•°çš„æ ‡ç­¾ã€‚åŒ…è£…åœ¨ä¸€èµ·:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="5e8a" class="mw lg je le b gy nq nr l ns nt">EPOCHS = 10<br/>BATCH_SIZE = 16<br/># using two numpy arrays<br/>features, labels = (np.array([np.random.sample((100,2))]), <br/>                    np.array([np.random.sample((100,1))]))</span><span id="7bca" class="mw lg je le b gy nu nr l ns nt">dataset = tf.data.Dataset.from_tensor_slices((features,labels)).repeat().batch(BATCH_SIZE)</span><span id="7fb5" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>x, y = iter.get_next()</span><span id="489a" class="mw lg je le b gy nu nr l ns nt"># make a simple model<br/>net = tf.layers.dense(x, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8, activation=tf.tanh)<br/>prediction = tf.layers.dense(net, 1, activation=tf.tanh)</span><span id="4d52" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, y) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span><span id="b8cd" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    for i in range(EPOCHS):<br/>        _, loss_value = sess.run([train_op, loss])<br/>        print("Iter: {}, Loss: {:.4f}".format(i, loss_value))</span></pre><p id="2d5f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¾“å‡º:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="6cb0" class="mw lg je le b gy nq nr l ns nt">Iter: 0, Loss: 0.1328 <br/>Iter: 1, Loss: 0.1312 <br/>Iter: 2, Loss: 0.1296 <br/>Iter: 3, Loss: 0.1281 <br/>Iter: 4, Loss: 0.1267 <br/>Iter: 5, Loss: 0.1254 <br/>Iter: 6, Loss: 0.1242 <br/>Iter: 7, Loss: 0.1231 <br/>Iter: 8, Loss: 0.1220 <br/>Iter: 9, Loss: 0.1210</span></pre><h1 id="ce25" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">æœ‰ç”¨çš„ä¸œè¥¿</h1><h2 id="2f21" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">ä¸€æ‰¹</h2><p id="8843" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">é€šå¸¸æ‰¹å¤„ç†æ•°æ®æ˜¯ä¸€ä»¶ç—›è‹¦çš„äº‹æƒ…ï¼Œæœ‰äº†<code class="fe lb lc ld le b">Dataset </code> APIï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ–¹æ³•<code class="fe lb lc ld le b">batch(BATCH_SIZE)</code>ä»¥æä¾›çš„å¤§å°è‡ªåŠ¨æ‰¹å¤„ç†æ•°æ®é›†ã€‚é»˜è®¤å€¼ä¸º1ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ‰¹é‡å¤§å°ä¸º4</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="770a" class="mw lg je le b gy nq nr l ns nt"># BATCHING<br/>BATCH_SIZE = 4<br/>x = np.random.sample((100,2))<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x).batch(BATCH_SIZE)</span><span id="fc7e" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span><span id="a9c4" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    print(sess.run(el)) </span></pre><p id="2023" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¾“å‡º:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="9f17" class="mw lg je le b gy nq nr l ns nt">[[ 0.65686128  0.99373963]<br/> [ 0.69690451  0.32446826]<br/> [ 0.57148422  0.68688242]<br/> [ 0.20335116  0.82473219]]</span></pre><h2 id="cfbd" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">é‡å¤</h2><p id="5fdb" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">ä½¿ç”¨<code class="fe lb lc ld le b">.repeat()</code>,æˆ‘ä»¬å¯ä»¥æŒ‡å®šæ•°æ®é›†è¿­ä»£çš„æ¬¡æ•°ã€‚å¦‚æœæ²¡æœ‰ä¼ é€’å‚æ•°ï¼Œå®ƒå°†æ°¸è¿œå¾ªç¯ä¸‹å»ï¼Œé€šå¸¸æœ€å¥½æ˜¯æ°¸è¿œå¾ªç¯ä¸‹å»ï¼Œç”¨æ ‡å‡†å¾ªç¯ç›´æ¥æ§åˆ¶å†å…ƒæ•°ã€‚</p><h2 id="9598" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">æ´—ç‰Œ</h2><p id="7450" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">æˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨æ–¹æ³•<code class="fe lb lc ld le b">shuffle()</code>æ¥æ‰“ä¹±æ•°æ®é›†ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¯ä¸ªæ—¶æœŸéƒ½ä¼šæ‰“ä¹±æ•°æ®é›†ã€‚</p><p id="7de7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">è®°ä½:æ‰“ä¹±æ•°æ®é›†å¯¹äºé¿å…è¿‡åº¦é€‚åº”éå¸¸é‡è¦ã€‚</em></p><p id="8af5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æˆ‘ä»¬è¿˜å¯ä»¥è®¾ç½®å‚æ•°<code class="fe lb lc ld le b">buffer_size</code>ï¼Œä¸€ä¸ªå›ºå®šå¤§å°çš„ç¼“å†²åŒºï¼Œä¸‹ä¸€ä¸ªå…ƒç´ å°†ä»å…¶ä¸­ç»Ÿä¸€é€‰æ‹©ã€‚ç¤ºä¾‹:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="0dc4" class="mw lg je le b gy nq nr l ns nt"># BATCHING<br/>BATCH_SIZE = 4<br/>x = np.array([[1],[2],[3],[4]])<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)<br/>dataset = dataset.shuffle(buffer_size=100)<br/>dataset = dataset.batch(BATCH_SIZE)</span><span id="f7d7" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span><span id="5882" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    print(sess.run(el))</span></pre><p id="8621" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">é¦–æ¬¡è¿è¡Œè¾“å‡º:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="7448" class="mw lg je le b gy nq nr l ns nt">[[4]<br/> [2]<br/> [3]<br/> [1]]</span></pre><p id="fd3a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ç¬¬äºŒè½®è¾“å‡º:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="096e" class="mw lg je le b gy nq nr l ns nt">[[3]<br/> [1]<br/> [2]<br/> [4]]</span></pre><p id="f06d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æ²¡é”™ã€‚å®ƒè¢«æ´—ç‰Œäº†ã€‚å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥è®¾ç½®<code class="fe lb lc ld le b">seed</code>å‚æ•°ã€‚</p><h1 id="b01e" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">åœ°å›¾</h1><p id="c92c" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">æ‚¨å¯ä»¥ä½¿ç”¨<code class="fe lb lc ld le b">map</code>æ–¹æ³•å°†è‡ªå®šä¹‰å‡½æ•°åº”ç”¨äºæ•°æ®é›†çš„æ¯ä¸ªæˆå‘˜ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªå…ƒç´ ä¹˜ä»¥2:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="655f" class="mw lg je le b gy nq nr l ns nt"># MAP<br/>x = np.array([[1],[2],[3],[4]])<br/># make a dataset from a numpy array<br/>dataset = tf.data.Dataset.from_tensor_slices(x)<br/>dataset = dataset.map(lambda x: x*2)</span><span id="7c19" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_one_shot_iterator()<br/>el = iter.get_next()</span><span id="36e4" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>#     this will run forever<br/>        for _ in range(len(x)):<br/>            print(sess.run(el))</span></pre><p id="55c3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¾“å‡º:</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="bf55" class="mw lg je le b gy nq nr l ns nt">[2]<br/>[4]<br/>[6]<br/>[8]</span></pre><h1 id="02e2" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">å®Œæ•´ç¤ºä¾‹</h1><h2 id="ac00" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated"><strong class="ak">å¯åˆå§‹åŒ–çš„</strong>è¿­ä»£å™¨</h2><p id="1a9b" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰¹å¤„ç†æ¥è®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª<strong class="kd jf">å¯åˆå§‹åŒ–çš„è¿­ä»£å™¨</strong>åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¹‹é—´åˆ‡æ¢</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="0c0f" class="mw lg je le b gy nq nr l ns nt"># Wrapping all together -&gt; Switch between train and test set using Initializable iterator<br/>EPOCHS = 10<br/># create a placeholder to dynamically switch between batch sizes<br/>batch_size = tf.placeholder(tf.int64)</span><span id="aba9" class="mw lg je le b gy nu nr l ns nt">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/>dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size).repeat()</span><span id="c369" class="mw lg je le b gy nu nr l ns nt"># using two numpy arrays<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((20,2)), np.random.sample((20,1)))</span><span id="c6bd" class="mw lg je le b gy nu nr l ns nt">iter = dataset.make_initializable_iterator()<br/>features, labels = iter.get_next()<br/># make a simple model<br/>net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8, activation=tf.tanh)<br/>prediction = tf.layers.dense(net, 1, activation=tf.tanh)</span><span id="99eb" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span><span id="6a05" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    # initialise iterator with train data<br/>    sess.run(iter.initializer, feed_dict={ x: train_data[0], y: train_data[1], batch_size: BATCH_SIZE})<br/>    print('Training...')<br/>    for i in range(EPOCHS):<br/>        tot_loss = 0<br/>        for _ in range(n_batches):<br/>            _, loss_value = sess.run([train_op, loss])<br/>            tot_loss += loss_value<br/>        print("Iter: {}, Loss: {:.4f}".format(i, tot_loss / n_batches))<br/>    # initialise iterator with test data<br/>    sess.run(iter.initializer, feed_dict={ x: test_data[0], y: test_data[1], batch_size: test_data[0].shape[0]})<br/>    print('Test Loss: {:4f}'.format(sess.run(loss)))</span></pre><p id="a4b0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªæ‰¹é‡å¤§å°çš„å ä½ç¬¦ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒååŠ¨æ€åˆ‡æ¢å®ƒ</strong></p><p id="6e20" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">è¾“å‡º</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="abe0" class="mw lg je le b gy nq nr l ns nt">Training...<br/>Iter: 0, Loss: 0.2977<br/>Iter: 1, Loss: 0.2152<br/>Iter: 2, Loss: 0.1787<br/>Iter: 3, Loss: 0.1597<br/>Iter: 4, Loss: 0.1277<br/>Iter: 5, Loss: 0.1334<br/>Iter: 6, Loss: 0.1000<br/>Iter: 7, Loss: 0.1154<br/>Iter: 8, Loss: 0.0989<br/>Iter: 9, Loss: 0.0948<br/>Test Loss: 0.082150</span></pre><h2 id="d1a5" class="mw lg je bd lh mx my dn ll mz na dp lp km nb nc lt kq nd ne lx ku nf ng mb nh bi translated">å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨</h2><p id="da06" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰¹å¤„ç†è®­ç»ƒä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼Œå¹¶ä¸”æˆ‘ä»¬ä½¿ç”¨<strong class="kd jf">å¯é‡æ–°åˆå§‹åŒ–çš„è¿­ä»£å™¨</strong>åœ¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä¹‹é—´åˆ‡æ¢</p><pre class="ni nj nk nl gt nm le nn no aw np bi"><span id="ef22" class="mw lg je le b gy nq nr l ns nt"># Wrapping all together -&gt; Switch between train and test set using Reinitializable iterator<br/>EPOCHS = 10<br/># create a placeholder to dynamically switch between batch sizes<br/>batch_size = tf.placeholder(tf.int64)</span><span id="9b9e" class="mw lg je le b gy nu nr l ns nt">x, y = tf.placeholder(tf.float32, shape=[None,2]), tf.placeholder(tf.float32, shape=[None,1])<br/>train_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size).repeat()<br/>test_dataset = tf.data.Dataset.from_tensor_slices((x,y)).batch(batch_size) # always batch even if you want to one shot it<br/># using two numpy arrays<br/>train_data = (np.random.sample((100,2)), np.random.sample((100,1)))<br/>test_data = (np.random.sample((20,2)), np.random.sample((20,1)))</span><span id="96b5" class="mw lg je le b gy nu nr l ns nt"># create a iterator of the correct shape and type<br/>iter = tf.data.Iterator.from_structure(train_dataset.output_types,<br/>                                           train_dataset.output_shapes)<br/>features, labels = iter.get_next()<br/># create the initialisation operations<br/>train_init_op = iter.make_initializer(train_dataset)<br/>test_init_op = iter.make_initializer(test_dataset)</span><span id="3a30" class="mw lg je le b gy nu nr l ns nt"># make a simple model<br/>net = tf.layers.dense(features, 8, activation=tf.tanh) # pass the first value from iter.get_next() as input<br/>net = tf.layers.dense(net, 8, activation=tf.tanh)<br/>prediction = tf.layers.dense(net, 1, activation=tf.tanh)</span><span id="e0a3" class="mw lg je le b gy nu nr l ns nt">loss = tf.losses.mean_squared_error(prediction, labels) # pass the second value from iter.get_net() as label<br/>train_op = tf.train.AdamOptimizer().minimize(loss)</span><span id="7b18" class="mw lg je le b gy nu nr l ns nt">with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    # initialise iterator with train data<br/>    sess.run(train_init_op, feed_dict = {x : train_data[0], y: train_data[1], batch_size: 16})<br/>    print('Training...')<br/>    for i in range(EPOCHS):<br/>        tot_loss = 0<br/>        for _ in range(n_batches):<br/>            _, loss_value = sess.run([train_op, loss])<br/>            tot_loss += loss_value<br/>        print("Iter: {}, Loss: {:.4f}".format(i, tot_loss / n_batches))<br/>    # initialise iterator with test data<br/>    sess.run(test_init_op, feed_dict = {x : test_data[0], y: test_data[1], batch_size:len(test_data[0])})<br/>    print('Test Loss: {:4f}'.format(sess.run(loss)))</span></pre><h1 id="b6d7" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">å…¶ä»–èµ„æº</h1><p id="e071" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated">å¼ é‡æµæ•°æ®é›†æ•™ç¨‹:<a class="ae la" href="https://www.tensorflow.org/programmers_guide/datasets" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/programmers_guide/datasets</a></p><p id="78e0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æ•°æ®é›†æ–‡æ¡£:</p><p id="21c9" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae la" href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/data/Dataset</a></p><h1 id="efb9" class="lf lg je bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">ç»“è®º</h1><p id="fbfa" class="pw-post-body-paragraph kb kc je kd b ke md kg kh ki me kk kl km mf ko kp kq mg ks kt ku mh kw kx ky im bi translated"><code class="fe lb lc ld le b">Dataset</code> APIä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§å¿«é€Ÿè€Œå¥å£®çš„æ–¹å¼æ¥åˆ›å»ºä¼˜åŒ–çš„è¾“å…¥ç®¡é“ï¼Œä»¥è®­ç»ƒã€è¯„ä¼°å’Œæµ‹è¯•æˆ‘ä»¬çš„æ¨¡å‹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¯ä»¥ç”¨å®ƒä»¬è¿›è¡Œçš„å¤§å¤šæ•°å¸¸è§æ“ä½œã€‚</p><p id="9559" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">ä½ å¯ä»¥ç”¨æˆ‘ä¸ºè¿™ç¯‡æ–‡ç« åšçš„ç¬”è®°æœ¬ä½œä¸ºå‚è€ƒã€‚</p><p id="b64a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">æ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼Œ</p><p id="7658" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">å¼—æœ—è¥¿æ–¯ç§‘Â·è¨ç»´é‡Œå¥¥</p></div></div>    
</body>
</html>