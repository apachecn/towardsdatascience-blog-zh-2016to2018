# äººå·¥æ™ºèƒ½é‡è§è‰ºæœ¯:ç¥ç»ä¼ é€’é£æ ¼

> åŸæ–‡ï¼š<https://towardsdatascience.com/artificial-intelligence-meets-art-neural-transfer-style-50e1c07aa7f7?source=collection_archive---------7----------------------->

![](img/0becfa773bde9af462af48fdbc03de3b.png)

Hokusai in Boston

# ä»‹ç»

ç¥ç»è½¬ç§»é£æ ¼æ˜¯äººå·¥æ™ºèƒ½åœ¨åˆ›é€ æ€§èƒŒæ™¯ä¸‹æœ€ä»¤äººæƒŠå¹çš„åº”ç”¨ä¹‹ä¸€ã€‚åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•å°†è‰ºæœ¯ç»˜ç”»é£æ ¼è½¬ç§»åˆ°é€‰å®šçš„å›¾åƒä¸Šï¼Œåˆ›é€ å‡ºä»¤äººæƒŠå¹çš„æ•ˆæœã€‚Leon A. Gatys ç­‰äººåœ¨ 2015 å¹´çš„è®ºæ–‡ [*ä¸­æ„æ€äº†**ç¥ç»ä¼ é€’é£æ ¼**çš„æ¦‚å¿µï¼Œä¸€ç§è‰ºæœ¯é£æ ¼*](https://arxiv.org/abs/1508.06576) çš„ç¥ç»ç®—æ³•ã€‚åœ¨é‚£ä¹‹åï¼Œè®¸å¤šç ”ç©¶äººå‘˜åº”ç”¨å¹¶æ”¹è¿›äº†è¿™ç§æ–¹æ³•ï¼Œå¢åŠ äº†æŸå¤±çš„å…ƒç´ ï¼Œå°è¯•äº†ä¸åŒçš„ä¼˜åŒ–å™¨ï¼Œå¹¶è¯•éªŒäº†ç”¨äºæ­¤ç›®çš„çš„ä¸åŒç¥ç»ç½‘ç»œã€‚
å°½ç®¡å¦‚æ­¤ï¼ŒåŸå§‹è®ºæ–‡ä»ç„¶æ˜¯ç†è§£è¿™ä¸€æ¦‚å¿µçš„æœ€ä½³æ¥æºï¼ŒVGG16 å’Œ VGG19 ç½‘ç»œæ˜¯è¿™æ–¹é¢æœ€å¸¸ç”¨çš„æ¨¡å‹ã€‚è¿™ç§é€‰æ‹©æ˜¯ä¸å¯»å¸¸çš„ï¼Œè€ƒè™‘åˆ°ä¸¤è€…éƒ½è¢«æœ€è¿‘çš„ç½‘ç»œè¶…è¶Šï¼Œåœ¨é£æ ¼è½¬ç§»ä¸­å®ç°çš„æœ€é«˜æ€§èƒ½è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚

å®Œæ•´ä»£ç å¯ä»¥æŸ¥çœ‹è¿™ä¸ª [**GitHub åº“**](https://github.com/maurock/neural_transfer_style) ã€‚

# å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ

è¿™ç§æŠ€æœ¯çš„ç›®æ ‡æ˜¯å°†å›¾åƒçš„æ ·å¼(æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ ·å¼å›¾åƒâ€)åº”ç”¨åˆ°ç›®æ ‡å›¾åƒï¼Œä¿ç•™åè€…çš„å†…å®¹ã€‚è®©æˆ‘ä»¬å®šä¹‰è¿™ä¸¤ä¸ªæœ¯è¯­:

*   **é£æ ¼**æ˜¯å›¾åƒä¸­çš„çº¹ç†å’Œè§†è§‰æ¨¡å¼ã€‚ä¸€ä¸ªä¾‹å­æ˜¯è‰ºæœ¯å®¶çš„ç¬”è§¦ã€‚
*   **å†…å®¹**æ˜¯ä¸€å¹…å›¾åƒçš„å®è§‚ç»“æ„ã€‚äººã€å»ºç­‘ç‰©ã€ç‰©ä½“éƒ½æ˜¯å›¾åƒå†…å®¹çš„ä¾‹å­ã€‚

ä»¤äººæƒŠå¹çš„æ•ˆæœå¦‚ä¸‹æ‰€ç¤º:

![](img/4da9bac3d1d2ce1f495d9d539a411e9b.png)

> ä½ æƒ³çœ‹åˆ°æ›´å¤šçš„æ•ˆæœå—ï¼Ÿåœ¨æ–‡ç« çš„æœ€åæ£€æŸ¥ä»–ä»¬ï¼

è®©æˆ‘ä»¬çœ‹çœ‹é«˜çº§æ­¥éª¤:

*   é€‰æ‹©è¦æ ·å¼åŒ–çš„å›¾åƒ
*   é€‰æ‹©æ ·å¼å‚è€ƒå›¾åƒã€‚é€šå¸¸ï¼Œè¿™æ˜¯ä¸€å¹…é£æ ¼å¥‡ç‰¹ä¸”æ˜“äºè¾¨è®¤çš„ç”»ã€‚
*   åˆå§‹åŒ–é¢„è®­ç»ƒçš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¹¶è·å¾—ä¸­é—´å±‚çš„ç‰¹å¾è¡¨ç¤ºã€‚å®Œæˆè¯¥æ­¥éª¤æ˜¯ä¸ºäº†å®ç°å†…å®¹å›¾åƒå’Œæ ·å¼å›¾åƒçš„è¡¨ç¤ºã€‚åœ¨å†…å®¹å›¾åƒä¸­ï¼Œæœ€å¥½çš„é€‰æ‹©æ˜¯è·å¾—æœ€é«˜å±‚çš„ç‰¹å¾è¡¨ç¤ºï¼Œå› ä¸ºå®ƒä»¬åŒ…å«å…³äºå›¾åƒå®è§‚ç»“æ„çš„ä¿¡æ¯ã€‚å¯¹äºæ ·å¼å‚è€ƒå½±åƒï¼Œä»ä¸åŒæ¯”ä¾‹çš„å¤šä¸ªå›¾å±‚ä¸­è·å–è¦ç´ åˆ¶å›¾è¡¨è¾¾ã€‚
*   å°†æœ€å°åŒ–çš„æŸå¤±å‡½æ•°å®šä¹‰ä¸º*å†…å®¹æŸå¤±*ã€*é£æ ¼æŸå¤±*å’Œ*å˜åŒ–æŸå¤±*ä¹‹å’Œã€‚æ¯æ¬¡è¿­ä»£ï¼Œä¼˜åŒ–å™¨éƒ½ä¼šç”Ÿæˆä¸€å¹…å›¾åƒã€‚å†…å®¹æŸå¤±æ˜¯ç”Ÿæˆå›¾åƒå’Œå†…å®¹å›¾åƒä¹‹é—´çš„å·®å¼‚(l2 å½’ä¸€åŒ–)ï¼Œè€Œæ ·å¼æŸå¤±æ˜¯ç”Ÿæˆå›¾åƒå’Œæ ·å¼ä¹‹é—´çš„å·®å¼‚ã€‚æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°è¿™äº›å˜é‡æ˜¯å¦‚ä½•è¢«æ•°å­¦å®šä¹‰çš„ã€‚
*   é‡å¤æœ€å°åŒ–æŸå¤±

# å¤„ç†å’Œå–æ¶ˆå¤„ç†å›¾åƒ

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æ ¼å¼åŒ–æˆ‘ä»¬çš„å›¾åƒä»¥ä¾›æˆ‘ä»¬çš„ç½‘ç»œä½¿ç”¨ã€‚æˆ‘ä»¬è¦ç”¨çš„ CNN æ˜¯é¢„å…ˆè®­ç»ƒå¥½çš„ VGG19 convnetã€‚å½“æˆ‘ä»¬å°†å›¾åƒå¤„ç†æˆå…¼å®¹çš„æ•°ç»„æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¯¹ç”Ÿæˆçš„å›¾åƒè¿›è¡Œè§£å¤„ç†ï¼Œä» BGR æ ¼å¼åˆ‡æ¢åˆ° RGB æ ¼å¼ã€‚è®©æˆ‘ä»¬æ„å»ºä¸¤ä¸ªè¾…åŠ©å‡½æ•°æ¥å®ç°è¿™ä¸€ç‚¹:

```
# Preprocessing image to make it compatible with the VGG19 model
**def** **preprocess_image**(image_path):
    img = load_img(image_path, target_size=(resized_width, resized_height))
    img = img_to_array(img)
    img = np.expand_dims(img, axis=**0**)
    img = vgg19.preprocess_input(img)
    **return** img

# Function to convert a tensor into an image
**def** **deprocess_image**(x):
    x = x.reshape((resized_width, resized_height, **3**))

    # Remove zero-center by mean pixel. Necessary when working with VGG model
    x[:, :, **0**] += **103.939**
    x[:, :, **1**] += **116.779**
    x[:, :, **2**] += **123.68**

    # Format BGR->RGB
    x = x[:, :, ::-**1**]
    x = np.clip(x, **0**, **255**).astype('uint8')
    **return** x
```

# å†…å®¹æŸå¤±

å†…å®¹æŸå¤±å°†ä¸»è¾“å…¥å›¾åƒçš„å†…å®¹ä¿ç•™åˆ°æ ·å¼ä¸­ã€‚ç”±äºå·ç§¯ç¥ç»ç½‘ç»œçš„è¾ƒé«˜å±‚åŒ…å«å›¾åƒå®è§‚ç»“æ„çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬å°†å†…å®¹æŸå¤±è®¡ç®—ä¸ºè¾“å…¥å›¾åƒçš„æœ€é«˜å±‚çš„è¾“å‡ºå’Œç”Ÿæˆå›¾åƒçš„ç›¸åŒå±‚ä¹‹é—´çš„å·®å¼‚(l2 å½’ä¸€åŒ–)ã€‚
å†…å®¹æŸå¤±å®šä¹‰ä¸º:

![](img/9417fb28773fcddc8dcebca4721a8a25.png)

Content loss

åœ¨ç­‰å¼ä¸­ï¼Œ *F* æ˜¯å†…å®¹å›¾åƒçš„ç‰¹å¾è¡¨ç¤º(å½“æˆ‘ä»¬è¿è¡Œæˆ‘ä»¬çš„è¾“å…¥å›¾åƒæ—¶ï¼Œç½‘ç»œè¾“å‡ºçš„å†…å®¹)ï¼Œè€Œ *P* æ˜¯åœ¨ç‰¹å®šéšè—å±‚ *l* ç”Ÿæˆçš„å›¾åƒçš„ç‰¹å¾è¡¨ç¤ºã€‚
å®ç°å¦‚ä¸‹:

```
# The content loss maintains the features of the content image in the generated image.
**def** **content_loss**(layer_features):
    base_image_features = layer_features[**0**, :, :, :]
    combination_features = layer_features[**2**, :, :, :]
    **return** K.sum(K.square(combination_features - base_image_features))
```

# é£æ ¼ä¸§å¤±

ç†è§£é£æ ¼æŸå¤±ä¸åƒç†è§£å†…å®¹æŸå¤±é‚£ä¹ˆç®€å•ã€‚ç›®æ ‡æ˜¯åœ¨æ–°ç”Ÿæˆçš„å›¾åƒä¸­ä¿ç•™å›¾åƒçš„æ ·å¼(å³ï¼Œä½œä¸ºç¬”è§¦çš„è§†è§‰å›¾æ¡ˆ)ã€‚åœ¨å‰ä¸€ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†ä¸­é—´å±‚çš„åŸå§‹è¾“å‡ºã€‚è¿™é‡Œï¼Œæˆ‘ä»¬æ¯”è¾ƒæ ·å¼å‚è€ƒå›¾åƒå’Œç”Ÿæˆå›¾åƒçš„ç‰¹å®šå±‚çš„ Gram çŸ©é˜µä¹‹é—´çš„å·®å¼‚ã€‚ **Gram çŸ©é˜µ**è¢«å®šä¹‰ä¸ºç»™å®šå±‚çš„çŸ¢é‡åŒ–ç‰¹å¾å›¾ä¹‹é—´çš„å†…ç§¯ã€‚çŸ©é˜µçš„æ„ä¹‰åœ¨äºæ•æ‰å±‚ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§ã€‚è®¡ç®—å¤šä¸ªå±‚çš„æŸå¤±å…è®¸åœ¨æ ·å¼å›¾åƒå’Œç”Ÿæˆçš„å›¾åƒä¹‹é—´ä¿ç•™ä¸åŒå±‚ä¸­å†…éƒ¨ç›¸å…³çš„ç›¸ä¼¼ç‰¹å¾ã€‚
å•å±‚çš„é£æ ¼æŸå¤±è®¡ç®—å¦‚ä¸‹:

![](img/b2ba623ff3bbdded849f686be4634cf5.png)

Style loss per layer

åœ¨ç­‰å¼ä¸­ï¼Œ *A* æ˜¯æ ·å¼å›¾åƒçš„ Gram çŸ©é˜µï¼Œ *G* æ˜¯ç”Ÿæˆçš„å›¾åƒçš„ Gram çŸ©é˜µï¼Œä¸¤è€…éƒ½ä¸ç»™å®šçš„å±‚æœ‰å…³ã€‚ *N* å’Œ *M* ä¸ºæ ·å¼å›¾åƒçš„å®½åº¦å’Œé«˜åº¦ã€‚
åœ¨ç­‰å¼ä¸­ï¼Œ *A* æ˜¯é£æ ¼å›¾åƒçš„å…‹çŸ©é˜µï¼Œ *G* æ˜¯ç”Ÿæˆçš„å›¾åƒçš„å…‹çŸ©é˜µï¼Œä¸¤è€…éƒ½ä¸ç»™å®šçš„å±‚æœ‰å…³ã€‚ *N* å’Œ *M* ä¸ºæ ·å¼å›¾åƒçš„å®½åº¦å’Œé«˜åº¦ã€‚
é¦–å…ˆä¸ºæ¯ä¸ªå•ç‹¬çš„å±‚è®¡ç®—æ ·å¼æŸå¤±ï¼Œç„¶åå°†å…¶åº”ç”¨äºè¢«è®¤ä¸ºæ˜¯å¯¹æ ·å¼å»ºæ¨¡çš„æ¯ä¸ªå±‚ã€‚è®©æˆ‘ä»¬æ¥å®ç°å®ƒ:

```
# The gram matrix of an image tensor is the inner product between the vectorized feature map in a layer.
# It is used to compute the style loss, minimizing the mean squared distance between the feature correlation map of the style image
# and the input image
**def** **gram_matrix**(x):
    features = K.batch_flatten(K.permute_dimensions(x, (**2**, **0**, **1**)))
    gram = K.dot(features, K.transpose(features))
    **return** gram

# The style_loss_per_layer represents the loss between the style of the style reference image and the generated image.
# It depends on the gram matrices of feature maps from the style reference image and from the generated image.
**def** **style_loss_per_layer**(style, combination):
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = **3**
    size = resized_width * resized_height
    **return** K.sum(K.square(S - C)) / (**4.** * (channels ** **2**) * (size ** **2**))

# The total_style_loss represents the total loss between the style of the style reference image and the generated image,
# taking into account all the layers considered for the style transfer, related to the style reference image.
**def** **total_style_loss**(feature_layers):
    loss = K.variable(**0.**)
    **for** layer_name **in** feature_layers:
        layer_features = outputs_dict[layer_name]
        style_reference_features = layer_features[**1**, :, :, :]
        combination_features = layer_features[**2**, :, :, :]
        sl = style_loss_per_layer(style_reference_features, combination_features)
        loss += (style_weight / len(feature_layers)) * sl
    **return** loss
```

# å˜å¼‚æŸå¤±

æœ€åï¼ŒæŸå¤±çš„æœ€åä¸€éƒ¨åˆ†æ˜¯å˜å¼‚æŸå¤±ã€‚åŸå§‹è®ºæ–‡ä¸­æ²¡æœ‰åŒ…æ‹¬è¿™ä¸€è¦ç´ ï¼Œä¸¥æ ¼æ¥è¯´ï¼Œå®ƒå¯¹äºé¡¹ç›®çš„æˆåŠŸå¹¶ä¸æ˜¯å¿…è¦çš„ã€‚å°½ç®¡å¦‚æ­¤ï¼Œç»éªŒè¯æ˜ï¼Œæ·»åŠ è¯¥å…ƒç´ ä¼šäº§ç”Ÿæ›´å¥½çš„ç»“æœï¼Œå› ä¸ºå®ƒå¹³æ»‘äº†ç›¸é‚»åƒç´ ä¹‹é—´çš„é¢œè‰²å˜åŒ–ã€‚è®©æˆ‘ä»¬æŠŠè¿™ä¸ªåŒ…æ‹¬è¿›å»:

```
# The total variation loss mantains the generated image loclaly coherent,
# smoothing the pixel variations among neighbour pixels.
**def** **total_variation_loss**(x):
    a = K.square(x[:, :resized_width - **1**, :resized_height - **1**, :] - x[:, **1**:, :resized_height - **1**, :])
    b = K.square(x[:, :resized_width - **1**, :resized_height - **1**, :] - x[:, :resized_width - **1**, **1**:, :])
    **return** K.sum(K.pow(a + b, **1.25**))
```

# å…¨æŸ

æœ€åï¼Œå°†æ‰€æœ‰è¿™äº›å› ç´ è€ƒè™‘åœ¨å†…ï¼Œè®¡ç®—æ€»æŸå¤±ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦æå–æˆ‘ä»¬é€‰æ‹©çš„ç‰¹å®šå±‚çš„è¾“å‡ºã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå­—å…¸ä¸º<*å±‚åï¼Œå±‚è¾“å‡º* >:

```
# Get the outputs of each key layer, through unique names.
outputs_dict = dict([(layer.name, layer.output) **for** layer **in** model.layers])
```

ç„¶åï¼Œæˆ‘ä»¬é€šè¿‡è°ƒç”¨å…ˆå‰ç¼–ç çš„å‡½æ•°æ¥è®¡ç®—æŸå¤±ã€‚æ¯ä¸ªåˆ†é‡éƒ½ä¹˜ä»¥ç‰¹å®šçš„æƒé‡ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´æƒé‡ä»¥äº§ç”Ÿå¼ºçƒˆæˆ–è¾ƒè½»çš„æ•ˆæœ:

```
**def** **total_loss**():
    loss = K.variable(**0.**)

    # contribution of content_loss
    feature_layers_content = outputs_dict['block5_conv2']
    loss += content_weight * content_loss(feature_layers_content)

    # contribution of style_loss
    feature_layers_style = ['block1_conv1', 'block2_conv1',
                            'block3_conv1', 'block4_conv1',
                            'block5_conv1']
    loss += total_style_loss(feature_layers_style) * style_weight

    # contribution of variation_loss
    loss += total_variation_weight * total_variation_loss(combination_image)
    **return** loss
```

# è®¾ç½®ç¥ç»ç½‘ç»œ

VGG19 ç½‘ç»œå°†ä¸€æ‰¹ä¸‰ä¸ªå›¾åƒä½œä¸ºè¾“å…¥:è¾“å…¥å†…å®¹å›¾åƒã€æ ·å¼å‚è€ƒå›¾åƒå’ŒåŒ…å«ç”Ÿæˆå›¾åƒçš„ç¬¦å·å¼ é‡ã€‚å‰ä¸¤ä¸ªæ˜¯å¸¸é‡å˜é‡ï¼Œä½¿ç”¨ keras.backend åŒ…å®šä¹‰ä¸º*å˜é‡*ã€‚ç¬¬ä¸‰ä¸ªå˜é‡å®šä¹‰ä¸º*å ä½ç¬¦*ï¼Œå› ä¸ºå®ƒä¼šéšç€ä¼˜åŒ–å™¨æ›´æ–°ç»“æœçš„æ—¶é—´è€Œå˜åŒ–ã€‚

![](img/a73d5c3461c5f304037056115aaa683e.png)

ä¸€æ—¦å˜é‡è¢«åˆå§‹åŒ–ï¼Œæˆ‘ä»¬å°±æŠŠå®ƒä»¬åŠ å…¥ä¸€ä¸ªå¼ é‡ï¼Œè¿™ä¸ªå¼ é‡å°†åœ¨ä»¥åæä¾›ç»™ç½‘ç»œã€‚

```
# Get tensor representations of our images
base_image = K.variable(preprocess_image(base_image_path))
style_reference_image = K.variable(preprocess_image(style_reference_image_path))

# Placeholder for generated image
combination_image = K.placeholder((**1**, resized_width, resized_height, **3**))

# Combine the 3 images into a single Keras tensor
input_tensor = K.concatenate([base_image,
                              style_reference_image,
                              combination_image], axis=**0**)
```

å®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰æŸè€—ã€æ¢¯åº¦å’Œè¾“å‡ºã€‚åŸå§‹è®ºæ–‡ä½¿ç”¨ç®—æ³• L-BFGS ä½œä¸ºä¼˜åŒ–å™¨ã€‚è¿™ç§ç®—æ³•çš„ä¸€ä¸ªé™åˆ¶æ˜¯å®ƒè¦æ±‚æŸå¤±å’Œæ¢¯åº¦åˆ†åˆ«é€šè¿‡ã€‚å› ä¸ºå•ç‹¬è®¡ç®—å®ƒä»¬æ•ˆç‡æä½ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å®ç°ä¸€ä¸ªèµ‹å€¼å™¨ç±»ï¼Œå®ƒå¯ä»¥åŒæ—¶è®¡ç®—æŸå¤±å’Œæ¢¯åº¦å€¼ï¼Œä½†åˆ†åˆ«è¿”å›å®ƒä»¬ã€‚è®©æˆ‘ä»¬è¿™æ ·åš:

```
loss = total_loss()

# Get the gradients of the generated image
grads = K.gradients(loss, combination_image)
outputs = [loss]
outputs += grads

f_outputs = K.function([combination_image], outputs)

# Evaluate the loss and the gradients respect to the generated image. It is called in the Evaluator, necessary to
# compute the gradients and the loss as two different functions (limitation of the L-BFGS algorithm) without
# excessive losses in performance
**def** **eval_loss_and_grads**(x):
    x = x.reshape((**1**, resized_width, resized_height, **3**))
    outs = f_outputs([x])
    loss_value = outs[**0**]
    **if** len(outs[**1**:]) == **1**:
        grad_values = outs[**1**].flatten().astype('float64')
    **else**:
        grad_values = np.array(outs[**1**:]).flatten().astype('float64')
    **return** loss_value, grad_values

# Evaluator returns the loss and the gradient in two separate functions, but the calculation of the two variables
# are dependent. This reduces the computation time, since otherwise it would be calculated separately.
**class** **Evaluator**(object):

    **def** **__init__**(self):
        self.loss_value = **None**
        self.grads_values = **None**

    **def** **loss**(self, x):
        **assert** self.loss_value **is** **None**
        loss_value, grad_values = eval_loss_and_grads(x)
        self.loss_value = loss_value
        self.grad_values = grad_values
        **return** self.loss_value

    **def** **grads**(self, x):
        **assert** self.loss_value **is** **not** **None**
        grad_values = np.copy(self.grad_values)
        self.loss_value = **None**
        self.grad_values = **None**
        **return** grad_values

evaluator = Evaluator()
```

# æœ€åä¸€æ¡£

ç»ˆäºä¸‡äº‹ä¿±å¤‡äº†ï¼æœ€åä¸€æ­¥æ˜¯å¤šæ¬¡è¿­ä»£ä¼˜åŒ–å™¨ï¼Œç›´åˆ°æˆ‘ä»¬è¾¾åˆ°æœŸæœ›çš„æŸå¤±æˆ–æœŸæœ›çš„ç»“æœã€‚æˆ‘ä»¬å°†ä¿å­˜è¿­ä»£çš„ç»“æœï¼Œä»¥æ£€æŸ¥ç®—æ³•æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œã€‚å¦‚æœç»“æœä¸ä»¤äººæ»¡æ„ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒæ•´æƒé‡ä»¥æ”¹å–„ç”Ÿæˆçš„å›¾åƒã€‚

```
# The oprimizer is fmin_l_bfgs
**for** i **in** range(iterations):
    print('Iteration: ', i)
    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,
                                     x.flatten(),
                                     fprime=evaluator.grads,
                                     maxfun=**15**)

    print('Current loss value:', min_val)

    # Save current generated image
    img = deprocess_image(x.copy())
    fname = 'img/new' + np.str(i) + '.png'
    save(fname, img)
```

è¦æŸ¥çœ‹å®Œæ•´ä»£ç ï¼Œè¯·å‚è€ƒé¡µé¢å¼€å¤´æä¾›çš„ GitHub é“¾æ¥ã€‚

# æƒŠäººçš„ç»“æœ

![](img/543654a95d45d3361475770f5d3714a4.png)![](img/2610bc33e817664db42274ae74b32dbc.png)![](img/6a3bee3e5029daa618938b248828166d.png)

> å¦‚æœä½ æƒ³å°è¯•ç‰¹å®šçš„æ•ˆæœï¼Œç»˜ç”»ï¼Œæˆ–è€…ä½ æœ‰ä»»ä½•å»ºè®®ï¼Œè¯·ç•™ä¸‹è¯„è®ºï¼

*å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘å¸Œæœ›ä½ èƒ½ç‚¹å‡»é¼“æŒæŒ‰é’®*ğŸ‘å› æ­¤å…¶ä»–äººå¯èƒ½ä¼šå¶ç„¶å‘ç°å®ƒã€‚å¯¹äºä»»ä½•æ„è§æˆ–å»ºè®®ï¼Œä¸è¦çŠ¹è±«ç•™ä¸‹è¯„è®ºï¼

## æˆ‘æ˜¯ä¸€åæ•°æ®ç§‘å­¦ä¸“ä¸šçš„å­¦ç”Ÿï¼Œçƒ­çˆ±æœºå™¨å­¦ä¹ åŠå…¶æ— å°½çš„åº”ç”¨ã€‚ä½ å¯ä»¥åœ¨ maurocomi.com[æ‰¾åˆ°æ›´å¤šå…³äºæˆ‘å’Œæˆ‘çš„é¡¹ç›®çš„ä¿¡æ¯ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨](http://www.maurocomi.com) [Linkedin](https://www.linkedin.com/in/mauro-comi/) ä¸Šæ‰¾åˆ°æˆ‘ï¼Œæˆ–è€…ç›´æ¥ç»™æˆ‘å‘é‚®ä»¶ã€‚æˆ‘æ€»æ˜¯ä¹äºèŠå¤©ï¼Œæˆ–è€…åˆä½œæ–°çš„ä»¤äººæƒŠå¥‡çš„é¡¹ç›®ã€‚