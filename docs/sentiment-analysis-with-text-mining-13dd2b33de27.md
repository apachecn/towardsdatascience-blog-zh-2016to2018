# åŸºäºæ–‡æœ¬æŒ–æ˜çš„æƒ…æ„Ÿåˆ†æ

> åŸæ–‡ï¼š<https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27?source=collection_archive---------2----------------------->

## äº†è§£å¦‚ä½•å‡†å¤‡æ–‡æœ¬æ•°æ®å¹¶è¿è¡Œä¸¤ä¸ªä¸åŒçš„åˆ†ç±»å™¨æ¥é¢„æµ‹æ¨æ–‡çš„æƒ…ç»ªã€‚

![](img/61b08fae380fe5c8bbf93e09c55907fd.png)

Photo by [Romain Vignes](https://unsplash.com/@rvignes?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†æ¢ç´¢ä¸€äº›ç”¨äºæƒ…æ„Ÿåˆ†æçš„æ–‡æœ¬æŒ–æ˜æŠ€æœ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†èŠ±ä¸€äº›æ—¶é—´å‡†å¤‡æ–‡æœ¬æ•°æ®ã€‚è¿™å°†æ¶‰åŠæ¸…ç†æ–‡æœ¬æ•°æ®ï¼Œåˆ é™¤åœç”¨è¯å’Œè¯å¹²ã€‚ä¸ºæ­¤ï¼ŒKaggle ä¸Šçš„ [Twitter ç¾å›½èˆªç©ºå…¬å¸æƒ…ç»ªæ•°æ®é›†éå¸¸é€‚åˆåˆä½œã€‚å®ƒåŒ…å« tweet çš„æ–‡æœ¬å’Œä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªå¯èƒ½æƒ…æ„Ÿå€¼çš„å˜é‡ã€‚](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)

ä¸ºäº†æ¨æ–­æ¨æ–‡çš„æƒ…æ„Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåˆ†ç±»å™¨:é€»è¾‘å›å½’å’Œå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç½‘æ ¼æœç´¢æ¥è°ƒæ•´ä¸¤ä¸ªåˆ†ç±»å™¨çš„è¶…å‚æ•°ã€‚

æˆ‘ä»¬å°†ç”¨ä¸‰ä¸ªæŒ‡æ ‡æ¥æ¯”è¾ƒæ€§èƒ½:ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚

æˆ‘ä»¬ä»å¯¼å…¥åŒ…å’Œé…ç½®ä¸€äº›è®¾ç½®å¼€å§‹ã€‚

```
import numpy as np 
import pandas as pd 
pd.set_option('display.max_colwidth', -1)
from time import time
import re
import string
import os
import emoji
from pprint import pprint
import collectionsimport matplotlib.pyplot as plt
import seaborn as sns
sns.set(style="darkgrid")
sns.set(font_scale=1.3)from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.metrics import classification_reportfrom sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.externals import joblibimport gensimfrom nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenizeimport warnings
warnings.filterwarnings('ignore')np.random.seed(37)
```

# åŠ è½½æ•°æ®

æˆ‘ä»¬è¯»å–äº†ä» Kaggle æ•°æ®é›†ä¸‹è½½çš„é€—å·åˆ†éš”æ–‡ä»¶ã€‚æˆ‘ä»¬æ‰“ä¹±äº†æ•°æ®å¸§ï¼Œä»¥é˜²ç±»è¢«æ’åºã€‚å¯¹åŸå§‹æŒ‡æ•°çš„`permutation`åº”ç”¨`reindex`æ–¹æ³•æœ‰åˆ©äºæ­¤ã€‚åœ¨è¿™æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`text`å˜é‡å’Œ`airline_sentiment`å˜é‡ã€‚

```
df = pd.read_csv('../input/Tweets.csv')
df = df.reindex(np.random.permutation(df.index))
df = df[['text', 'airline_sentiment']]
```

# æ¢ç´¢æ€§æ•°æ®åˆ†æ

## ç›®æ ‡å˜é‡

æˆ‘ä»¬å°†é¢„æµ‹ä¸‰ä¸ªç±»åˆ«æ ‡ç­¾:è´Ÿé¢ã€ä¸­æ€§æˆ–æ­£é¢ã€‚

æ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹é¢çš„å›¾è¡¨ä¸­çœ‹åˆ°çš„ï¼Œåˆ†ç±»æ ‡ç­¾æ˜¯ä¸å¹³è¡¡çš„ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µåº”è¯¥è®°ä½çš„äº‹æƒ…ã€‚æœ‰äº† seaborn åŒ…çš„`factorplot`ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–ç›®æ ‡å˜é‡çš„åˆ†å¸ƒã€‚

```
sns.factorplot(x="airline_sentiment", data=df, kind="count", size=6, aspect=1.5, palette="PuBuGn_d")
plt.show();
```

![](img/d091427d229243d724e1f4ac320ecc12.png)

Imbalanced distribution of the target class labels

## è¾“å…¥å˜é‡

ä¸ºäº†åˆ†æ`text` å˜é‡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç±»`TextCounts`ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æ–‡æœ¬å˜é‡çš„ä¸€äº›åŸºæœ¬ç»Ÿè®¡æ•°æ®ã€‚

*   `count_words`:æ¨æ–‡å­—æ•°
*   `count_mentions`:å…¶ä»– Twitter è´¦æˆ·çš„æ¨èä»¥@å¼€å¤´
*   `count_hashtags`:æ ‡ç­¾å­—æ•°ï¼Œä»¥#å¼€å¤´
*   `count_capital_words`:ä¸€äº›å¤§å†™å•è¯æœ‰æ—¶è¢«ç”¨æ¥â€œå«å–Šâ€å’Œè¡¨è¾¾(è´Ÿé¢)æƒ…ç»ª
*   `count_excl_quest_marks`:é—®å·æˆ–æ„Ÿå¹å·çš„ä¸ªæ•°
*   `count_urls`:æ¨æ–‡ä¸­çš„é“¾æ¥æ•°é‡ï¼Œä»¥ http(s)å¼€å¤´
*   `count_emojis`:è¡¨æƒ…ç¬¦å·çš„æ•°é‡ï¼Œè¿™å¯èƒ½æ˜¯æƒ…ç»ªçš„ä¸€ä¸ªå¥½è¿¹è±¡

```
class TextCounts(BaseEstimator, TransformerMixin):

    def count_regex(self, pattern, tweet):
        return len(re.findall(pattern, tweet))

    def fit(self, X, y=None, **fit_params):
        # fit method is used when specific operations need to be done on the train data, but not on the test data
        return self

    def transform(self, X, **transform_params):
        count_words = X.apply(lambda x: self.count_regex(r'\w+', x)) 
        count_mentions = X.apply(lambda x: self.count_regex(r'@\w+', x))
        count_hashtags = X.apply(lambda x: self.count_regex(r'#\w+', x))
        count_capital_words = X.apply(lambda x: self.count_regex(r'\b[A-Z]{2,}\b', x))
        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\?', x))
        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\s]+[\s]?', x))
        # We will replace the emoji symbols with a description, which makes using a regex for counting easier
        # Moreover, it will result in having more words in the tweet
        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))

        df = pd.DataFrame({'count_words': count_words
                           , 'count_mentions': count_mentions
                           , 'count_hashtags': count_hashtags
                           , 'count_capital_words': count_capital_words
                           , 'count_excl_quest_marks': count_excl_quest_marks
                           , 'count_urls': count_urls
                           , 'count_emojis': count_emojis
                          })

        return df
tc = TextCounts()
df_eda = tc.fit_transform(df.text)
df_eda['airline_sentiment'] = df.airline_sentiment
```

æŸ¥çœ‹ TextStats å˜é‡ä¸ class å˜é‡çš„å…³ç³»å¯èƒ½ä¼šå¾ˆæœ‰è¶£ã€‚å› æ­¤æˆ‘ä»¬ç¼–å†™äº†ä¸€ä¸ªå‡½æ•°`show_dist`,å®ƒä¸ºæ¯ä¸ªç›®æ ‡ç±»æä¾›äº†æè¿°æ€§çš„ç»Ÿè®¡æ•°æ®å’Œå›¾è¡¨ã€‚

```
def show_dist(df, col):
    print('Descriptive stats for {}'.format(col))
    print('-'*(len(col)+22))
    print(df.groupby('airline_sentiment')[col].describe())
    bins = np.arange(df[col].min(), df[col].max() + 1)
    g = sns.FacetGrid(df, col='airline_sentiment', size=5, hue='airline_sentiment', palette="PuBuGn_d")
    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)
    plt.show()
```

ä¸‹é¢ä½ å¯ä»¥æ‰¾åˆ°æ¯ä¸ªç›®æ ‡ç±»åˆ«çš„ tweet å­—æ•°åˆ†å¸ƒã€‚ä¸ºç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å°†ä»…é™äºè¿™ä¸ªå˜é‡ã€‚æ‰€æœ‰ TextCounts å˜é‡çš„å›¾è¡¨éƒ½åœ¨ Github çš„[ç¬”è®°æœ¬é‡Œã€‚](https://github.com/bertcarremans/TwitterUSAirlineSentiment)

![](img/4da06de8a1834b9ca420b45840cc0c6c.png)

*   æ¨æ–‡ä¸­ä½¿ç”¨çš„å­—æ•°ç›¸å½“ä½ã€‚æœ€å¤§çš„å­—æ•°æ˜¯ 36 ä¸ªï¼Œç”šè‡³æœ‰åªæœ‰ 2 ä¸ªå­—çš„æ¨æ–‡ã€‚æ‰€ä»¥åœ¨æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒï¼Œä¸è¦åˆ é™¤å¤ªå¤šçš„å•è¯ã€‚ä½†æ˜¯æ–‡å­—å¤„ç†ä¼šæ›´å¿«ã€‚è´Ÿé¢æ¨æ–‡æ¯”ä¸­æ€§æˆ–æ­£é¢æ¨æ–‡åŒ…å«æ›´å¤šå•è¯ã€‚
*   æ‰€æœ‰æ¨æ–‡è‡³å°‘æœ‰ä¸€æ¬¡æåŠã€‚è¿™æ˜¯åŸºäº Twitter æ•°æ®ä¸­çš„æåŠæå–æ¨æ–‡çš„ç»“æœã€‚å°±æƒ…æ„Ÿè€Œè¨€ï¼ŒæåŠæ¬¡æ•°ä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆä¸åŒã€‚
*   å¤§å¤šæ•°æ¨æ–‡ä¸åŒ…å«å“ˆå¸Œæ ‡ç­¾ã€‚æ‰€ä»¥è¿™ä¸ªå˜é‡åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´ä¸ä¼šè¢«ä¿ç•™ã€‚åŒæ ·ï¼Œå¯¹äºæƒ…æ„Ÿï¼Œæ•£åˆ—æ ‡ç­¾çš„æ•°é‡æ²¡æœ‰å·®åˆ«ã€‚
*   å¤§å¤šæ•°æ¨æ–‡ä¸åŒ…å«å¤§å†™å•è¯ï¼Œæˆ‘ä»¬çœ‹ä¸åˆ°æƒ…ç»ªåˆ†å¸ƒçš„å·®å¼‚ã€‚
*   ç§¯æçš„æ¨æ–‡ä¼¼ä¹ä½¿ç”¨äº†æ›´å¤šçš„æ„Ÿå¹æˆ–é—®å·ã€‚
*   å¤§å¤šæ•°æ¨æ–‡ä¸åŒ…å«ç½‘å€ã€‚
*   å¤§å¤šæ•°æ¨æ–‡ä¸ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€‚

# æ–‡æœ¬æ¸…ç†

åœ¨æˆ‘ä»¬å¼€å§‹ä½¿ç”¨ tweets çš„æ–‡æœ¬ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ¸…ç†å®ƒã€‚æˆ‘ä»¬å°†åœ¨`CleanText` **è¯¾ä¸Šåšè¿™ä»¶äº‹ã€‚**åœ¨è¿™ä¸ªè¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œ:

*   åˆ é™¤æåŠï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿæƒ³æ¨å¹¿åˆ°å…¶ä»–èˆªç©ºå…¬å¸çš„æ¨æ–‡ã€‚
*   åˆ é™¤æ•£åˆ—æ ‡ç­¾ç¬¦å·(#)ï¼Œä½†ä¸è¦åˆ é™¤å®é™…çš„æ ‡ç­¾ï¼Œå› ä¸ºè¿™å¯èƒ½åŒ…å«ä¿¡æ¯
*   å°†æ‰€æœ‰å•è¯è®¾ä¸ºå°å†™
*   åˆ é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒåŒ…æ‹¬é—®å·å’Œæ„Ÿå¹å·
*   åˆ é™¤ç½‘å€ï¼Œå› ä¸ºå®ƒä»¬ä¸åŒ…å«æœ‰ç”¨çš„ä¿¡æ¯ã€‚æˆ‘ä»¬æ²¡æœ‰æ³¨æ„åˆ°æƒ…æ„Ÿç±»åˆ«ä¹‹é—´ä½¿ç”¨çš„ URL æ•°é‡çš„å·®å¼‚
*   ç¡®ä¿å°†è¡¨æƒ…ç¬¦å·è½¬æ¢æˆä¸€ä¸ªå•è¯ã€‚
*   åˆ é™¤æ•°å­—
*   åˆ é™¤åœç”¨è¯
*   åº”ç”¨`PorterStemmer`ä¿ç•™å•è¯çš„è¯å¹²

```
class CleanText(BaseEstimator, TransformerMixin):
    def remove_mentions(self, input_text):
        return re.sub(r'@\w+', '', input_text)

    def remove_urls(self, input_text):
        return re.sub(r'http.?://[^\s]+[\s]?', '', input_text)

    def emoji_oneword(self, input_text):
        # By compressing the underscore, the emoji is kept as one word
        return input_text.replace('_','')

    def remove_punctuation(self, input_text):
        # Make translation table
        punct = string.punctuation
        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space
        return input_text.translate(trantab) def remove_digits(self, input_text):
        return re.sub('\d+', '', input_text)

    def to_lower(self, input_text):
        return input_text.lower()

    def remove_stopwords(self, input_text):
        stopwords_list = stopwords.words('english')
        # Some words which might indicate a certain sentiment are kept via a whitelist
        whitelist = ["n't", "not", "no"]
        words = input_text.split() 
        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] 
        return " ".join(clean_words) 

    def stemming(self, input_text):
        porter = PorterStemmer()
        words = input_text.split() 
        stemmed_words = [porter.stem(word) for word in words]
        return " ".join(stemmed_words)

    def fit(self, X, y=None, **fit_params):
        return self

    def transform(self, X, **transform_params):
        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)
        return clean_X
```

ä¸ºäº†å±•ç¤ºæ¸…ç†åçš„æ–‡æœ¬å˜é‡çš„å¤–è§‚ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç¤ºä¾‹ã€‚

```
ct = CleanText()
sr_clean = ct.fit_transform(df.text)
sr_clean.sample(5)
```

> é«˜å…´ rt æ‰“èµŒé¸Ÿæ„¿é£å—æ–¹å†¬å¤©
> ç‚¹ upc ä»£ç æ£€æŸ¥ baggag å‘Šè¯‰ luggag vacat day tri æ³³è£…
> vx jfk la dirti é£æœºä¸æ ‡å‡†
> å‘Šè¯‰æ„å‘³ç€å·¥ä½œéœ€è¦ä¼°è®¡æ—¶é—´åˆ°è¾¾è¯·éœ€è¦ç¬”è®°æœ¬ç”µè„‘å·¥ä½œæ„Ÿè°¢
> å½“ç„¶ä¸šåŠ¡å» els èˆªç©ºæ—…è¡Œå§“åå‡¯ç‘Ÿç³ç´¢ç‰¹ç½—

æ–‡æœ¬æ¸…ç†çš„ä¸€ä¸ªå‰¯ä½œç”¨æ˜¯ä¸€äº›è¡Œçš„æ–‡æœ¬ä¸­æ²¡æœ‰ä»»ä½•å•è¯ã€‚å¯¹äº`CountVectorizer`å’Œ`TfIdfVectorizer`æ¥è¯´ï¼Œè¿™ä¸æˆé—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äº`Word2Vec`ç®—æ³•æ¥è¯´ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªé”™è¯¯ã€‚æœ‰ä¸åŒçš„ç­–ç•¥æ¥å¤„ç†è¿™äº›ç¼ºå¤±çš„ä»·å€¼è§‚ã€‚

*   åˆ é™¤æ•´è¡Œï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿™æ˜¯ä¸å¯å–çš„ã€‚
*   ç”¨ç±»ä¼¼*[no_text]*çš„å ä½ç¬¦æ–‡æœ¬ä¼°ç®—ç¼ºå¤±å€¼
*   å½“åº”ç”¨ Word2Vec æ—¶:ä½¿ç”¨æ‰€æœ‰å‘é‡çš„å¹³å‡å€¼

è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨å ä½ç¬¦æ–‡æœ¬è¿›è¡Œä¼°ç®—ã€‚

```
empty_clean = sr_clean == ''
print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))
sr_clean.loc[empty_clean] = '[no_text]'
```

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æ¸…ç†äº†æ¨æ–‡çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹æœ€å¸¸ç”¨çš„è¯æ˜¯ä»€ä¹ˆã€‚ä¸‹é¢æˆ‘ä»¬å°†å±•ç¤ºå‰ 20 ä¸ªå•è¯ã€‚å‡ºç°é¢‘ç‡æœ€é«˜çš„è¯æ˜¯â€œé€ƒâ€ã€‚

```
cv = CountVectorizer()
bow = cv.fit_transform(sr_clean)
word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))
word_counter = collections.Counter(word_freq)
word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])fig, ax = plt.subplots(figsize=(12, 10))
sns.barplot(x="word", y="freq", data=word_counter_df, palette="PuBuGn_d", ax=ax)
plt.show();
```

![](img/bc20216c86f3bcc941c7cd785f7e52b3.png)

# åˆ›å»ºæµ‹è¯•æ•°æ®

ä¸ºäº†æ£€æŸ¥æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬éœ€è¦ä¸€å¥—æµ‹è¯•è®¾å¤‡ã€‚å¯¹è®­ç»ƒæ•°æ®çš„è¯„ä¼°æ˜¯ä¸æ­£ç¡®çš„ã€‚æ‚¨ä¸åº”è¯¥åœ¨ç”¨äºè®­ç»ƒæ¨¡å‹çš„ç›¸åŒæ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å°†`TextCounts`å˜é‡ä¸`CleanText`å˜é‡ç»“åˆèµ·æ¥ã€‚æœ€åˆï¼Œæˆ‘åœ¨`GridSearchCV`ä¸­é”™è¯¯åœ°æ‰§è¡Œäº† TextCounts å’Œ CleanTextã€‚è¿™èŠ±è´¹äº†å¤ªé•¿æ—¶é—´ï¼Œå› ä¸ºæ¯æ¬¡è¿è¡Œ GridSearch éƒ½è¦åº”ç”¨è¿™äº›å‡½æ•°ã€‚åªè¿è¡Œä¸€æ¬¡å°±è¶³å¤Ÿäº†ã€‚

```
df_model = df_eda
df_model['clean_text'] = sr_clean
df_model.columns.tolist()
```

æ‰€ä»¥`df_model`ç°åœ¨åŒ…å«äº†å‡ ä¸ªå˜é‡ã€‚ä½†æ˜¯æˆ‘ä»¬çš„çŸ¢é‡å™¨(è§ä¸‹æ–‡)å°†åªéœ€è¦`clean_text`å˜é‡ã€‚å¯ä»¥æ·»åŠ `TextCounts`å˜é‡ã€‚ä¸ºäº†é€‰æ‹©åˆ—ï¼Œæˆ‘ç¼–å†™äº†ä¸‹é¢çš„ç±»`ColumnExtractor`ã€‚

```
class ColumnExtractor(TransformerMixin, BaseEstimator):
    def __init__(self, cols):
        self.cols = cols def transform(self, X, **transform_params):
        return X[self.cols] def fit(self, X, y=None, **fit_params):
        return selfX_train, X_test, y_train, y_test = train_test_split(df_model.drop('airline_sentiment', axis=1), df_model.airline_sentiment, test_size=0.1, random_state=37)
```

# è¶…å‚æ•°è°ƒæ•´å’Œäº¤å‰éªŒè¯

æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹åˆ°çš„ï¼ŒçŸ¢é‡å™¨å’Œåˆ†ç±»å™¨éƒ½æœ‰å¯é…ç½®çš„å‚æ•°ã€‚ä¸ºäº†é€‰æ‹©æœ€ä½³å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å•ç‹¬çš„éªŒè¯é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚åŸ¹è®­æœŸé—´æ²¡æœ‰ä½¿ç”¨è¯¥éªŒè¯é›†ã€‚ç„¶è€Œï¼Œä»…ä½¿ç”¨ä¸€ä¸ªéªŒè¯é›†å¯èƒ½ä¸ä¼šäº§ç”Ÿå¯é çš„éªŒè¯ç»“æœã€‚ç”±äºå¶ç„¶çš„æœºä¼šï¼Œæ‚¨å¯èƒ½åœ¨éªŒè¯é›†ä¸Šæœ‰ä¸€ä¸ªå¥½çš„æ¨¡å‹æ€§èƒ½ã€‚å¦‚æœæ‚¨ä»¥å…¶ä»–æ–¹å¼åˆ†å‰²æ•°æ®ï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°å…¶ä»–ç»“æœã€‚ä¸ºäº†å¾—åˆ°æ›´å‡†ç¡®çš„ä¼°è®¡ï¼Œæˆ‘ä»¬è¿›è¡Œäº¤å‰éªŒè¯ã€‚

é€šè¿‡äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬å¯ä»¥å¤šæ¬¡å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚ç„¶ååœ¨ä¸åŒçš„æŠ˜å ä¸Šå¯¹è¯„ä¼°åº¦é‡è¿›è¡Œå¹³å‡ã€‚å¹¸è¿çš„æ˜¯ï¼ŒGridSearchCV åº”ç”¨äº†ç°æˆçš„äº¤å‰éªŒè¯ã€‚

ä¸ºäº†æ‰¾åˆ°çŸ¢é‡å™¨å’Œåˆ†ç±»å™¨çš„æœ€ä½³å‚æ•°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª`Pipeline`ã€‚

# è¯„ä¼°æŒ‡æ ‡

é»˜è®¤æƒ…å†µä¸‹ï¼ŒGridSearchCV ä½¿ç”¨é»˜è®¤è®¡åˆ†å™¨æ¥è®¡ç®—`best_score_`ã€‚å¯¹äº`MultiNomialNb`å’Œ`LogisticRegression`æ¥è¯´ï¼Œè¿™ä¸ªé»˜è®¤çš„è¯„åˆ†æ ‡å‡†æ˜¯å‡†ç¡®æ€§ã€‚

åœ¨æˆ‘ä»¬çš„å‡½æ•°`grid_vect`ä¸­ï¼Œæˆ‘ä»¬é¢å¤–ç”Ÿæˆäº†æµ‹è¯•æ•°æ®çš„`classification_report`ã€‚è¿™ä¸ºæ¯ä¸ªç›®æ ‡ç±»æä¾›äº†ä¸€äº›æœ‰è¶£çš„åº¦é‡ã€‚è¿™åœ¨è¿™é‡Œå¯èƒ½æ›´åˆé€‚ã€‚è¿™äº›æŒ‡æ ‡æ˜¯ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°**ã€‚**

*   Precision **:** åœ¨æˆ‘ä»¬é¢„æµ‹ä¸ºæŸä¸ªç±»çš„æ‰€æœ‰è¡Œä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®é¢„æµ‹äº†å¤šå°‘ï¼Ÿ
*   å›æƒ³ä¸€ä¸‹ **:** åœ¨æŸä¸ªç±»çš„æ‰€æœ‰è¡Œä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®é¢„æµ‹äº†å¤šå°‘è¡Œï¼Ÿ
*   F1 å¾—åˆ† **:** ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ã€‚

åˆ©ç”¨[æ··æ·†çŸ©é˜µ](https://en.wikipedia.org/wiki/Confusion_matrix)çš„å…ƒç´ ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚

```
# Based on [http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html)
def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):

    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'
                      ,'count_mentions','count_urls','count_words']

    if is_w2v:
        w2vcols = []
        for i in range(SIZE):
            w2vcols.append(i)
        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))
                                 , ('w2v', ColumnExtractor(cols=w2vcols))]
                                , n_jobs=-1)
    else:
        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))
                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]
                                , n_jobs=-1) pipeline = Pipeline([
        ('features', features)
        , ('clf', clf)
    ])

    # Join the parameters dictionaries together
    parameters = dict()
    if parameters_text:
        parameters.update(parameters_text)
    parameters.update(parameters_clf) # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics
    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)

    print("Performing grid search...")
    print("pipeline:", [name for name, _ in pipeline.steps])
    print("parameters:")
    pprint(parameters) t0 = time()
    grid_search.fit(X_train, y_train)
    print("done in %0.3fs" % (time() - t0))
    print() print("Best CV score: %0.3f" % grid_search.best_score_)
    print("Best parameters set:")
    best_parameters = grid_search.best_estimator_.get_params()
    for param_name in sorted(parameters.keys()):
        print("\t%s: %r" % (param_name, best_parameters[param_name]))

    print("Test score with best_estimator_: %0.3f" % grid_search.best_estimator_.score(X_test, y_test))
    print("\n")
    print("Classification Report Test Data")
    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))

    return grid_search
```

# GridSearchCV çš„å‚æ•°ç½‘æ ¼

åœ¨ç½‘æ ¼æœç´¢ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚ç”¨äºæµ‹è¯•æ€§èƒ½çš„ä¸€ç»„å‚æ•°å¦‚ä¸‹æ‰€ç¤ºã€‚

```
# Parameter grid settings for the vectorizers (Count and TFIDF)
parameters_vect = {
    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),
    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),
    'features__pipe__vect__min_df': (1,2)
} # Parameter grid settings for MultinomialNB
parameters_mnb = {
    'clf__alpha': (0.25, 0.5, 0.75)
} # Parameter grid settings for LogisticRegression
parameters_logreg = {
    'clf__C': (0.25, 0.5, 1.0),
    'clf__penalty': ('l1', 'l2')
}
```

# åˆ†ç±»å™¨

è¿™é‡Œæˆ‘ä»¬å°†æ¯”è¾ƒä¸€ä¸‹`MultinomialNB`å’Œ`LogisticRegression`çš„æ€§èƒ½ã€‚

```
mnb = MultinomialNB()
logreg = LogisticRegression()
```

# è®¡æ•°çŸ¢é‡å™¨

ä¸ºäº†åœ¨åˆ†ç±»å™¨ä¸­ä½¿ç”¨å•è¯ï¼Œæˆ‘ä»¬éœ€è¦å°†å•è¯è½¬æ¢æˆæ•°å­—ã€‚Sklearn çš„`CountVectorizer`è·å–æ‰€æœ‰æ¨æ–‡ä¸­çš„æ‰€æœ‰å•è¯ï¼Œåˆ†é…ä¸€ä¸ª IDï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªæ¨æ–‡ä¸­è¯¥å•è¯çš„å‡ºç°é¢‘ç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå•è¯åŒ…ä½œä¸ºåˆ†ç±»å™¨çš„è¾“å…¥ã€‚è¿™ä¸€è¢‹å•è¯æ˜¯ä¸€ä¸ªç¨€ç–çš„æ•°æ®é›†ã€‚è¿™æ„å‘³ç€æ¯æ¡è®°å½•éƒ½å°†æœ‰è®¸å¤šé›¶ï¼Œä»£è¡¨æ²¡æœ‰åœ¨ tweet ä¸­å‡ºç°çš„å•è¯ã€‚

```
countvect = CountVectorizer()# MultinomialNB
best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)
joblib.dump(best_mnb_countvect, '../output/best_mnb_countvect.pkl')# LogisticRegression
best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)
joblib.dump(best_logreg_countvect, '../output/best_logreg_countvect.pkl')
```

# TF-IDF çŸ¢é‡å™¨

CountVectorizer çš„ä¸€ä¸ªé—®é¢˜æ˜¯å¯èƒ½ä¼šæœ‰é¢‘ç¹å‡ºç°çš„å•è¯ã€‚è¿™äº›è¯å¯èƒ½æ²¡æœ‰æ­§è§†æ€§ä¿¡æ¯ã€‚å› æ­¤å®ƒä»¬å¯ä»¥è¢«ç§»é™¤ã€‚ [TF-IDF(è¯é¢‘â€”é€†æ–‡æ¡£é¢‘ç‡)](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)å¯ä»¥ç”¨æ¥å¯¹è¿™äº›é¢‘ç¹å‡ºç°çš„è¯è¿›è¡Œé™æƒã€‚

```
tfidfvect = TfidfVectorizer()# MultinomialNB
best_mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)
joblib.dump(best_mnb_tfidf, '../output/best_mnb_tfidf.pkl')# LogisticRegression
best_logreg_tfidf = grid_vect(logreg, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)
joblib.dump(best_logreg_tfidf, '../output/best_logreg_tfidf.pkl')
```

# Word2Vec

å°†å•è¯è½¬æ¢æˆæ•°å€¼çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`Word2Vec`ã€‚Word2Vec å°†æ¯ä¸ªå•è¯æ˜ å°„åˆ°å¤šç»´ç©ºé—´ä¸­ã€‚å®ƒé€šè¿‡è€ƒè™‘ä¸€ä¸ªè¯åœ¨æ¨æ–‡ä¸­å‡ºç°çš„ä¸Šä¸‹æ–‡æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ç»“æœï¼Œç›¸ä¼¼çš„å•è¯åœ¨å¤šç»´ç©ºé—´ä¸­ä¹Ÿå½¼æ­¤æ¥è¿‘ã€‚

Word2Vec ç®—æ³•æ˜¯ gensim åŒ…çš„ä¸€éƒ¨åˆ†ã€‚

Word2Vec ç®—æ³•ä½¿ç”¨å•è¯åˆ—è¡¨ä½œä¸ºè¾“å…¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†`nltk`åŒ…çš„`word_tokenize`æ–¹æ³•ã€‚

```
SIZE = 50X_train['clean_text_wordlist'] = X_train.clean_text.apply(lambda x : word_tokenize(x))
X_test['clean_text_wordlist'] = X_test.clean_text.apply(lambda x : word_tokenize(x))model = gensim.models.Word2Vec(X_train.clean_text_wordlist
, min_count=1
, size=SIZE
, window=5
, workers=4)model.most_similar('plane', topn=3)
```

Word2Vec æ¨¡å‹æä¾›äº†æ‰€æœ‰ tweets ä¸­çš„è¯æ±‡ã€‚å¯¹äºæ¯ä¸ªå•è¯ï¼Œä½ ä¹Ÿæœ‰å®ƒçš„å‘é‡å€¼ã€‚å‘é‡å€¼çš„æ•°é‡ç­‰äºæ‰€é€‰çš„å¤§å°ã€‚è¿™äº›æ˜¯æ¯ä¸ªå•è¯åœ¨å¤šç»´ç©ºé—´ä¸­æ˜ å°„çš„ç»´åº¦ã€‚å‡ºç°æ¬¡æ•°å°‘äº`min_count`çš„å•è¯ä¸ä¼šä¿ç•™åœ¨è¯æ±‡è¡¨ä¸­ã€‚

min_count å‚æ•°çš„ä¸€ä¸ªå‰¯ä½œç”¨æ˜¯ä¸€äº› tweets å¯èƒ½æ²¡æœ‰å‘é‡å€¼ã€‚å½“ tweet ä¸­çš„å•è¯åœ¨å°‘äº min_count çš„ tweet ä¸­å‡ºç°æ—¶ï¼Œå°±ä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚ç”±äº tweets çš„è¯­æ–™åº“å¾ˆå°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æœ‰å‘ç”Ÿè¿™ç§æƒ…å†µçš„é£é™©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°† min_count å€¼è®¾ç½®ä¸º 1ã€‚

æ¨æ–‡å¯ä»¥æœ‰ä¸åŒæ•°é‡çš„å‘é‡ï¼Œè¿™å–å†³äºå®ƒåŒ…å«çš„å­—æ•°ã€‚ä¸ºäº†ä½¿ç”¨è¿™ä¸ªè¾“å‡ºè¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬å°†è®¡ç®—æ¯æ¡ tweet çš„æ‰€æœ‰å‘é‡çš„å¹³å‡å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ç›¸åŒæ•°é‡(å³å¤§å°)çš„è¾“å…¥å˜é‡ã€‚

æˆ‘ä»¬ç”¨å‡½æ•°`compute_avg_w2v_vector`æ¥åšè¿™ä»¶äº‹ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬è¿˜æ£€æŸ¥ tweet ä¸­çš„å•è¯æ˜¯å¦å‡ºç°åœ¨ Word2Vec æ¨¡å‹çš„è¯æ±‡è¡¨ä¸­ã€‚å¦‚æœä¸æ˜¯ï¼Œåˆ™è¿”å›ä¸€ä¸ªç”¨ 0.0 å¡«å……çš„åˆ—è¡¨ã€‚å¦åˆ™æ˜¯å•è¯å‘é‡çš„å¹³å‡å€¼ã€‚

```
def compute_avg_w2v_vector(w2v_dict, tweet):
    list_of_word_vectors = [w2v_dict[w] for w in tweet if w in w2v_dict.vocab.keys()]

    if len(list_of_word_vectors) == 0:
        result = [0.0]*SIZE
    else:
        result = np.sum(list_of_word_vectors, axis=0) / len(list_of_word_vectors)

    return resultX_train_w2v = X_train['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))
X_test_w2v = X_test['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))
```

è¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ªå‘é‡ç»´æ•°ç­‰äº`SIZE`çš„åºåˆ—ã€‚ç°åœ¨æˆ‘ä»¬å°†åˆ†å‰²è¿™ä¸ªå‘é‡å¹¶åˆ›å»ºä¸€ä¸ªæ•°æ®å¸§ï¼Œæ¯ä¸ªå‘é‡å€¼åœ¨å•ç‹¬çš„åˆ—ä¸­ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°† Word2Vec å˜é‡è¿æ¥åˆ°å…¶ä»– TextCounts å˜é‡ã€‚æˆ‘ä»¬éœ€è¦é‡ç”¨`X_train`å’Œ`X_test`çš„ç´¢å¼•ã€‚å¦åˆ™ï¼Œè¿™å°†åœ¨ä»¥åçš„è¿æ¥ä¸­äº§ç”Ÿé—®é¢˜(é‡å¤)ã€‚

```
X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index= X_train.index)
X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index= X_test.index)# Concatenate with the TextCounts variables
X_train_w2v = pd.concat([X_train_w2v, X_train.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)
X_test_w2v = pd.concat([X_test_w2v, X_test.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)
```

æˆ‘ä»¬åªè€ƒè™‘é€»è¾‘å›å½’ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ Word2Vec å‘é‡ä¸­æœ‰è´Ÿå€¼ã€‚å¤šé¡¹å¼ lNB å‡è®¾å˜é‡å…·æœ‰[å¤šé¡¹å¼åˆ†å¸ƒ](https://en.wikipedia.org/wiki/Multinomial_distribution)ã€‚å› æ­¤å®ƒä»¬ä¸èƒ½åŒ…å«è´Ÿå€¼ã€‚

```
best_logreg_w2v = grid_vect(logreg, parameters_logreg, X_train_w2v, X_test_w2v, is_w2v=True)
joblib.dump(best_logreg_w2v, '../output/best_logreg_w2v.pkl')
```

# ç»“è®º

*   å½“ä½¿ç”¨è®¡æ•°çŸ¢é‡å™¨çš„ç‰¹æ€§æ—¶ï¼Œè¿™ä¸¤ç§åˆ†ç±»å™¨éƒ½èƒ½è·å¾—æœ€ä½³ç»“æœ
*   é€»è¾‘å›å½’ä¼˜äºå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨
*   æµ‹è¯•é›†ä¸Šçš„æœ€ä½³æ€§èƒ½æ¥è‡ªå¸¦æœ‰ CountVectorizer ç‰¹æ€§çš„ LogisticRegressionã€‚

æœ€ä½³å‚æ•°:

*   c å€¼ä¸º 1
*   L2 æ­£åˆ™åŒ–
*   max_df: 0.5 æˆ–æœ€å¤§æ–‡æ¡£é¢‘ç‡ 50%ã€‚
*   min_df: 1 æˆ–è€…è¿™äº›è¯éœ€è¦å‡ºç°åœ¨è‡³å°‘ä¸¤æ¡æ¨æ–‡ä¸­
*   ngram_range: (1ï¼Œ2)ï¼Œä¸¤ä¸ªå•è¯éƒ½ä½œä¸ºäºŒå…ƒè¯­æ³•ä½¿ç”¨

è¯„ä¼°æŒ‡æ ‡:

*   æµ‹è¯•å‡†ç¡®ç‡ä¸º 81.3%ã€‚è¿™ä¼˜äºé¢„æµ‹æ‰€æœ‰è§‚å¯Ÿçš„å¤šæ•°ç±»(è¿™é‡Œæ˜¯è´Ÿé¢æƒ…ç»ª)çš„åŸºçº¿æ€§èƒ½ã€‚åŸºçº¿ä¼šç»™å‡º 63%çš„å‡†ç¡®åº¦ã€‚
*   è¿™ä¸‰ä¸ªç±»åˆ«çš„ç²¾åº¦éƒ½ç›¸å½“é«˜ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬é¢„æµ‹ä¸ºè´Ÿé¢çš„æ‰€æœ‰æ¡ˆä¾‹ä¸­ï¼Œ80%æ˜¯è´Ÿé¢çš„ã€‚
*   ä¸­æ€§ç±»çš„å¬å›ç‡å¾ˆä½ã€‚åœ¨æˆ‘ä»¬æµ‹è¯•æ•°æ®çš„æ‰€æœ‰ä¸­æ€§æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åªé¢„æµ‹ 48%æ˜¯ä¸­æ€§çš„ã€‚

# å¯¹æ–°æ¨æ–‡åº”ç”¨æœ€ä½³æ¨¡å‹

ä¸ºäº†å¥½ç©ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸€äº›åŒ…å«â€œ@VirginAmericaâ€çš„æ–°æ¨æ–‡ã€‚æˆ‘æ‰‹åŠ¨é€‰æ‹©äº† 3 æ¡è´Ÿé¢å’Œ 3 æ¡æ­£é¢çš„æ¨æ–‡ã€‚

å¤šäºäº† GridSearchCVï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“äº†ä»€ä¹ˆæ˜¯æœ€å¥½çš„è¶…å‚æ•°ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æ‰€æœ‰è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒæœ€ä½³æ¨¡å‹ï¼ŒåŒ…æ‹¬æˆ‘ä»¬ä¹‹å‰åˆ†ç¦»çš„æµ‹è¯•æ•°æ®ã€‚

```
textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'
,'count_mentions','count_urls','count_words']features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))
, ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))
, ('vect', CountVectorizer(max_df=0.5, min_df=1, ngram_range=(1,2)))]))]
, n_jobs=-1)pipeline = Pipeline([
('features', features)
, ('clf', LogisticRegression(C=1.0, penalty='l2'))
])best_model = pipeline.fit(df_model.drop('airline_sentiment', axis=1), df_model.airline_sentiment)# Applying on new positive tweets
new_positive_tweets = pd.Series(["Thank you @VirginAmerica for you amazing customer support team on Tuesday 11/28 at @EWRairport and returning my lost bag in less than 24h! #efficiencyiskey #virginamerica"
,"Love flying with you guys ask these years. Sad that this will be the last trip ğŸ˜‚ @VirginAmerica #LuxuryTravel"
,"Wow @VirginAmerica main cabin select is the way to fly!! This plane is nice and clean & I have tons of legroom! Wahoo! NYC bound! âœˆï¸"])df_counts_pos = tc.transform(new_positive_tweets)
df_clean_pos = ct.transform(new_positive_tweets)
df_model_pos = df_counts_pos
df_model_pos['clean_text'] = df_clean_posbest_model.predict(df_model_pos).tolist()# Applying on new negative tweets
new_negative_tweets = pd.Series(["@VirginAmerica shocked my initially with the service, but then went on to shock me further with no response to what my complaint was. #unacceptable @Delta @richardbranson"
,"@VirginAmerica this morning I was forced to repack a suitcase w a medical device because it was barely overweight - wasn't even given an option to pay extra. My spouses suitcase then burst at the seam with the added device and had to be taped shut. Awful experience so far!"
,"Board airplane home. Computer issue. Get off plane, traverse airport to gate on opp side. Get on new plane hour later. Plane too heavy. 8 volunteers get off plane. Ohhh the adventure of travel âœˆï¸ @VirginAmerica"])df_counts_neg = tc.transform(new_negative_tweets)
df_clean_neg = ct.transform(new_negative_tweets)
df_model_neg = df_counts_neg
df_model_neg['clean_text'] = df_clean_negbest_model.predict(df_model_neg).tolist()
```

è¯¥æ¨¡å‹å¯¹æ‰€æœ‰æ¨æ–‡è¿›è¡Œäº†æ­£ç¡®åˆ†ç±»ã€‚åº”è¯¥ä½¿ç”¨æ›´å¤§çš„æµ‹è¯•é›†æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ä½†æ˜¯åœ¨è¿™ä¸ªå°æ•°æ®é›†ä¸Šï¼Œå®ƒåšäº†æˆ‘ä»¬æƒ³è¦åšçš„äº‹æƒ…ã€‚

æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¯»è¿™ä¸ªæ•…äº‹ã€‚å¦‚æœä½ åšåˆ°äº†ï¼Œè¯·é¼“æŒã€‚