<html>
<head>
<title>Sentiment Analysis with Text Mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">åŸºäºæ–‡æœ¬æŒ–æ˜çš„æƒ…æ„Ÿåˆ†æ</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27?source=collection_archive---------2-----------------------#2018-08-25">https://towardsdatascience.com/sentiment-analysis-with-text-mining-13dd2b33de27?source=collection_archive---------2-----------------------#2018-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a0fa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">äº†è§£å¦‚ä½•å‡†å¤‡æ–‡æœ¬æ•°æ®å¹¶è¿è¡Œä¸¤ä¸ªä¸åŒçš„åˆ†ç±»å™¨æ¥é¢„æµ‹æ¨æ–‡çš„æƒ…ç»ªã€‚</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/61b08fae380fe5c8bbf93e09c55907fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FIhWkx0Ty2435Fjb"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@rvignes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Romain Vignes</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="361d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†æ¢ç´¢ä¸€äº›ç”¨äºæƒ…æ„Ÿåˆ†æçš„æ–‡æœ¬æŒ–æ˜æŠ€æœ¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†èŠ±ä¸€äº›æ—¶é—´å‡†å¤‡æ–‡æœ¬æ•°æ®ã€‚è¿™å°†æ¶‰åŠæ¸…ç†æ–‡æœ¬æ•°æ®ï¼Œåˆ é™¤åœç”¨è¯å’Œè¯å¹²ã€‚ä¸ºæ­¤ï¼ŒKaggle ä¸Šçš„<a class="ae kv" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment" rel="noopener ugc nofollow" target="_blank"> Twitter ç¾å›½èˆªç©ºå…¬å¸æƒ…ç»ªæ•°æ®é›†éå¸¸é€‚åˆåˆä½œã€‚å®ƒåŒ…å« tweet çš„æ–‡æœ¬å’Œä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªå¯èƒ½æƒ…æ„Ÿå€¼çš„å˜é‡ã€‚</a></p><p id="1e5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ä¸ºäº†æ¨æ–­æ¨æ–‡çš„æƒ…æ„Ÿï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªåˆ†ç±»å™¨:é€»è¾‘å›å½’å’Œå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ç½‘æ ¼æœç´¢æ¥è°ƒæ•´ä¸¤ä¸ªåˆ†ç±»å™¨çš„è¶…å‚æ•°ã€‚</p><p id="f696" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æˆ‘ä»¬å°†ç”¨ä¸‰ä¸ªæŒ‡æ ‡æ¥æ¯”è¾ƒæ€§èƒ½:ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°ã€‚</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="244c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æˆ‘ä»¬ä»å¯¼å…¥åŒ…å’Œé…ç½®ä¸€äº›è®¾ç½®å¼€å§‹ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="2e40" class="me mf iq ma b gy mg mh l mi mj">import numpy as np <br/>import pandas as pd <br/>pd.set_option('display.max_colwidth', -1)<br/>from time import time<br/>import re<br/>import string<br/>import os<br/>import emoji<br/>from pprint import pprint<br/>import collections</span><span id="312a" class="me mf iq ma b gy mk mh l mi mj">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style="darkgrid")<br/>sns.set(font_scale=1.3)</span><span id="9965" class="me mf iq ma b gy mk mh l mi mj">from sklearn.base import BaseEstimator, TransformerMixin<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.pipeline import Pipeline, FeatureUnion<br/>from sklearn.metrics import classification_report</span><span id="0e09" class="me mf iq ma b gy mk mh l mi mj">from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.externals import joblib</span><span id="9942" class="me mf iq ma b gy mk mh l mi mj">import gensim</span><span id="65d2" class="me mf iq ma b gy mk mh l mi mj">from nltk.corpus import stopwords<br/>from nltk.stem import PorterStemmer<br/>from nltk.tokenize import word_tokenize</span><span id="1024" class="me mf iq ma b gy mk mh l mi mj">import warnings<br/>warnings.filterwarnings('ignore')</span><span id="cd8f" class="me mf iq ma b gy mk mh l mi mj">np.random.seed(37)</span></pre><h1 id="052d" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">åŠ è½½æ•°æ®</h1><p id="aa49" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">æˆ‘ä»¬è¯»å–äº†ä» Kaggle æ•°æ®é›†ä¸‹è½½çš„é€—å·åˆ†éš”æ–‡ä»¶ã€‚æˆ‘ä»¬æ‰“ä¹±äº†æ•°æ®å¸§ï¼Œä»¥é˜²ç±»è¢«æ’åºã€‚å¯¹åŸå§‹æŒ‡æ•°çš„<code class="fe nh ni nj ma b">permutation</code>åº”ç”¨<code class="fe nh ni nj ma b">reindex</code>æ–¹æ³•æœ‰åˆ©äºæ­¤ã€‚åœ¨è¿™æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨<code class="fe nh ni nj ma b">text</code>å˜é‡å’Œ<code class="fe nh ni nj ma b">airline_sentiment</code>å˜é‡ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="6805" class="me mf iq ma b gy mg mh l mi mj">df = pd.read_csv('../input/Tweets.csv')<br/>df = df.reindex(np.random.permutation(df.index))<br/>df = df[['text', 'airline_sentiment']]</span></pre><h1 id="7240" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">æ¢ç´¢æ€§æ•°æ®åˆ†æ</h1><h2 id="85ee" class="me mf iq bd mm nk nl dn mq nm nn dp mu lf no np mw lj nq nr my ln ns nt na nu bi translated">ç›®æ ‡å˜é‡</h2><p id="0e7c" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">æˆ‘ä»¬å°†é¢„æµ‹ä¸‰ä¸ªç±»åˆ«æ ‡ç­¾:è´Ÿé¢ã€ä¸­æ€§æˆ–æ­£é¢ã€‚</p><p id="2e33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æ­£å¦‚æˆ‘ä»¬åœ¨ä¸‹é¢çš„å›¾è¡¨ä¸­çœ‹åˆ°çš„ï¼Œåˆ†ç±»æ ‡ç­¾æ˜¯ä¸å¹³è¡¡çš„ã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨æ¨¡å‹è®­ç»ƒé˜¶æ®µåº”è¯¥è®°ä½çš„äº‹æƒ…ã€‚æœ‰äº† seaborn åŒ…çš„<code class="fe nh ni nj ma b">factorplot</code>ï¼Œæˆ‘ä»¬å¯ä»¥å¯è§†åŒ–ç›®æ ‡å˜é‡çš„åˆ†å¸ƒã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="93a3" class="me mf iq ma b gy mg mh l mi mj">sns.factorplot(x="airline_sentiment", data=df, kind="count", size=6, aspect=1.5, palette="PuBuGn_d")<br/>plt.show();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/d091427d229243d724e1f4ac320ecc12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*v99Gfk4iL4POvy2F.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Imbalanced distribution of the target class labels</figcaption></figure><h2 id="9c86" class="me mf iq bd mm nk nl dn mq nm nn dp mu lf no np mw lj nq nr my ln ns nt na nu bi translated">è¾“å…¥å˜é‡</h2><p id="1510" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">ä¸ºäº†åˆ†æ<code class="fe nh ni nj ma b">text </code>å˜é‡ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç±»<code class="fe nh ni nj ma b">TextCounts</code>ã€‚åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæˆ‘ä»¬è®¡ç®—æ–‡æœ¬å˜é‡çš„ä¸€äº›åŸºæœ¬ç»Ÿè®¡æ•°æ®ã€‚</p><ul class=""><li id="7347" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_words</code>:æ¨æ–‡å­—æ•°</li><li id="1106" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_mentions</code>:å…¶ä»– Twitter è´¦æˆ·çš„æ¨èä»¥@å¼€å¤´</li><li id="8299" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_hashtags</code>:æ ‡ç­¾å­—æ•°ï¼Œä»¥#å¼€å¤´</li><li id="d0f7" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_capital_words</code>:ä¸€äº›å¤§å†™å•è¯æœ‰æ—¶è¢«ç”¨æ¥â€œå«å–Šâ€å’Œè¡¨è¾¾(è´Ÿé¢)æƒ…ç»ª</li><li id="c46a" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_excl_quest_marks</code>:é—®å·æˆ–æ„Ÿå¹å·çš„ä¸ªæ•°</li><li id="21c3" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_urls</code>:æ¨æ–‡ä¸­çš„é“¾æ¥æ•°é‡ï¼Œä»¥ http(s)å¼€å¤´</li><li id="07e1" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated"><code class="fe nh ni nj ma b">count_emojis</code>:è¡¨æƒ…ç¬¦å·çš„æ•°é‡ï¼Œè¿™å¯èƒ½æ˜¯æƒ…ç»ªçš„ä¸€ä¸ªå¥½è¿¹è±¡</li></ul><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="227d" class="me mf iq ma b gy mg mh l mi mj">class TextCounts(BaseEstimator, TransformerMixin):<br/>    <br/>    def count_regex(self, pattern, tweet):<br/>        return len(re.findall(pattern, tweet))<br/>    <br/>    def fit(self, X, y=None, **fit_params):<br/>        # fit method is used when specific operations need to be done on the train data, but not on the test data<br/>        return self<br/>    <br/>    def transform(self, X, **transform_params):<br/>        count_words = X.apply(lambda x: self.count_regex(r'\w+', x)) <br/>        count_mentions = X.apply(lambda x: self.count_regex(r'@\w+', x))<br/>        count_hashtags = X.apply(lambda x: self.count_regex(r'#\w+', x))<br/>        count_capital_words = X.apply(lambda x: self.count_regex(r'\b[A-Z]{2,}\b', x))<br/>        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\?', x))<br/>        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\s]+[\s]?', x))<br/>        # We will replace the emoji symbols with a description, which makes using a regex for counting easier<br/>        # Moreover, it will result in having more words in the tweet<br/>        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&amp;]+:', x))<br/>        <br/>        df = pd.DataFrame({'count_words': count_words<br/>                           , 'count_mentions': count_mentions<br/>                           , 'count_hashtags': count_hashtags<br/>                           , 'count_capital_words': count_capital_words<br/>                           , 'count_excl_quest_marks': count_excl_quest_marks<br/>                           , 'count_urls': count_urls<br/>                           , 'count_emojis': count_emojis<br/>                          })<br/>        <br/>        return df<br/>tc = TextCounts()<br/>df_eda = tc.fit_transform(df.text)<br/>df_eda['airline_sentiment'] = df.airline_sentiment</span></pre><p id="fc92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æŸ¥çœ‹ TextStats å˜é‡ä¸ class å˜é‡çš„å…³ç³»å¯èƒ½ä¼šå¾ˆæœ‰è¶£ã€‚å› æ­¤æˆ‘ä»¬ç¼–å†™äº†ä¸€ä¸ªå‡½æ•°<code class="fe nh ni nj ma b">show_dist</code>,å®ƒä¸ºæ¯ä¸ªç›®æ ‡ç±»æä¾›äº†æè¿°æ€§çš„ç»Ÿè®¡æ•°æ®å’Œå›¾è¡¨ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="ab71" class="me mf iq ma b gy mg mh l mi mj">def show_dist(df, col):<br/>    print('Descriptive stats for {}'.format(col))<br/>    print('-'*(len(col)+22))<br/>    print(df.groupby('airline_sentiment')[col].describe())<br/>    bins = np.arange(df[col].min(), df[col].max() + 1)<br/>    g = sns.FacetGrid(df, col='airline_sentiment', size=5, hue='airline_sentiment', palette="PuBuGn_d")<br/>    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)<br/>    plt.show()</span></pre><p id="2ec1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ä¸‹é¢ä½ å¯ä»¥æ‰¾åˆ°æ¯ä¸ªç›®æ ‡ç±»åˆ«çš„ tweet å­—æ•°åˆ†å¸ƒã€‚ä¸ºç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å°†ä»…é™äºè¿™ä¸ªå˜é‡ã€‚æ‰€æœ‰ TextCounts å˜é‡çš„å›¾è¡¨éƒ½åœ¨ Github çš„<a class="ae kv" href="https://github.com/bertcarremans/TwitterUSAirlineSentiment" rel="noopener ugc nofollow" target="_blank">ç¬”è®°æœ¬é‡Œã€‚</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/4da06de8a1834b9ca420b45840cc0c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*snmvA3GQOb_S9wV8.png"/></div></div></figure><ul class=""><li id="ac09" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">æ¨æ–‡ä¸­ä½¿ç”¨çš„å­—æ•°ç›¸å½“ä½ã€‚æœ€å¤§çš„å­—æ•°æ˜¯ 36 ä¸ªï¼Œç”šè‡³æœ‰åªæœ‰ 2 ä¸ªå­—çš„æ¨æ–‡ã€‚æ‰€ä»¥åœ¨æ•°æ®æ¸…ç†è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»å°å¿ƒï¼Œä¸è¦åˆ é™¤å¤ªå¤šçš„å•è¯ã€‚ä½†æ˜¯æ–‡å­—å¤„ç†ä¼šæ›´å¿«ã€‚è´Ÿé¢æ¨æ–‡æ¯”ä¸­æ€§æˆ–æ­£é¢æ¨æ–‡åŒ…å«æ›´å¤šå•è¯ã€‚</li><li id="6bc7" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">æ‰€æœ‰æ¨æ–‡è‡³å°‘æœ‰ä¸€æ¬¡æåŠã€‚è¿™æ˜¯åŸºäº Twitter æ•°æ®ä¸­çš„æåŠæå–æ¨æ–‡çš„ç»“æœã€‚å°±æƒ…æ„Ÿè€Œè¨€ï¼ŒæåŠæ¬¡æ•°ä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆä¸åŒã€‚</li><li id="3b4d" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å¤§å¤šæ•°æ¨æ–‡ä¸åŒ…å«å“ˆå¸Œæ ‡ç­¾ã€‚æ‰€ä»¥è¿™ä¸ªå˜é‡åœ¨æ¨¡å‹è®­ç»ƒæœŸé—´ä¸ä¼šè¢«ä¿ç•™ã€‚åŒæ ·ï¼Œå¯¹äºæƒ…æ„Ÿï¼Œæ•£åˆ—æ ‡ç­¾çš„æ•°é‡æ²¡æœ‰å·®åˆ«ã€‚</li><li id="d37e" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å¤§å¤šæ•°æ¨æ–‡ä¸åŒ…å«å¤§å†™å•è¯ï¼Œæˆ‘ä»¬çœ‹ä¸åˆ°æƒ…ç»ªåˆ†å¸ƒçš„å·®å¼‚ã€‚</li><li id="da32" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">ç§¯æçš„æ¨æ–‡ä¼¼ä¹ä½¿ç”¨äº†æ›´å¤šçš„æ„Ÿå¹æˆ–é—®å·ã€‚</li><li id="9aeb" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å¤§å¤šæ•°æ¨æ–‡ä¸åŒ…å«ç½‘å€ã€‚</li><li id="7c29" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å¤§å¤šæ•°æ¨æ–‡ä¸ä½¿ç”¨è¡¨æƒ…ç¬¦å·ã€‚</li></ul><h1 id="a380" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">æ–‡æœ¬æ¸…ç†</h1><p id="de93" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">åœ¨æˆ‘ä»¬å¼€å§‹ä½¿ç”¨ tweets çš„æ–‡æœ¬ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ¸…ç†å®ƒã€‚æˆ‘ä»¬å°†åœ¨<code class="fe nh ni nj ma b">CleanText</code> <strong class="ky ir">è¯¾ä¸Šåšè¿™ä»¶äº‹ã€‚</strong>åœ¨è¿™ä¸ªè¯¾ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œä»¥ä¸‹æ“ä½œ:</p><ul class=""><li id="9ed4" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">åˆ é™¤æåŠï¼Œå› ä¸ºæˆ‘ä»¬ä¹Ÿæƒ³æ¨å¹¿åˆ°å…¶ä»–èˆªç©ºå…¬å¸çš„æ¨æ–‡ã€‚</li><li id="cf41" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">åˆ é™¤æ•£åˆ—æ ‡ç­¾ç¬¦å·(#)ï¼Œä½†ä¸è¦åˆ é™¤å®é™…çš„æ ‡ç­¾ï¼Œå› ä¸ºè¿™å¯èƒ½åŒ…å«ä¿¡æ¯</li><li id="3233" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å°†æ‰€æœ‰å•è¯è®¾ä¸ºå°å†™</li><li id="e066" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">åˆ é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒåŒ…æ‹¬é—®å·å’Œæ„Ÿå¹å·</li><li id="28fd" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">åˆ é™¤ç½‘å€ï¼Œå› ä¸ºå®ƒä»¬ä¸åŒ…å«æœ‰ç”¨çš„ä¿¡æ¯ã€‚æˆ‘ä»¬æ²¡æœ‰æ³¨æ„åˆ°æƒ…æ„Ÿç±»åˆ«ä¹‹é—´ä½¿ç”¨çš„ URL æ•°é‡çš„å·®å¼‚</li><li id="3337" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">ç¡®ä¿å°†è¡¨æƒ…ç¬¦å·è½¬æ¢æˆä¸€ä¸ªå•è¯ã€‚</li><li id="0303" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">åˆ é™¤æ•°å­—</li><li id="6840" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">åˆ é™¤åœç”¨è¯</li><li id="9501" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">åº”ç”¨<code class="fe nh ni nj ma b">PorterStemmer</code>ä¿ç•™å•è¯çš„è¯å¹²</li></ul><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="aa45" class="me mf iq ma b gy mg mh l mi mj">class CleanText(BaseEstimator, TransformerMixin):<br/>    def remove_mentions(self, input_text):<br/>        return re.sub(r'@\w+', '', input_text)<br/>    <br/>    def remove_urls(self, input_text):<br/>        return re.sub(r'http.?://[^\s]+[\s]?', '', input_text)<br/>    <br/>    def emoji_oneword(self, input_text):<br/>        # By compressing the underscore, the emoji is kept as one word<br/>        return input_text.replace('_','')<br/>    <br/>    def remove_punctuation(self, input_text):<br/>        # Make translation table<br/>        punct = string.punctuation<br/>        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space<br/>        return input_text.translate(trantab)</span><span id="f501" class="me mf iq ma b gy mk mh l mi mj">    def remove_digits(self, input_text):<br/>        return re.sub('\d+', '', input_text)<br/>    <br/>    def to_lower(self, input_text):<br/>        return input_text.lower()<br/>    <br/>    def remove_stopwords(self, input_text):<br/>        stopwords_list = stopwords.words('english')<br/>        # Some words which might indicate a certain sentiment are kept via a whitelist<br/>        whitelist = ["n't", "not", "no"]<br/>        words = input_text.split() <br/>        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) &gt; 1] <br/>        return " ".join(clean_words) <br/>    <br/>    def stemming(self, input_text):<br/>        porter = PorterStemmer()<br/>        words = input_text.split() <br/>        stemmed_words = [porter.stem(word) for word in words]<br/>        return " ".join(stemmed_words)<br/>    <br/>    def fit(self, X, y=None, **fit_params):<br/>        return self<br/>    <br/>    def transform(self, X, **transform_params):<br/>        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)<br/>        return clean_X</span></pre><p id="413c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ä¸ºäº†å±•ç¤ºæ¸…ç†åçš„æ–‡æœ¬å˜é‡çš„å¤–è§‚ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç¤ºä¾‹ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="922c" class="me mf iq ma b gy mg mh l mi mj">ct = CleanText()<br/>sr_clean = ct.fit_transform(df.text)<br/>sr_clean.sample(5)</span></pre><blockquote class="ol om on"><p id="67cf" class="kw kx oo ky b kz la jr lb lc ld ju le op lg lh li oq lk ll lm or lo lp lq lr ij bi translated">é«˜å…´ rt æ‰“èµŒé¸Ÿæ„¿é£å—æ–¹å†¬å¤©<br/>ç‚¹ upc ä»£ç æ£€æŸ¥ baggag å‘Šè¯‰ luggag vacat day tri æ³³è£…<br/> vx jfk la dirti é£æœºä¸æ ‡å‡†<br/>å‘Šè¯‰æ„å‘³ç€å·¥ä½œéœ€è¦ä¼°è®¡æ—¶é—´åˆ°è¾¾è¯·éœ€è¦ç¬”è®°æœ¬ç”µè„‘å·¥ä½œæ„Ÿè°¢<br/>å½“ç„¶ä¸šåŠ¡å» els èˆªç©ºæ—…è¡Œå§“åå‡¯ç‘Ÿç³ç´¢ç‰¹ç½—</p></blockquote><p id="afe1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æ–‡æœ¬æ¸…ç†çš„ä¸€ä¸ªå‰¯ä½œç”¨æ˜¯ä¸€äº›è¡Œçš„æ–‡æœ¬ä¸­æ²¡æœ‰ä»»ä½•å•è¯ã€‚å¯¹äº<code class="fe nh ni nj ma b">CountVectorizer</code>å’Œ<code class="fe nh ni nj ma b">TfIdfVectorizer</code>æ¥è¯´ï¼Œè¿™ä¸æˆé—®é¢˜ã€‚ç„¶è€Œï¼Œå¯¹äº<code class="fe nh ni nj ma b">Word2Vec</code>ç®—æ³•æ¥è¯´ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªé”™è¯¯ã€‚æœ‰ä¸åŒçš„ç­–ç•¥æ¥å¤„ç†è¿™äº›ç¼ºå¤±çš„ä»·å€¼è§‚ã€‚</p><ul class=""><li id="8aef" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">åˆ é™¤æ•´è¡Œï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿™æ˜¯ä¸å¯å–çš„ã€‚</li><li id="7649" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">ç”¨ç±»ä¼¼*[no_text]*çš„å ä½ç¬¦æ–‡æœ¬ä¼°ç®—ç¼ºå¤±å€¼</li><li id="cd99" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å½“åº”ç”¨ Word2Vec æ—¶:ä½¿ç”¨æ‰€æœ‰å‘é‡çš„å¹³å‡å€¼</li></ul><p id="d1e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">è¿™é‡Œæˆ‘ä»¬å°†ä½¿ç”¨å ä½ç¬¦æ–‡æœ¬è¿›è¡Œä¼°ç®—ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="1673" class="me mf iq ma b gy mg mh l mi mj">empty_clean = sr_clean == ''<br/>print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))<br/>sr_clean.loc[empty_clean] = '[no_text]'</span></pre><p id="5187" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»æ¸…ç†äº†æ¨æ–‡çš„æ–‡æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹æœ€å¸¸ç”¨çš„è¯æ˜¯ä»€ä¹ˆã€‚ä¸‹é¢æˆ‘ä»¬å°†å±•ç¤ºå‰ 20 ä¸ªå•è¯ã€‚å‡ºç°é¢‘ç‡æœ€é«˜çš„è¯æ˜¯â€œé€ƒâ€ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="ee0e" class="me mf iq ma b gy mg mh l mi mj">cv = CountVectorizer()<br/>bow = cv.fit_transform(sr_clean)<br/>word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))<br/>word_counter = collections.Counter(word_freq)<br/>word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])</span><span id="cedc" class="me mf iq ma b gy mk mh l mi mj">fig, ax = plt.subplots(figsize=(12, 10))<br/>sns.barplot(x="word", y="freq", data=word_counter_df, palette="PuBuGn_d", ax=ax)<br/>plt.show();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/bc20216c86f3bcc941c7cd785f7e52b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hBvkYfey1Astmd02.png"/></div></div></figure><h1 id="f641" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">åˆ›å»ºæµ‹è¯•æ•°æ®</h1><p id="6d42" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">ä¸ºäº†æ£€æŸ¥æ¨¡å‹çš„æ€§èƒ½ï¼Œæˆ‘ä»¬éœ€è¦ä¸€å¥—æµ‹è¯•è®¾å¤‡ã€‚å¯¹è®­ç»ƒæ•°æ®çš„è¯„ä¼°æ˜¯ä¸æ­£ç¡®çš„ã€‚æ‚¨ä¸åº”è¯¥åœ¨ç”¨äºè®­ç»ƒæ¨¡å‹çš„ç›¸åŒæ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ã€‚</p><p id="0ae5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬å°†<code class="fe nh ni nj ma b">TextCounts</code>å˜é‡ä¸<code class="fe nh ni nj ma b">CleanText</code>å˜é‡ç»“åˆèµ·æ¥ã€‚æœ€åˆï¼Œæˆ‘åœ¨<code class="fe nh ni nj ma b">GridSearchCV</code>ä¸­é”™è¯¯åœ°æ‰§è¡Œäº† TextCounts å’Œ CleanTextã€‚è¿™èŠ±è´¹äº†å¤ªé•¿æ—¶é—´ï¼Œå› ä¸ºæ¯æ¬¡è¿è¡Œ GridSearch éƒ½è¦åº”ç”¨è¿™äº›å‡½æ•°ã€‚åªè¿è¡Œä¸€æ¬¡å°±è¶³å¤Ÿäº†ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="bd7c" class="me mf iq ma b gy mg mh l mi mj">df_model = df_eda<br/>df_model['clean_text'] = sr_clean<br/>df_model.columns.tolist()</span></pre><p id="4e5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æ‰€ä»¥<code class="fe nh ni nj ma b">df_model</code>ç°åœ¨åŒ…å«äº†å‡ ä¸ªå˜é‡ã€‚ä½†æ˜¯æˆ‘ä»¬çš„çŸ¢é‡å™¨(è§ä¸‹æ–‡)å°†åªéœ€è¦<code class="fe nh ni nj ma b">clean_text</code>å˜é‡ã€‚å¯ä»¥æ·»åŠ <code class="fe nh ni nj ma b">TextCounts</code>å˜é‡ã€‚ä¸ºäº†é€‰æ‹©åˆ—ï¼Œæˆ‘ç¼–å†™äº†ä¸‹é¢çš„ç±»<code class="fe nh ni nj ma b">ColumnExtractor</code>ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="80f5" class="me mf iq ma b gy mg mh l mi mj">class ColumnExtractor(TransformerMixin, BaseEstimator):<br/>    def __init__(self, cols):<br/>        self.cols = cols</span><span id="1764" class="me mf iq ma b gy mk mh l mi mj">    def transform(self, X, **transform_params):<br/>        return X[self.cols]</span><span id="4a95" class="me mf iq ma b gy mk mh l mi mj">    def fit(self, X, y=None, **fit_params):<br/>        return self</span><span id="3a04" class="me mf iq ma b gy mk mh l mi mj">X_train, X_test, y_train, y_test = train_test_split(df_model.drop('airline_sentiment', axis=1), df_model.airline_sentiment, test_size=0.1, random_state=37)</span></pre><h1 id="7339" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">è¶…å‚æ•°è°ƒæ•´å’Œäº¤å‰éªŒè¯</h1><p id="e621" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹åˆ°çš„ï¼ŒçŸ¢é‡å™¨å’Œåˆ†ç±»å™¨éƒ½æœ‰å¯é…ç½®çš„å‚æ•°ã€‚ä¸ºäº†é€‰æ‹©æœ€ä½³å‚æ•°ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å•ç‹¬çš„éªŒè¯é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚åŸ¹è®­æœŸé—´æ²¡æœ‰ä½¿ç”¨è¯¥éªŒè¯é›†ã€‚ç„¶è€Œï¼Œä»…ä½¿ç”¨ä¸€ä¸ªéªŒè¯é›†å¯èƒ½ä¸ä¼šäº§ç”Ÿå¯é çš„éªŒè¯ç»“æœã€‚ç”±äºå¶ç„¶çš„æœºä¼šï¼Œæ‚¨å¯èƒ½åœ¨éªŒè¯é›†ä¸Šæœ‰ä¸€ä¸ªå¥½çš„æ¨¡å‹æ€§èƒ½ã€‚å¦‚æœæ‚¨ä»¥å…¶ä»–æ–¹å¼åˆ†å‰²æ•°æ®ï¼Œæ‚¨å¯èƒ½ä¼šå¾—åˆ°å…¶ä»–ç»“æœã€‚ä¸ºäº†å¾—åˆ°æ›´å‡†ç¡®çš„ä¼°è®¡ï¼Œæˆ‘ä»¬è¿›è¡Œäº¤å‰éªŒè¯ã€‚</p><p id="4e5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">é€šè¿‡äº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬å¯ä»¥å¤šæ¬¡å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚ç„¶ååœ¨ä¸åŒçš„æŠ˜å ä¸Šå¯¹è¯„ä¼°åº¦é‡è¿›è¡Œå¹³å‡ã€‚å¹¸è¿çš„æ˜¯ï¼ŒGridSearchCV åº”ç”¨äº†ç°æˆçš„äº¤å‰éªŒè¯ã€‚</p><p id="e780" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ä¸ºäº†æ‰¾åˆ°çŸ¢é‡å™¨å’Œåˆ†ç±»å™¨çš„æœ€ä½³å‚æ•°ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª<code class="fe nh ni nj ma b">Pipeline</code>ã€‚</p><h1 id="4a41" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">è¯„ä¼°æŒ‡æ ‡</h1><p id="a114" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">é»˜è®¤æƒ…å†µä¸‹ï¼ŒGridSearchCV ä½¿ç”¨é»˜è®¤è®¡åˆ†å™¨æ¥è®¡ç®—<code class="fe nh ni nj ma b">best_score_</code>ã€‚å¯¹äº<code class="fe nh ni nj ma b">MultiNomialNb</code>å’Œ<code class="fe nh ni nj ma b">LogisticRegression</code>æ¥è¯´ï¼Œè¿™ä¸ªé»˜è®¤çš„è¯„åˆ†æ ‡å‡†æ˜¯å‡†ç¡®æ€§ã€‚</p><p id="412b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">åœ¨æˆ‘ä»¬çš„å‡½æ•°<code class="fe nh ni nj ma b">grid_vect</code>ä¸­ï¼Œæˆ‘ä»¬é¢å¤–ç”Ÿæˆäº†æµ‹è¯•æ•°æ®çš„<code class="fe nh ni nj ma b">classification_report</code>ã€‚è¿™ä¸ºæ¯ä¸ªç›®æ ‡ç±»æä¾›äº†ä¸€äº›æœ‰è¶£çš„åº¦é‡ã€‚è¿™åœ¨è¿™é‡Œå¯èƒ½æ›´åˆé€‚ã€‚è¿™äº›æŒ‡æ ‡æ˜¯ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ F1 åˆ†æ•°<strong class="ky ir">ã€‚</strong></p><ul class=""><li id="fe0e" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">Precision <strong class="ky ir"> : </strong>åœ¨æˆ‘ä»¬é¢„æµ‹ä¸ºæŸä¸ªç±»çš„æ‰€æœ‰è¡Œä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®é¢„æµ‹äº†å¤šå°‘ï¼Ÿ</li><li id="a6bd" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">å›æƒ³ä¸€ä¸‹<strong class="ky ir"> : </strong>åœ¨æŸä¸ªç±»çš„æ‰€æœ‰è¡Œä¸­ï¼Œæˆ‘ä»¬æ­£ç¡®é¢„æµ‹äº†å¤šå°‘è¡Œï¼Ÿ</li><li id="9856" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">F1 å¾—åˆ†<strong class="ky ir"> : </strong>ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„è°ƒå’Œå¹³å‡å€¼ã€‚</li></ul><p id="ddd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">åˆ©ç”¨<a class="ae kv" href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="noopener ugc nofollow" target="_blank">æ··æ·†çŸ©é˜µ</a>çš„å…ƒç´ ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç²¾ç¡®åº¦å’Œå¬å›ç‡ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="9051" class="me mf iq ma b gy mg mh l mi mj"># Based on <a class="ae kv" href="http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html" rel="noopener ugc nofollow" target="_blank">http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html</a><br/>def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):<br/>    <br/>    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'<br/>                      ,'count_mentions','count_urls','count_words']<br/>    <br/>    if is_w2v:<br/>        w2vcols = []<br/>        for i in range(SIZE):<br/>            w2vcols.append(i)<br/>        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))<br/>                                 , ('w2v', ColumnExtractor(cols=w2vcols))]<br/>                                , n_jobs=-1)<br/>    else:<br/>        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))<br/>                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]<br/>                                , n_jobs=-1)</span><span id="9c19" class="me mf iq ma b gy mk mh l mi mj">    <br/>    pipeline = Pipeline([<br/>        ('features', features)<br/>        , ('clf', clf)<br/>    ])<br/>    <br/>    # Join the parameters dictionaries together<br/>    parameters = dict()<br/>    if parameters_text:<br/>        parameters.update(parameters_text)<br/>    parameters.update(parameters_clf)</span><span id="e4ba" class="me mf iq ma b gy mk mh l mi mj">    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics<br/>    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)<br/>    <br/>    print("Performing grid search...")<br/>    print("pipeline:", [name for name, _ in pipeline.steps])<br/>    print("parameters:")<br/>    pprint(parameters)</span><span id="416e" class="me mf iq ma b gy mk mh l mi mj">    t0 = time()<br/>    grid_search.fit(X_train, y_train)<br/>    print("done in %0.3fs" % (time() - t0))<br/>    print()</span><span id="9767" class="me mf iq ma b gy mk mh l mi mj">    print("Best CV score: %0.3f" % grid_search.best_score_)<br/>    print("Best parameters set:")<br/>    best_parameters = grid_search.best_estimator_.get_params()<br/>    for param_name in sorted(parameters.keys()):<br/>        print("\t%s: %r" % (param_name, best_parameters[param_name]))<br/>        <br/>    print("Test score with best_estimator_: %0.3f" % grid_search.best_estimator_.score(X_test, y_test))<br/>    print("\n")<br/>    print("Classification Report Test Data")<br/>    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))<br/>                        <br/>    return grid_search</span></pre><h1 id="a8c2" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">GridSearchCV çš„å‚æ•°ç½‘æ ¼</h1><p id="07b3" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">åœ¨ç½‘æ ¼æœç´¢ä¸­ï¼Œæˆ‘ä»¬å°†ç ”ç©¶åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚ç”¨äºæµ‹è¯•æ€§èƒ½çš„ä¸€ç»„å‚æ•°å¦‚ä¸‹æ‰€ç¤ºã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="7d6a" class="me mf iq ma b gy mg mh l mi mj"># Parameter grid settings for the vectorizers (Count and TFIDF)<br/>parameters_vect = {<br/>    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),<br/>    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),<br/>    'features__pipe__vect__min_df': (1,2)<br/>}<br/></span><span id="bea5" class="me mf iq ma b gy mk mh l mi mj"># Parameter grid settings for MultinomialNB<br/>parameters_mnb = {<br/>    'clf__alpha': (0.25, 0.5, 0.75)<br/>}<br/></span><span id="bc65" class="me mf iq ma b gy mk mh l mi mj"># Parameter grid settings for LogisticRegression<br/>parameters_logreg = {<br/>    'clf__C': (0.25, 0.5, 1.0),<br/>    'clf__penalty': ('l1', 'l2')<br/>}</span></pre><h1 id="94d2" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">åˆ†ç±»å™¨</h1><p id="4d0f" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">è¿™é‡Œæˆ‘ä»¬å°†æ¯”è¾ƒä¸€ä¸‹<code class="fe nh ni nj ma b">MultinomialNB</code>å’Œ<code class="fe nh ni nj ma b">LogisticRegression</code>çš„æ€§èƒ½ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="144d" class="me mf iq ma b gy mg mh l mi mj">mnb = MultinomialNB()<br/>logreg = LogisticRegression()</span></pre><h1 id="90f9" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">è®¡æ•°çŸ¢é‡å™¨</h1><p id="84cc" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">ä¸ºäº†åœ¨åˆ†ç±»å™¨ä¸­ä½¿ç”¨å•è¯ï¼Œæˆ‘ä»¬éœ€è¦å°†å•è¯è½¬æ¢æˆæ•°å­—ã€‚Sklearn çš„<code class="fe nh ni nj ma b">CountVectorizer</code>è·å–æ‰€æœ‰æ¨æ–‡ä¸­çš„æ‰€æœ‰å•è¯ï¼Œåˆ†é…ä¸€ä¸ª IDï¼Œå¹¶ç»Ÿè®¡æ¯ä¸ªæ¨æ–‡ä¸­è¯¥å•è¯çš„å‡ºç°é¢‘ç‡ã€‚ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªå•è¯åŒ…ä½œä¸ºåˆ†ç±»å™¨çš„è¾“å…¥ã€‚è¿™ä¸€è¢‹å•è¯æ˜¯ä¸€ä¸ªç¨€ç–çš„æ•°æ®é›†ã€‚è¿™æ„å‘³ç€æ¯æ¡è®°å½•éƒ½å°†æœ‰è®¸å¤šé›¶ï¼Œä»£è¡¨æ²¡æœ‰åœ¨ tweet ä¸­å‡ºç°çš„å•è¯ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="4293" class="me mf iq ma b gy mg mh l mi mj">countvect = CountVectorizer()</span><span id="ee3b" class="me mf iq ma b gy mk mh l mi mj"># MultinomialNB<br/>best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)<br/>joblib.dump(best_mnb_countvect, '../output/best_mnb_countvect.pkl')</span><span id="36f7" class="me mf iq ma b gy mk mh l mi mj"># LogisticRegression<br/>best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)<br/>joblib.dump(best_logreg_countvect, '../output/best_logreg_countvect.pkl')</span></pre><h1 id="2e9f" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">TF-IDF çŸ¢é‡å™¨</h1><p id="9c99" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">CountVectorizer çš„ä¸€ä¸ªé—®é¢˜æ˜¯å¯èƒ½ä¼šæœ‰é¢‘ç¹å‡ºç°çš„å•è¯ã€‚è¿™äº›è¯å¯èƒ½æ²¡æœ‰æ­§è§†æ€§ä¿¡æ¯ã€‚å› æ­¤å®ƒä»¬å¯ä»¥è¢«ç§»é™¤ã€‚<a class="ae kv" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> TF-IDF(è¯é¢‘â€”é€†æ–‡æ¡£é¢‘ç‡)</a>å¯ä»¥ç”¨æ¥å¯¹è¿™äº›é¢‘ç¹å‡ºç°çš„è¯è¿›è¡Œé™æƒã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="fb8f" class="me mf iq ma b gy mg mh l mi mj">tfidfvect = TfidfVectorizer()</span><span id="81cd" class="me mf iq ma b gy mk mh l mi mj"># MultinomialNB<br/>best_mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)<br/>joblib.dump(best_mnb_tfidf, '../output/best_mnb_tfidf.pkl')</span><span id="03f8" class="me mf iq ma b gy mk mh l mi mj"># LogisticRegression<br/>best_logreg_tfidf = grid_vect(logreg, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)<br/>joblib.dump(best_logreg_tfidf, '../output/best_logreg_tfidf.pkl')</span></pre><h1 id="dbb9" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">Word2Vec</h1><p id="9396" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">å°†å•è¯è½¬æ¢æˆæ•°å€¼çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨<code class="fe nh ni nj ma b">Word2Vec</code>ã€‚Word2Vec å°†æ¯ä¸ªå•è¯æ˜ å°„åˆ°å¤šç»´ç©ºé—´ä¸­ã€‚å®ƒé€šè¿‡è€ƒè™‘ä¸€ä¸ªè¯åœ¨æ¨æ–‡ä¸­å‡ºç°çš„ä¸Šä¸‹æ–‡æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚ç»“æœï¼Œç›¸ä¼¼çš„å•è¯åœ¨å¤šç»´ç©ºé—´ä¸­ä¹Ÿå½¼æ­¤æ¥è¿‘ã€‚</p><p id="7ddd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec ç®—æ³•æ˜¯ gensim åŒ…çš„ä¸€éƒ¨åˆ†ã€‚</p><p id="6307" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec ç®—æ³•ä½¿ç”¨å•è¯åˆ—è¡¨ä½œä¸ºè¾“å…¥ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†<code class="fe nh ni nj ma b">nltk</code>åŒ…çš„<code class="fe nh ni nj ma b">word_tokenize</code>æ–¹æ³•ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="765d" class="me mf iq ma b gy mg mh l mi mj">SIZE = 50</span><span id="10ac" class="me mf iq ma b gy mk mh l mi mj">X_train['clean_text_wordlist'] = X_train.clean_text.apply(lambda x : word_tokenize(x))<br/>X_test['clean_text_wordlist'] = X_test.clean_text.apply(lambda x : word_tokenize(x))</span><span id="581e" class="me mf iq ma b gy mk mh l mi mj">model = gensim.models.Word2Vec(X_train.clean_text_wordlist<br/>, min_count=1<br/>, size=SIZE<br/>, window=5<br/>, workers=4)</span><span id="fb55" class="me mf iq ma b gy mk mh l mi mj">model.most_similar('plane', topn=3)</span></pre><p id="08d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Word2Vec æ¨¡å‹æä¾›äº†æ‰€æœ‰ tweets ä¸­çš„è¯æ±‡ã€‚å¯¹äºæ¯ä¸ªå•è¯ï¼Œä½ ä¹Ÿæœ‰å®ƒçš„å‘é‡å€¼ã€‚å‘é‡å€¼çš„æ•°é‡ç­‰äºæ‰€é€‰çš„å¤§å°ã€‚è¿™äº›æ˜¯æ¯ä¸ªå•è¯åœ¨å¤šç»´ç©ºé—´ä¸­æ˜ å°„çš„ç»´åº¦ã€‚å‡ºç°æ¬¡æ•°å°‘äº<code class="fe nh ni nj ma b">min_count</code>çš„å•è¯ä¸ä¼šä¿ç•™åœ¨è¯æ±‡è¡¨ä¸­ã€‚</p><p id="e2d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">min_count å‚æ•°çš„ä¸€ä¸ªå‰¯ä½œç”¨æ˜¯ä¸€äº› tweets å¯èƒ½æ²¡æœ‰å‘é‡å€¼ã€‚å½“ tweet ä¸­çš„å•è¯åœ¨å°‘äº min_count <em class="oo"> </em>çš„ tweet ä¸­å‡ºç°æ—¶ï¼Œå°±ä¼šå‡ºç°è¿™ç§æƒ…å†µã€‚ç”±äº tweets çš„è¯­æ–™åº“å¾ˆå°ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æœ‰å‘ç”Ÿè¿™ç§æƒ…å†µçš„é£é™©ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°† min_count å€¼è®¾ç½®ä¸º 1ã€‚</p><p id="76bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æ¨æ–‡å¯ä»¥æœ‰ä¸åŒæ•°é‡çš„å‘é‡ï¼Œè¿™å–å†³äºå®ƒåŒ…å«çš„å­—æ•°ã€‚ä¸ºäº†ä½¿ç”¨è¿™ä¸ªè¾“å‡ºè¿›è¡Œå»ºæ¨¡ï¼Œæˆ‘ä»¬å°†è®¡ç®—æ¯æ¡ tweet çš„æ‰€æœ‰å‘é‡çš„å¹³å‡å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰ç›¸åŒæ•°é‡(å³å¤§å°)çš„è¾“å…¥å˜é‡ã€‚</p><p id="4624" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æˆ‘ä»¬ç”¨å‡½æ•°<code class="fe nh ni nj ma b">compute_avg_w2v_vector</code>æ¥åšè¿™ä»¶äº‹ã€‚åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬è¿˜æ£€æŸ¥ tweet ä¸­çš„å•è¯æ˜¯å¦å‡ºç°åœ¨ Word2Vec æ¨¡å‹çš„è¯æ±‡è¡¨ä¸­ã€‚å¦‚æœä¸æ˜¯ï¼Œåˆ™è¿”å›ä¸€ä¸ªç”¨ 0.0 å¡«å……çš„åˆ—è¡¨ã€‚å¦åˆ™æ˜¯å•è¯å‘é‡çš„å¹³å‡å€¼ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="1359" class="me mf iq ma b gy mg mh l mi mj">def compute_avg_w2v_vector(w2v_dict, tweet):<br/>    list_of_word_vectors = [w2v_dict[w] for w in tweet if w in w2v_dict.vocab.keys()]<br/>    <br/>    if len(list_of_word_vectors) == 0:<br/>        result = [0.0]*SIZE<br/>    else:<br/>        result = np.sum(list_of_word_vectors, axis=0) / len(list_of_word_vectors)<br/>        <br/>    return result</span><span id="527c" class="me mf iq ma b gy mk mh l mi mj">X_train_w2v = X_train['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))<br/>X_test_w2v = X_test['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))</span></pre><p id="ce33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">è¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ªå‘é‡ç»´æ•°ç­‰äº<code class="fe nh ni nj ma b">SIZE</code>çš„åºåˆ—ã€‚ç°åœ¨æˆ‘ä»¬å°†åˆ†å‰²è¿™ä¸ªå‘é‡å¹¶åˆ›å»ºä¸€ä¸ªæ•°æ®å¸§ï¼Œæ¯ä¸ªå‘é‡å€¼åœ¨å•ç‹¬çš„åˆ—ä¸­ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°† Word2Vec å˜é‡è¿æ¥åˆ°å…¶ä»– TextCounts å˜é‡ã€‚æˆ‘ä»¬éœ€è¦é‡ç”¨<code class="fe nh ni nj ma b">X_train</code>å’Œ<code class="fe nh ni nj ma b">X_test</code>çš„ç´¢å¼•ã€‚å¦åˆ™ï¼Œè¿™å°†åœ¨ä»¥åçš„è¿æ¥ä¸­äº§ç”Ÿé—®é¢˜(é‡å¤)ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="c8f4" class="me mf iq ma b gy mg mh l mi mj">X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index= X_train.index)<br/>X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index= X_test.index)</span><span id="b4da" class="me mf iq ma b gy mk mh l mi mj"># Concatenate with the TextCounts variables<br/>X_train_w2v = pd.concat([X_train_w2v, X_train.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)<br/>X_test_w2v = pd.concat([X_test_w2v, X_test.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)</span></pre><p id="c548" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æˆ‘ä»¬åªè€ƒè™‘é€»è¾‘å›å½’ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨ Word2Vec å‘é‡ä¸­æœ‰è´Ÿå€¼ã€‚å¤šé¡¹å¼ lNB å‡è®¾å˜é‡å…·æœ‰<a class="ae kv" href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="noopener ugc nofollow" target="_blank">å¤šé¡¹å¼åˆ†å¸ƒ</a>ã€‚å› æ­¤å®ƒä»¬ä¸èƒ½åŒ…å«è´Ÿå€¼ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="f998" class="me mf iq ma b gy mg mh l mi mj">best_logreg_w2v = grid_vect(logreg, parameters_logreg, X_train_w2v, X_test_w2v, is_w2v=True)<br/>joblib.dump(best_logreg_w2v, '../output/best_logreg_w2v.pkl')</span></pre><h1 id="52df" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">ç»“è®º</h1><ul class=""><li id="8f53" class="nw nx iq ky b kz nc lc nd lf ot lj ou ln ov lr ob oc od oe bi translated">å½“ä½¿ç”¨è®¡æ•°çŸ¢é‡å™¨çš„ç‰¹æ€§æ—¶ï¼Œè¿™ä¸¤ç§åˆ†ç±»å™¨éƒ½èƒ½è·å¾—æœ€ä½³ç»“æœ</li><li id="da30" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">é€»è¾‘å›å½’ä¼˜äºå¤šé¡¹å¼æœ´ç´ è´å¶æ–¯åˆ†ç±»å™¨</li><li id="bd1f" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">æµ‹è¯•é›†ä¸Šçš„æœ€ä½³æ€§èƒ½æ¥è‡ªå¸¦æœ‰ CountVectorizer ç‰¹æ€§çš„ LogisticRegressionã€‚</li></ul><p id="0b6d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æœ€ä½³å‚æ•°:</p><ul class=""><li id="6a27" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">c å€¼ä¸º 1</li><li id="9b3d" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">L2 æ­£åˆ™åŒ–</li><li id="10cc" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">max_df: 0.5 æˆ–æœ€å¤§æ–‡æ¡£é¢‘ç‡ 50%ã€‚</li><li id="5e92" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">min_df: 1 æˆ–è€…è¿™äº›è¯éœ€è¦å‡ºç°åœ¨è‡³å°‘ä¸¤æ¡æ¨æ–‡ä¸­</li><li id="f706" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">ngram_range: (1ï¼Œ2)ï¼Œä¸¤ä¸ªå•è¯éƒ½ä½œä¸ºäºŒå…ƒè¯­æ³•ä½¿ç”¨</li></ul><p id="0fd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">è¯„ä¼°æŒ‡æ ‡:</p><ul class=""><li id="707e" class="nw nx iq ky b kz la lc ld lf ny lj nz ln oa lr ob oc od oe bi translated">æµ‹è¯•å‡†ç¡®ç‡ä¸º 81.3%ã€‚è¿™ä¼˜äºé¢„æµ‹æ‰€æœ‰è§‚å¯Ÿçš„å¤šæ•°ç±»(è¿™é‡Œæ˜¯è´Ÿé¢æƒ…ç»ª)çš„åŸºçº¿æ€§èƒ½ã€‚åŸºçº¿ä¼šç»™å‡º 63%çš„å‡†ç¡®åº¦ã€‚</li><li id="54ad" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">è¿™ä¸‰ä¸ªç±»åˆ«çš„ç²¾åº¦éƒ½ç›¸å½“é«˜ã€‚ä¾‹å¦‚ï¼Œåœ¨æˆ‘ä»¬é¢„æµ‹ä¸ºè´Ÿé¢çš„æ‰€æœ‰æ¡ˆä¾‹ä¸­ï¼Œ80%æ˜¯è´Ÿé¢çš„ã€‚</li><li id="ddba" class="nw nx iq ky b kz of lc og lf oh lj oi ln oj lr ob oc od oe bi translated">ä¸­æ€§ç±»çš„å¬å›ç‡å¾ˆä½ã€‚åœ¨æˆ‘ä»¬æµ‹è¯•æ•°æ®çš„æ‰€æœ‰ä¸­æ€§æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åªé¢„æµ‹ 48%æ˜¯ä¸­æ€§çš„ã€‚</li></ul><h1 id="ffbf" class="ml mf iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">å¯¹æ–°æ¨æ–‡åº”ç”¨æœ€ä½³æ¨¡å‹</h1><p id="7049" class="pw-post-body-paragraph kw kx iq ky b kz nc jr lb lc nd ju le lf ne lh li lj nf ll lm ln ng lp lq lr ij bi translated">ä¸ºäº†å¥½ç©ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æœ€ä½³æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºä¸€äº›åŒ…å«â€œ@VirginAmericaâ€çš„æ–°æ¨æ–‡ã€‚æˆ‘æ‰‹åŠ¨é€‰æ‹©äº† 3 æ¡è´Ÿé¢å’Œ 3 æ¡æ­£é¢çš„æ¨æ–‡ã€‚</p><p id="5625" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">å¤šäºäº† GridSearchCVï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“äº†ä»€ä¹ˆæ˜¯æœ€å¥½çš„è¶…å‚æ•°ã€‚å› æ­¤ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥åœ¨æ‰€æœ‰è®­ç»ƒæ•°æ®ä¸Šè®­ç»ƒæœ€ä½³æ¨¡å‹ï¼ŒåŒ…æ‹¬æˆ‘ä»¬ä¹‹å‰åˆ†ç¦»çš„æµ‹è¯•æ•°æ®ã€‚</p><pre class="kg kh ki kj gt lz ma mb mc aw md bi"><span id="797d" class="me mf iq ma b gy mg mh l mi mj">textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'<br/>,'count_mentions','count_urls','count_words']</span><span id="edd9" class="me mf iq ma b gy mk mh l mi mj">features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))<br/>, ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))<br/>, ('vect', CountVectorizer(max_df=0.5, min_df=1, ngram_range=(1,2)))]))]<br/>, n_jobs=-1)</span><span id="b5e7" class="me mf iq ma b gy mk mh l mi mj">pipeline = Pipeline([<br/>('features', features)<br/>, ('clf', LogisticRegression(C=1.0, penalty='l2'))<br/>])</span><span id="9b6c" class="me mf iq ma b gy mk mh l mi mj">best_model = pipeline.fit(df_model.drop('airline_sentiment', axis=1), df_model.airline_sentiment)</span><span id="123a" class="me mf iq ma b gy mk mh l mi mj"># Applying on new positive tweets<br/>new_positive_tweets = pd.Series(["Thank you @VirginAmerica for you amazing customer support team on Tuesday 11/28 at @EWRairport and returning my lost bag in less than 24h! #efficiencyiskey #virginamerica"<br/>,"Love flying with you guys ask these years. Sad that this will be the last trip ğŸ˜‚ @VirginAmerica #LuxuryTravel"<br/>,"Wow @VirginAmerica main cabin select is the way to fly!! This plane is nice and clean &amp; I have tons of legroom! Wahoo! NYC bound! âœˆï¸"])</span><span id="96f2" class="me mf iq ma b gy mk mh l mi mj">df_counts_pos = tc.transform(new_positive_tweets)<br/>df_clean_pos = ct.transform(new_positive_tweets)<br/>df_model_pos = df_counts_pos<br/>df_model_pos['clean_text'] = df_clean_pos</span><span id="25c0" class="me mf iq ma b gy mk mh l mi mj">best_model.predict(df_model_pos).tolist()</span><span id="db14" class="me mf iq ma b gy mk mh l mi mj"># Applying on new negative tweets<br/>new_negative_tweets = pd.Series(["@VirginAmerica shocked my initially with the service, but then went on to shock me further with no response to what my complaint was. #unacceptable @Delta @richardbranson"<br/>,"@VirginAmerica this morning I was forced to repack a suitcase w a medical device because it was barely overweight - wasn't even given an option to pay extra. My spouses suitcase then burst at the seam with the added device and had to be taped shut. Awful experience so far!"<br/>,"Board airplane home. Computer issue. Get off plane, traverse airport to gate on opp side. Get on new plane hour later. Plane too heavy. 8 volunteers get off plane. Ohhh the adventure of travel âœˆï¸ @VirginAmerica"])</span><span id="45ef" class="me mf iq ma b gy mk mh l mi mj">df_counts_neg = tc.transform(new_negative_tweets)<br/>df_clean_neg = ct.transform(new_negative_tweets)<br/>df_model_neg = df_counts_neg<br/>df_model_neg['clean_text'] = df_clean_neg</span><span id="b973" class="me mf iq ma b gy mk mh l mi mj">best_model.predict(df_model_neg).tolist()</span></pre><p id="fd13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">è¯¥æ¨¡å‹å¯¹æ‰€æœ‰æ¨æ–‡è¿›è¡Œäº†æ­£ç¡®åˆ†ç±»ã€‚åº”è¯¥ä½¿ç”¨æ›´å¤§çš„æµ‹è¯•é›†æ¥è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ä½†æ˜¯åœ¨è¿™ä¸ªå°æ•°æ®é›†ä¸Šï¼Œå®ƒåšäº†æˆ‘ä»¬æƒ³è¦åšçš„äº‹æƒ…ã€‚</p><p id="f933" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">æˆ‘å¸Œæœ›ä½ å–œæ¬¢è¯»è¿™ä¸ªæ•…äº‹ã€‚å¦‚æœä½ åšåˆ°äº†ï¼Œè¯·é¼“æŒã€‚</p></div></div>    
</body>
</html>