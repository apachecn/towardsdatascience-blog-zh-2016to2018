# Apache Spark å¤§æ•°æ®ä¹‹æ—…:ç¬¬ 1 éƒ¨åˆ†

> åŸæ–‡ï¼š<https://towardsdatascience.com/a-journey-into-big-data-with-apache-spark-part-1-5dfcc2bccdd2?source=collection_archive---------3----------------------->

## å…³äºäº†è§£ Apache Spark ç”¨äºå¤§æ•°æ®å¤„ç†çš„ç³»åˆ—æ–‡ç« çš„ç¬¬ä¸€ç¯‡ã€‚

æˆ‘çš„å¤§æ•°æ®ä¹‹æ—…å§‹äº 2018 å¹´ 5 æœˆã€‚åå¤šå¹´æ¥ï¼Œæˆ‘ä¸€ç›´æ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œå‚ä¸å¹¶é¢†å¯¼äº†ä¸€äº› Sky Betting & Gaming æœ€å¤§çš„äº§å“å’ŒæœåŠ¡çš„å¼€å‘ã€‚åœ¨æ­¤æœŸé—´ï¼Œæˆ‘å­¦åˆ°äº†å¾ˆå¤šå…³äºå¦‚ä½•æ„å»ºå’Œæ“ä½œå¯æ‰©å±•åº”ç”¨ç¨‹åºçš„çŸ¥è¯†â€”â€”ä»å¯¹æ—¥å¿—å’ŒæŒ‡æ ‡çš„éœ€æ±‚ï¼Œåˆ°æ€§èƒ½ä¼˜åŒ–å’Œå¯ç»´æŠ¤æ€§ã€‚å½“æˆ‘è¢«è¦æ±‚æˆä¸ºæˆ‘ä»¬æ•°æ®éƒ¨è½çš„å·¥ç¨‹ç»ç†æ—¶ï¼Œæˆ‘æ—¢å…´å¥‹åˆæåº¦ç´§å¼ â€”â€”æˆ‘ä»¥å‰é¢†å¯¼è¿‡å°å›¢é˜Ÿï¼Œä½†å¯¹å¤§æ•°æ®ä¸€æ— æ‰€çŸ¥ã€‚æ‰€ä»¥æˆ‘å†³å®šå­¦ï¼

â€œä½ ä»å“ªé‡Œå¼€å§‹åšè¿™æ ·çš„äº‹æƒ…ï¼Ÿâ€è¿™æ˜¯æˆ‘ç»å¸¸é—®è‡ªå·±çš„é—®é¢˜ã€‚å¤§æ•°æ®ç”Ÿæ€ç³»ç»Ÿéå¸¸åºå¤§ã€‚æ‰€ä»¥æˆ‘é—®æˆ‘å›¢é˜Ÿä¸­çš„ä¸€ä½é¦–å¸­å·¥ç¨‹å¸ˆä»–ä»¬ä¼šæ¨èä»€ä¹ˆï¼ŒApache Spark æ˜¯ä»–ä»¬çš„é¦–é€‰ã€‚

## ä»€ä¹ˆæ˜¯ç«èŠ±ï¼Ÿ

ä» Apache Spark ç½‘ç«™:

> **Apache Spark**æ˜¯ç”¨äºå¤§è§„æ¨¡æ•°æ®å¤„ç†çš„ç»Ÿä¸€åˆ†æå¼•æ“ã€‚

ç”¨äººç±»çš„è¯æ¥è¯´ï¼ŒSpark æ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œå½“æŸ¥è¯¢å†…å­˜æ•°æ®é›†æ—¶ï¼Œå®ƒæœ‰ä¸€ä¸ªè·¨å¤šç§è¯­è¨€çš„é€šç”¨æ¥å£ï¼Œç”¨äº SQL å’Œ MapReduce ä¹‹ç±»çš„ä¸œè¥¿ã€‚ç”±äºæ˜¯åˆ†å¸ƒå¼çš„ï¼ŒSpark å¯ä»¥åœ¨ä¸€ä¸ªæœºå™¨é›†ç¾¤ä¸Šå¤„ç†éå¸¸å¤§çš„æ•°æ®é›†ã€‚

## ä¸ºä»€ä¹ˆæ˜¯ç«èŠ±ï¼Ÿ

ä»å·¥ç¨‹è§’åº¦æ¥çœ‹ï¼ŒSpark æœ‰å¤šç§è¯­è¨€ç‰ˆæœ¬:Scalaã€Javaã€Python å’Œ rã€‚è¿™çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„èƒœåˆ©ï¼Œå› ä¸ºå®ƒæ²¡æœ‰é™åˆ¶æˆ‘ä»¬å¯ä»¥ç§Ÿç”¨ç‰¹å®šè¯­è¨€çš„å†…å®¹ã€‚Spark æœ‰è‡ªå·±çš„ DSL ( [é¢†åŸŸç‰¹å®šè¯­è¨€](https://en.wikipedia.org/wiki/Domain-specific_language))ï¼Œè¿™åœ¨æ‰€æœ‰å®ç°ä¸­éƒ½æ˜¯ç›¸åŒçš„ï¼Œè¿™æ„å‘³ç€å°½ç®¡é€‰æ‹©äº†ä¸åŒçš„å®ç°è¯­è¨€ï¼Œä½†è¿˜æ˜¯æœ‰ä¸€ç§é€šç”¨è¯­è¨€ã€‚æˆ‘ä»¬çš„æ•°æ®ç§‘å­¦å›¢é˜Ÿä½¿ç”¨ PySpark å’Œ Spark çš„ç»„åˆï¼Œæ›´å¹¿æ³›çš„å›¢é˜Ÿä½¿ç”¨ Scalaâ€”â€”æˆ‘ä»¬è®©å›¢é˜Ÿ(å¤§éƒ¨åˆ†)é€‰æ‹©ä»–ä»¬è‡ªå·±çš„å·¥å…·ã€‚

Spark å¯¹æ•°æ®æ¡†æ¶çš„ä½¿ç”¨éå¸¸é€‚åˆé€šå¸¸çš„è½¯ä»¶å·¥ç¨‹å’Œåº”ç”¨ç¨‹åºè®¾è®¡åŸåˆ™ï¼Œä¾‹å¦‚å•å…ƒæµ‹è¯•ã€æ•°æ®å»ºæ¨¡ã€å•ä¸€è´£ä»»åŸåˆ™ç­‰ç­‰ã€‚åˆä¸€æ¬¡å¤§è·å…¨èƒœã€‚æˆ‘å¯ä»¥åƒä»¥å‰ä¸€æ ·ç»§ç»­æ„å»ºåº”ç”¨ç¨‹åº(æˆ‘æ˜¯[12 å› å­](https://12factor.net)åŸåˆ™çš„å¿ å®ç²‰ä¸)ã€‚

Spark è¿˜é™„å¸¦äº†ä¸€ä¸ª SQL æ¥å£ï¼Œå¯¹äºå¤§å¤šæ•°æ›¾ç»éœ€è¦åœ¨æŸä¸ªåœ°æ–¹å­˜å‚¨å’ŒæŸ¥è¯¢æ•°æ®çš„ç¨‹åºå‘˜/å·¥ç¨‹å¸ˆæ¥è¯´ï¼Œè¿™é€šå¸¸æ˜¯å¾ˆç†Ÿæ‚‰çš„ã€‚

å¦‚æœä½ æ„Ÿå…´è¶£çš„è¯ï¼ŒSpark ç”šè‡³æœ‰ä¸€ä¸ªæµåª’ä½“åº“ã€‚è™½ç„¶æˆ‘ä¸ä¼šé©¬ä¸Šè¿™ä¹ˆåš(è™½ç„¶æˆ‘æ˜¯ Apache Kafka çš„è¶…çº§ç²‰ä¸)ï¼Œä½†å®ƒå¯¹æœªæ¥çš„å­¦ä¹ (æˆ‘æƒ³ä¹Ÿæ˜¯å•†ä¸šç”¨ä¾‹)å¾ˆæœ‰å¸å¼•åŠ›ã€‚

## æˆ‘ä»¬å¼€å§‹å§

Apache Spark ä¼¼ä¹ä»¤äººæœ›è€Œç”Ÿç•(å°±åƒä»»ä½•æ–°äº‹ç‰©ä¸€æ ·)ï¼Œæˆ‘å‘ç°è‡ªå·±åˆåœ¨é—®*â€œæˆ‘ä»å“ªé‡Œå¼€å§‹ï¼Ÿâ€æˆ‘åšä¿¡ä»å¤´å¼€å§‹ç†è§£äº‹ç‰©ä¼šè®©ä½ æˆä¸ºæ›´å¥½çš„å·¥ç¨‹å¸ˆã€‚è¿™ä¸€ç‚¹ï¼ŒåŠ ä¸Šåœ¨æŸä¸ªåœ°æ–¹å®é™…ç»ƒä¹ ä½¿ç”¨ Spark çš„éœ€è¦ï¼Œå¼•å¯¼æˆ‘æ„å»ºä¸€ä¸ªå°å‹æœ¬åœ° Spark ç‹¬ç«‹é›†ç¾¤ã€‚*

*æ³¨:æˆ‘æ˜¯ Docker çš„å¿ å®ç²‰ä¸ã€‚å®ƒä¸ºä½ æä¾›äº†ä¸€ç§æ‰“åŒ…æ–¹å¼ï¼Œå¹¶ä»¥ä¸€ç§éå¸¸ä¾¿æºçš„æ–¹å¼åˆ†å‘ã€‚æˆ‘éœ€è¦åœ¨è‡ªå·±çš„æœºå™¨ä¸Šå®‰è£…çš„æƒŸä¸€ä¾èµ–é¡¹æ˜¯ Docker æœ¬èº«ã€‚æˆ‘å°†åœ¨æˆ‘æ‰€æœ‰çš„ä¾‹å­ä¸­ä½¿ç”¨ Dockerï¼Œæ‰€ä»¥ä½ å¯èƒ½æƒ³ä»è¿™é‡Œçš„*[](https://www.docker.com/get-started)**ä¸­å¾—åˆ°å®ƒã€‚æˆ‘è¿˜åœ¨è¿è¡Œ OSXï¼Œå®ƒå¯¹ Docker æœ‰ä¸€äº›å°çš„é™åˆ¶ï¼Œä½†æˆ‘ä»¬å¯ä»¥è§£å†³å®ƒä»¬ã€‚**

*æˆ‘ä»¬çš„é¦–è¦ä»»åŠ¡æ˜¯æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„åŒ…å« Java ç‰ˆæœ¬çš„ Docker æ˜ åƒã€‚ä½œä¸ºå¼€æºè½¯ä»¶çš„ç²‰ä¸ï¼Œæˆ‘é€‰æ‹©äº†ä½¿ç”¨ OpenJDKã€‚æˆ‘ä¸æ˜¯ä¸€ä¸ªè†¨èƒ€çš„ç²‰ä¸ï¼Œå¹¶å†³å®š Alpine Linux æ˜¯æ­£ç¡®çš„é€‰æ‹©ã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å·²ç»å¯ç”¨çš„å›¾ç‰‡ã€‚è®©æˆ‘ä»¬å¼€å§‹åˆ›å»ºæˆ‘ä»¬çš„*docker æ–‡ä»¶ã€‚**

*åœ¨ä¸€ä¸ªåä¸º *Dockerfile* çš„ç©ºç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–°æ–‡ä»¶ã€‚åœ¨é¡¶éƒ¨æ·»åŠ ä»¥ä¸‹ä¸€è¡Œ:*

```
*FROM openjdk:8-alpine*
```

*ç°åœ¨æˆ‘ä»¬å¯ä»¥æ„å»ºæˆ‘ä»¬çš„æ˜ åƒï¼Œå®ƒä½¿ç”¨ OpenJDK æ˜ åƒä½œä¸ºåŸºç¡€ã€‚é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ„å»ºæ˜ åƒ:*

```
*docker build .*
```

*è¿™å°†æå–åŸºç¡€å›¾åƒå¹¶åˆ›å»ºæˆ‘ä»¬è‡ªå·±çš„å›¾åƒç‰ˆæœ¬ã€‚æ‚¨å°†çœ‹åˆ°ç±»ä¼¼äºä»¥ä¸‹è¾“å‡ºçš„å†…å®¹:*

```
*Successfully built 68ddc890acca*
```

*æˆ‘ä»¬ä¸å†åœ¨æ¯æ¬¡æ„å»ºå›¾åƒæ—¶è®°å½•è¾“å‡ºæ•£åˆ—ï¼Œè¿™ä¸€æ¬¡æˆ‘ä»¬å°†*æ ‡è®°*å›¾åƒï¼Œç»™å®ƒå‘½åä»¥ä¾¿æ›´å®¹æ˜“ä½¿ç”¨(å½“ç„¶ï¼Œç”¨æ‚¨è‡ªå·±çš„åå­—æ›¿æ¢$MYNAME)ã€‚è¿™æ˜¯å¯¹ä¸Šé¢çš„æ„å»ºå‘½ä»¤çš„ä¸€ä¸ªå°å°çš„è°ƒæ•´:*

```
*docker build -t $MYNAME/spark:latest .*
```

*ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬ç»™å®ƒçš„æ ‡ç­¾æ¥å¼•ç”¨å®ƒï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç”Ÿæˆçš„æ•£åˆ—æ¥å¼•ç”¨æ„å»ºçš„æ˜ åƒã€‚é€šè¿‡ä½¿ç”¨æ ‡ç­¾åè¿è¡Œå®¹å™¨è¿›è¡Œæµ‹è¯•ï¼Œå¦‚ä¸‹æ‰€ç¤º:*

```
*docker run -it --rm $MYNAME/spark:latest /bin/sh*
```

*è¿™ä¸ªå›¾åƒè¿˜æ²¡æœ‰å¤šå¤§ç”¨å¤„ï¼Œå› ä¸ºå®ƒåªåŒ…å« Javaã€‚è®©æˆ‘ä»¬å®‰è£…ä¸€äº›å®ç”¨ç¨‹åºï¼Œæˆ‘ä»¬å°†éœ€è¦ä¸‹è½½å’Œå®‰è£… Sparkã€‚æˆ‘ä»¬å°†éœ€è¦`wget`æ¥ä¸‹è½½åŒ…å« Spark çš„å½’æ¡£æ–‡ä»¶ï¼Œå¹¶éœ€è¦`tar`æ¥ä»è¯¥å½’æ¡£æ–‡ä»¶ä¸­æå–æ–‡ä»¶ã€‚æˆ‘ä»¬è¿˜éœ€è¦`bash`æ¥ç®¡ç†ä¸€äº›äº‹æƒ…ã€‚åœ¨ *Dockerfile* ä¸­çš„æ–°è¡Œä¸Šï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹(æˆ‘ä»¬éœ€è¦åŒ…å«`--update`ï¼Œè¿™æ · Alpine å°±æœ‰äº†ä¸€ä¸ªæ–°çš„å­˜å‚¨åº“åˆ—è¡¨ï¼Œå¯ä»¥ä»ä¸­ä¸‹è½½äºŒè¿›åˆ¶æ–‡ä»¶):*

```
*RUN apk --update add wget tar bash*
```

*ç°åœ¨é‡å»ºå›¾åƒå¹¶è§‚å¯Ÿ`wget`ã€`tar`å’Œ`bash`çš„å®‰è£…ã€‚*

*æˆ‘ä»¬ç°åœ¨å¯ä»¥ä¸‹è½½å¹¶å®‰è£… Sparkã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŸºäº Scala 2.11 å’Œ Hadoop 2.7 çš„ Spark (2.4.0)çš„æœ€æ–°ç‰ˆæœ¬(ä¸è¦æ‹…å¿ƒ Hadoopï¼Œæˆ‘ä»¬å¯¹æ­¤ä¸æ„Ÿå…´è¶£â€¦è¿˜æ²¡æœ‰)ã€‚å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„*docker æ–‡ä»¶*ä¸­çš„æ–°è¡Œï¼Œå¹¶è¿›è¡Œç¼–è¯‘:*

```
*RUN wget [http://apache.mirror.anlx.net/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz](http://apache.mirror.anlx.net/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz)*
```

*Docker éå¸¸èªæ˜ï¼Œå®ƒå°†é‡ç”¨æˆ‘ä»¬ä¹‹å‰æ„å»ºçš„*å±‚*ï¼Œæ‰€ä»¥ç°åœ¨å®ƒæ‰€è¦åšçš„å°±æ˜¯ä¸‹è½½ Spark å½’æ¡£æ–‡ä»¶ã€‚ä¸€æ—¦æ„å»ºå®Œæˆï¼Œæ˜ åƒå°†åŒ…å« Spark å½’æ¡£æ–‡ä»¶ï¼Œä¾›æˆ‘ä»¬å®‰è£…ã€‚å®‰è£…å°±æ˜¯ç®€å•åœ°ä»æ¡£æ¡ˆä¸­æå– Spark å¹¶æŠŠå®ƒæ”¾åœ¨ä¸€ä¸ªåˆé€‚çš„åœ°æ–¹ã€‚å°†å¦ä¸€è¡Œæ·»åŠ åˆ° *Dockerfile* ä¸­ï¼Œè¿™æ ·åš(ä¸ºäº†å¯è¯»æ€§ï¼Œæˆ‘å°†å‘½ä»¤åˆ†æˆå¤šè¡Œã€‚`rm`æ˜¯ç®€å•åœ°æ¸…ç†ä¸‹è½½çš„å½’æ¡£æ–‡ä»¶)ç„¶åæ„å»º:*

```
*RUN tar -xzf spark-2.4.0-bin-hadoop2.7.tgz && \
    mv spark-2.4.0-bin-hadoop2.7 /spark && \
    rm spark-2.4.0-bin-hadoop2.7.tgz*
```

*æ­å–œä½ ï¼ç°åœ¨ï¼Œæ‚¨å·²ç»åœ¨ Docker æ˜ åƒä¸Šä¸‹è½½å¹¶å®‰è£…äº† Sparkã€‚æ˜¯æ—¶å€™æµ‹è¯•ä¸€ä¸‹äº†ï¼è®©æˆ‘ä»¬å¼€å§‹ä¸€ä¸ªå®¹å™¨ï¼Œå¹¶å¾—åˆ°ä¸€ä¸ªå¤–å£³ã€‚æ³¨æ„æ„å»ºå›¾åƒç»“æŸæ—¶çš„ ID è¾“å‡ºï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨å°†ä½¿ç”¨å®ƒã€‚åœ¨æ‚¨çš„ shell ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤:*

```
*docker run --rm -it $MYNAME/spark:latest /bin/sh*
```

*ç„¶åæˆ‘ä»¬å°†åœ¨å®¹å™¨å†…éƒ¨è¿è¡Œçš„å¤–å£³ä¸­ç»“æŸï¼Œç”¨å®ƒæ¥å¯åŠ¨ Spark *Master* ã€‚å½“å¼€å§‹æˆåŠŸå¯åŠ¨æ—¶ï¼ŒSpark éœ€è¦å‡ ä¸ªé€‰é¡¹ã€‚è¿™äº›æ˜¯ä¸»æœåŠ¡å™¨ç›‘å¬çš„ç«¯å£ã€WebUI çš„ç«¯å£å’Œä¸»æœåŠ¡å™¨çš„ä¸»æœºå:*

```
*/spark/bin/spark-class org.apache.spark.deploy.master.Master --ip `hostname` --port 7077 --webui-port 8080*
```

*å¸Œæœ›æ‚¨åº”è¯¥çœ‹åˆ°ä¸€å †æ—¥å¿—è¾“å‡ºï¼å¦‚æœæœ‰ï¼Œæ­å–œä½ ï¼æ‚¨å·²ç»æˆåŠŸå¼€å§‹è¿è¡Œ Spark Masterã€‚*

*ä¸‹ä¸€æ­¥æ˜¯å°†ä¸€äº› Worker æ·»åŠ åˆ°é›†ç¾¤ä¸­ï¼Œä½†æ˜¯é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åœ¨ Master ä¸Šè®¾ç½®ä¸€äº›é…ç½®ï¼Œä»¥ä¾¿ Worker å¯ä»¥ä¸å®ƒå¯¹è¯ã€‚ä¸ºäº†ä½¿äº‹æƒ…å˜å¾—ç®€å•ï¼Œæˆ‘ä»¬å°†ä¸º Master æä¾›ä¸€ä¸ªåˆé€‚çš„åç§°ï¼Œå¹¶å…¬å¼€ Master ç«¯å£(æœ€åä¸€ä¸ªå‘½ä»¤ä¸­çš„`--port`é€‰é¡¹),åŒæ—¶ä½¿ WebUI å¯¹æˆ‘ä»¬å¯ç”¨ã€‚é€šè¿‡ä½¿ç”¨ CTRL+C å’Œ CTRL+D åœæ­¢ä¸» shell å¹¶é€€å‡ºå®¹å™¨ã€‚ç°åœ¨æ‚¨åº”è¯¥åœ¨æœ¬åœ° shell ä¸­äº†ã€‚åªéœ€è°ƒæ•´`docker run`å‘½ä»¤ï¼Œæ·»åŠ `--name`ã€`--hostname`å’Œ`-p`é€‰é¡¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼Œç„¶åè¿è¡Œ:*

```
*docker run --rm -it --name spark-master --hostname spark-master \
    -p 7077:7077 -p 8080:8080 $MYNAME/spark:latest /bin/sh*
```

*è¿è¡Œ`docker ps`,æ‚¨åº”è¯¥çœ‹åˆ°å®¹å™¨è¿è¡Œæ—¶çš„è¾“å‡ºå¦‚ä¸‹(æˆ‘å·²ç»åˆ é™¤äº†ä¸€äº›è¾“å‡ºï¼Œä»¥ä½¿å…¶é€‚åˆä»£ç å—):*

```
*CONTAINER ID  PORTS                                NAMES
3dfc3a95f7f4  ..:7077->7077/tcp, ..:8080->8080/tcp spark-master*
```

*åœ¨å®¹å™¨ä¸­ï¼Œé‡æ–°è¿è¡Œå‘½ä»¤æ¥å¯åŠ¨ Spark Masterï¼Œä¸€æ—¦å¯åŠ¨ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿæµè§ˆåˆ° [http://localhost:8080](http://localhost:8080) å¹¶çœ‹åˆ°é›†ç¾¤çš„ WebUiï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚*

*![](img/e96a7e59fa021deec6a8d41e9392732c.png)*

*Spark Master WebUI*

## *æ·»åŠ å·¥ä½œèŠ‚ç‚¹*

*æ­£å¦‚æˆ‘æåˆ°çš„ï¼Œæˆ‘æ­£åœ¨ä½¿ç”¨ Docker for Macï¼Œè¿™ä½¿å¾— DNS å¾ˆéº»çƒ¦ï¼Œå¹¶ä¸”å¦‚æœä¸è¿è¡Œæœ¬åœ° VPN æœåŠ¡å™¨æˆ–å…¶ä»–ä¸œè¥¿æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡ IP è®¿é—®å®¹å™¨å‡ ä¹æ˜¯ä¸å¯èƒ½çš„â€”â€”è¿™è¶…å‡ºäº†æœ¬æ–‡çš„èŒƒå›´ã€‚å¹¸è¿çš„æ˜¯ï¼ŒDocker æœ‰è‡ªå·±çš„ç½‘ç»œåŠŸèƒ½(å…·ä½“ç»†èŠ‚ä¹Ÿä¸åœ¨æœ¬æ–‡è®¨è®ºèŒƒå›´ä¹‹å†…)ï¼Œæˆ‘ä»¬å°†ç”¨å®ƒæ¥ä¸ºæœ¬åœ°é›†ç¾¤åˆ›å»ºä¸€ä¸ªç½‘ç»œã€‚åˆ›å»ºç½‘ç»œéå¸¸ç®€å•ï¼Œåªéœ€è¿è¡Œä»¥ä¸‹å‘½ä»¤:*

```
*docker network create spark_network*
```

*æˆ‘ä»¬ä¸éœ€è¦æŒ‡å®šä»»ä½•ç‰¹å®šçš„é€‰é¡¹ï¼Œå› ä¸ºé»˜è®¤é€‰é¡¹å¯¹æˆ‘ä»¬çš„ç”¨ä¾‹æ¥è¯´å·²ç»å¾ˆå¥½äº†ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°åˆ›å»ºæˆ‘ä»¬çš„ä¸»æœåŠ¡å™¨ï¼Œä»¥å°†å…¶è¿æ¥åˆ°æ–°ç½‘ç»œã€‚è¿è¡Œ`docker stop spark-master`å’Œ`docker rm spark-master`åˆ é™¤å½“å‰è¿è¡Œä¸»æœºçš„*å®ä¾‹*ã€‚è¦åœ¨æ–°ç½‘ç»œä¸Šé‡æ–°åˆ›å»ºä¸»æœåŠ¡å™¨ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†`--network`é€‰é¡¹æ·»åŠ åˆ°`docker run`ï¼Œå¦‚ä¸‹æ‰€ç¤º:*

```
*docker run --rm -it --name spark-master --hostname spark-master \
    -p 7077:7077 -p 8080:8080 --network spark_network \
    $MYNAME/spark:latest /bin/sh*
```

*è¿™ä¸æˆ‘ä»¬ç¬¬ä¸€æ¬¡è¿è¡Œ Spark Master æ—¶æ²¡æœ‰ä»€ä¹ˆä¸åŒï¼Œåªæ˜¯å®ƒä½¿ç”¨äº†ä¸€ä¸ªæ–°å®šä¹‰çš„ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªç½‘ç»œæ¥è¿æ¥å·¥ä½œäººå‘˜ï¼Œä»¥ä½¿é›†ç¾¤å·¥ä½œğŸ‘ã€‚ç°åœ¨ä¸»èŠ‚ç‚¹å·²ç»å¯åŠ¨å¹¶è¿è¡Œï¼Œè®©æˆ‘ä»¬å‘å®ƒæ·»åŠ ä¸€ä¸ª Worker èŠ‚ç‚¹ã€‚è¿™å°±æ˜¯ Docker çš„é­”åŠ›çœŸæ­£é—ªè€€çš„åœ°æ–¹ã€‚è¦åˆ›å»ºä¸€ä¸ª Worker å¹¶å°†å…¶æ·»åŠ åˆ°é›†ç¾¤ä¸­ï¼Œæˆ‘ä»¬åªéœ€*å¯åŠ¨åŒä¸€ä¸ª docker æ˜ åƒçš„æ–°å®ä¾‹*å¹¶è¿è¡Œå‘½ä»¤æ¥å¯åŠ¨ Workerã€‚æˆ‘ä»¬éœ€è¦ç»™è¿™ä¸ªå·¥ä½œè€…ä¸€ä¸ªæ–°çš„åå­—ï¼Œä½†æ˜¯é™¤äº†è¿™ä¸ªå‘½ä»¤ä¹‹å¤–ï¼Œå…¶ä»–çš„åŸºæœ¬ä¿æŒä¸å˜:*

```
*docker run --rm -it --name spark-worker --hostname spark-worker \
    --network spark_network \
    $MYNAME/spark:latest /bin/sh*
```

*è¦åœ¨å®¹å™¨ä¸Šå¯åŠ¨ Spark Workerï¼Œæˆ‘ä»¬åªéœ€è¿è¡Œ:*

```
*/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \
    --webui-port 8080 spark://spark-master:7077*
```

*å½“å®ƒå¯åŠ¨å¹¶è¿æ¥åˆ°ä¸»æœºæ—¶ï¼Œæ‚¨åº”è¯¥çœ‹åˆ°è¾“å‡ºçš„æœ€åä¸€è¡Œ:*

```
*INFO  Worker:54 - Successfully registered with master spark://spark-master:7077*
```

*ä¸»æœåŠ¡å™¨å°†è¾“å‡ºä»¥ä¸‹è¡Œ:*

```
*INFO  Master:54 - Registering worker 172.21.0.2:37013 with 4 cores, 1024.0 MB RAM*
```

*æ­å–œä½ ï¼æ‚¨å·²ç»ä½¿ç”¨ Docker è®¾ç½®äº†ä¸€ä¸ª Spark **é›†ç¾¤**ï¼*

## *ä½†æ˜¯è¿™æœ‰ç”¨å—ï¼Ÿ*

*ä¸ºäº†æ£€æŸ¥å®ƒæ˜¯å¦å·¥ä½œï¼Œæˆ‘ä»¬å¯ä»¥åŠ è½½ä¸» WebUIï¼Œæˆ‘ä»¬åº”è¯¥çœ‹åˆ° Worker èŠ‚ç‚¹åˆ—åœ¨â€œWorkersâ€éƒ¨åˆ†ä¸‹ï¼Œä½†è¿™å®é™…ä¸Šåªæ˜¯ç¡®è®¤äº†å°† Worker è¿æ¥åˆ°ä¸» WebUI çš„æ—¥å¿—è¾“å‡ºã€‚*

*![](img/2b24d6d0899468392da05be17ab928a8.png)*

*Spark Master WebUI with Worker*

*ä¸ºäº†è¿›è¡ŒçœŸæ­£çš„æµ‹è¯•ï¼Œæˆ‘ä»¬éœ€è¦åœ¨é›†ç¾¤ä¸­å®é™…è¿è¡Œä¸€äº› Spark ä»£ç ã€‚è®©æˆ‘ä»¬è¿è¡Œ docker æ˜ åƒçš„ä¸€ä¸ªæ–°å®ä¾‹ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥è¿è¡Œå®‰è£… Spark æ—¶æä¾›çš„ä¸€ä¸ªç¤ºä¾‹ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥é‡ç”¨ç°æœ‰çš„ docker æ˜ åƒï¼Œå¹¶ç®€å•åœ°å¯åŠ¨ä¸€ä¸ªæ–°çš„å®ä¾‹ï¼Œç”¨ä½œé©±åŠ¨ç¨‹åº*(å‘é›†ç¾¤æäº¤åº”ç”¨ç¨‹åºçš„ä¸œè¥¿)ã€‚è¿™ä¸ªä¸éœ€è¦`--hostname`ã€`--name`å’Œ`-p`é€‰é¡¹:**

```
**docker run --rm -it --network spark_network \
    $MYNAME/spark:latest /bin/sh**
```

**åœ¨å®¹å™¨ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤å‘é›†ç¾¤æäº¤åº”ç”¨ç¨‹åº:**

```
**/spark/bin/spark-submit --master spark://spark-master:7077 --class \
    org.apache.spark.examples.SparkPi \
    /spark/examples/jars/spark-examples_2.11-2.4.0.jar 1000**
```

**æä¾›çš„ç¤ºä¾‹è®¡ç®—å‡ºå“ªä¸ª Pi æ­£åœ¨ä½¿ç”¨é›†ç¾¤ã€‚**

**å½“åº”ç”¨ç¨‹åºè¿è¡Œæ—¶ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿåœ¨ Spark Master WebUI ä¸­çœ‹åˆ°:**

**![](img/3ef48b5d6e177ec62164f8837026652b.png)**

**Spark Master WebUI â€” Running Application**

**å®Œæˆåï¼Œæ‚¨å°†åœ¨æ—¥å¿—ä¸­çœ‹åˆ° Pi è¾“å‡ºçš„å€¼:**

```
**Pi is roughly 3.1414459514144597**
```

**æ‚¨è¿˜ä¼šåœ¨ WebUI ä¸­çœ‹åˆ°å®Œæ•´çš„åº”ç”¨ç¨‹åºæŠ¥å‘Š:**

**![](img/c3fda6c9ffdcdd10c048c28ab931fc8e.png)**

**Spark Master WebUI â€” Completed Application**

## **ç”¨ Docker Compose æŠŠå®ƒé’©åœ¨ä¸€èµ·**

**Docker Compose æ˜¯ Docker æä¾›çš„ä¸€ä¸ªç®€æ´çš„å·¥å…·ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶ç”¨ä½œç¼–æ’å·¥å…·ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸å¿…è‡ªå·±åœ¨è®¸å¤šç»ˆç«¯çª—å£ä¸­è¿è¡Œå‘½ä»¤äº†ã€‚æˆ‘ä»¬æœ¬è´¨ä¸Šæ˜¯å°†å„ç§å‘½ä»¤ç¼åˆåœ¨ä¸€èµ·ï¼Œå¹¶å°†ä¸€äº›ä¸œè¥¿å‚æ•°åŒ–ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿è¡Œ`docker-compose up`ï¼Œé›†ç¾¤å°±å¼€å§‹è¿è¡Œäº†ã€‚**

**ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä¸€äº›è„šæœ¬æ¥å¤åˆ¶åˆ°æ˜ åƒå¹¶åœ¨å®¹å™¨å¯åŠ¨æ—¶è¿è¡Œã€‚æˆ‘ä»¬å°†åˆ›å»ºçš„ç¬¬ä¸€ä¸ªæ˜¯è®¾ç½® Spark Masterã€‚åˆ›å»ºä¸€ä¸ªåä¸º`start-master.sh`çš„æ–°æ–‡ä»¶ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å‘½ä»¤:**

```
**#!/bin/sh/spark/bin/spark-class org.apache.spark.deploy.master.Master \
    --ip $SPARK_LOCAL_IP \
    --port $SPARK_MASTER_PORT \
    --webui-port $SPARK_MASTER_WEBUI_PORT**
```

**æˆ‘ä»¬æ²¡æœ‰åœ¨è„šæœ¬ä¸­ç›´æ¥æŒ‡å®š IPã€Master å’Œ WebUI ç«¯å£ï¼Œè€Œæ˜¯å°†å®ƒä»¬å‚æ•°åŒ–ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ä½œä¸ºç¯å¢ƒå˜é‡æä¾›ã€‚ä»è€Œåœ¨ä¸»è®¾å¤‡ç›‘å¬çš„é…ç½®(ç«¯å£)æ–¹é¢æä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€‚è¦å°†è„šæœ¬æ”¾åˆ°å›¾åƒä¸Šï¼Œæˆ‘ä»¬éœ€è¦å¤åˆ¶å®ƒã€‚ç„¶è€Œï¼Œåœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿è„šæœ¬æ˜¯å¯æ‰§è¡Œçš„ã€‚åªè¦è¿è¡Œ`chmod +x start-worker.sh`å°±å¯ä»¥äº†ã€‚ç°åœ¨å°†è„šæœ¬æ·»åŠ åˆ°å›¾åƒä¸­ï¼Œåœ¨æœ€åä¸€ä¸ª`RUN`å‘½ä»¤ä¸‹é¢çš„ *Dockerfile* ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹:**

```
**COPY start-master.sh /start-master.sh**
```

**åœ¨æˆ‘ä»¬é‡æ–°æ„å»ºæ˜ åƒä¹‹å‰ï¼Œæˆ‘ä»¬ä¹Ÿå°†ä¸º Worker åˆ›å»ºä¸€ä¸ªç±»ä¼¼çš„è„šæœ¬ã€‚åˆ›å»ºä¸€ä¸ªåä¸º`start-worker.sh`çš„æ–°æ–‡ä»¶ï¼Œå¹¶æ·»åŠ ä»¥ä¸‹å‘½ä»¤:**

```
**#!/bin/sh/spark/bin/spark-class org.apache.spark.deploy.worker.Worker \
    --webui-port $SPARK_WORKER_WEBUI_PORT \
    $SPARK_MASTER**
```

**åŒæ ·ï¼Œæˆ‘ä»¬å·²ç»å¯¹é…ç½®è¿›è¡Œäº†å‚æ•°åŒ–ï¼Œä»¥æä¾›æ›´å¤§çš„çµæ´»æ€§ã€‚ä½¿æ–°è„šæœ¬å¯æ‰§è¡Œ(`chmod +x start-worker.sh`)ï¼Œå¹¶åœ¨ *Dockerfile* ä¸­æ·»åŠ å¦ä¸€ä¸ª`COPY`è¡Œï¼Œä»¥å°†è„šæœ¬æ·»åŠ åˆ°å›¾åƒä¸­:**

```
**COPY start-worker.sh /start-worker.sh**
```

**é‡å»ºå¹¶è¿è¡Œã€‚å¦‚æœä¸€åˆ‡é¡ºåˆ©ï¼Œæ‚¨å°†è¿›å…¥å®¹å™¨ä¸­çš„ä¸€ä¸ª shellï¼Œè¯¥å®¹å™¨ä¸­çš„è„šæœ¬ä½äºæ–‡ä»¶ç³»ç»Ÿçš„æ ¹ç›®å½•ä¸­:**

```
**bash-4.4# pwd
/
bash-4.4# ls -l
total 72
...
-rwxr-xr-x    1 root   root   357 Dec 6 11:56 start-master.sh
-rwxr-xr-x    1 root   root   284 Dec 6 17:10 start-worker.sh
...**
```

**å›æ¥åï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`docker-compose`æ¥ç¼–æ’é›†ç¾¤ã€‚**

**åˆ›å»ºä¸€ä¸ªåä¸º`docker-compose.yml`çš„æ–°æ–‡ä»¶ï¼Œå¹¶è¾“å…¥ä»¥ä¸‹å†…å®¹:**

```
**version: "3.3"
services:
  spark-master:
    image: $MYNAME/spark:latest
    container_name: spark-master
    hostname: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - spark-network
    environment:
      - "SPARK_LOCAL_IP=spark-master"
      - "SPARK_MASTER_PORT=7077"
      - "SPARK_MASTER_WEBUI_PORT=8080"
    command: "/start-master.sh"
  spark-worker:
    image: $MYNAME/spark:latest
    depends_on:
      - spark-master
    ports:
      - 8080
    networks:
      - spark-network
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_WORKER_WEBUI_PORT=8080"
    command: "/start-worker.sh"
networks:
  spark-network:
    driver: bridge
    ipam:
      driver: default**
```

**æˆ‘ä»¬åœ¨è¿™é‡Œæ‰€åšçš„å°±æ˜¯æŒ‡å®šè¦ä½¿ç”¨çš„æ˜ åƒåï¼Œè®¾ç½®ä¸»æœºåå’Œå®¹å™¨åï¼Œæ˜¾ç¤ºæ­£ç¡®çš„ç«¯å£å¹¶è¿æ¥åˆ°æˆ‘ä»¬åœ¨åº•éƒ¨åˆ›å»ºçš„ç½‘ç»œï¼Œè®¾ç½®ä¸€äº›ç¯å¢ƒé…ç½®å’Œå¯åŠ¨æ—¶è¦è¿è¡Œçš„å‘½ä»¤ã€‚æˆ‘ä»¬è¿˜å°† Worker è®¾ç½®ä¸ºä¾èµ–äºæ­£åœ¨è¿è¡Œçš„ä¸»æœåŠ¡å™¨ã€‚**

**è¦å¯åŠ¨é›†ç¾¤ï¼Œæˆ‘ä»¬åªéœ€è¿è¡Œ`docker-compose up`ã€‚Docker Compose çš„ä¸€ä¸ªä¼˜ç‚¹æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç®€å•åœ°åœ¨ Compose å‘½ä»¤ä¸­æ·»åŠ ä¸€ä¸ª`--scale`é€‰é¡¹æ¥æ‰©å±• Workersã€‚å‡è®¾æˆ‘ä»¬éœ€è¦ 3 ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼Œæˆ‘ä»¬è¿è¡Œ:**

```
**docker-compose up --scale spark-worker=3**
```

**å°±è¿™æ ·ã€‚æˆ‘ä»¬å®Œäº†ã€‚ä¸‹æ¬¡æˆ‘ä»¬å¼€å§‹å­¦ä¹ åœ¨ Scala ä¸­æ„å»ºæˆ‘ä»¬è‡ªå·±çš„åº”ç”¨ç¨‹åºæ—¶ï¼Œè¯·ç»§ç»­æ”¶å¬ã€‚**